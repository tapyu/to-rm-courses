\documentclass{article}

% Language setting
% Replace `english' with e.g. `spanish' to change the document language
\usepackage[english]{babel}
\usepackage{float}
\usepackage{multirow}

% math
\usepackage{amsfonts}
\newcommand{\trans}{\mathsf{T}}
\newcommand{\hermit}{\mathsf{H}}
\newcommand{\tr}[1]{\ensuremath{\textnormal{tr}\left(#1\right)}} % trace
\newcommand{\adj}[1]{\ensuremath{\textnormal{adj}\left(#1\right)}} % adjoint
\newcommand{\obs}[1]{\textcolor{red}{(#1)}}
\newcommand*\diff{\mathop{}\!\mathrm{d}}

% |a| -> absolute value of a, which is a scalar
\newcommand\abs[1]{\left\lvert#1\right\rvert}

% Set page size and margins
% Replace `letterpaper' with`a4paper' for UK/EU standard size
\usepackage[letterpaper,top=2cm,bottom=2cm,left=3cm,right=3cm,marginparwidth=1.75cm]{geometry}

% Useful packages
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}

% title packages
\usepackage{authblk}

% bmatrix with additional gaps
\usepackage{tabstackengine}
\stackMath
\setstackgap{L}{30pt} % vertical gap
\setstacktabbedgap{10pt} % horizontal gap
\def\lrgap{\kern6pt}
\def\xbracketVectorstack#1{\left[\lrgap\Vectorstack{#1}\lrgap\right]} % use it for vectors
\def\xbracketMatrixstack#1{\left[\lrgap\tabbedCenterstack{#1}\lrgap\right]} % use it for matrices

% biblatex
\usepackage[backend=bibtex, sorting=none, style=numeric-comp, defernumbers=true]{biblatex} % using biblatex
\addbibresource{refs.bib} % add reference file
% for each cited reference create a category named "cited"

% strike out texts
\usepackage{soul}

% begin
\title{\textbf{Solutions of the most common Matrix Calculus}  \vspace{-.3cm}}
\author{Rubem Vasconcelos Pacelli\\
  {\tt rubem.engenharia@gmail.com}}
\affil{Department of Teleinformatics Engineering, Federal University of Ceará.\\Fortaleza, Ceará, Brazil. \vspace{-.5cm}}

\begin{document}
\maketitle
\tableofcontents

\section{Introduction}
Since my Master's degree, I've been struggling with matrix differentiation as I could not find good references that cover it nicely. The bibliographies I found at that time were books from Economics \cite{dhrymes1978mathematics}, and they use a \st{weird} distinct notation.

After delving a lot, I finally found a good reference from Professor Randal's classnotes \cite{barnes2006matrix} (honorable mention for the Matrix Cookbook too \cite{petersen2008matrix}). However, to my surprise, when I tried to apply these matrix differentiation propositions, I got ``wrong'' answers! Only in my Doctorate, I discovered what was going on: \emph{there are two ways to represent a derivative of a vector} \cite{Singh}. If you do not select the author's representation, you will end up with the same result, but in a row vector\footnote{Although the expression ``row vector'' is quite common, I really advocate to avoid it since, once defined a vector as a column, \(\mathbf{y}^{\trans} \in \mathbb{C}^{1\times n}\) is actually a linear transformation from \(\mathbb{R}^{n}\) to \(\mathbb{R}\). That is, it has nothing to do with a vector, which is an entity in a \(n\)-dimensional space. Therefore, throughout this note, I will refer to it as \(1\times n\) matrix.} instead of a column vector and vice-versa. For the cases where the resulting derivative is a matrix, you will get its transpose. The first representation is called Jacobian formulation or numerator layout, while the second one is called Hessian formulation or denominator layout.

Due to the lack of references and the need to get my own guide, I decided to make this quick guide. The main goal is to derive the partial derivatives for the most common matrix calculus expressions you came across. If you are only looking for a quick table of results, the most comprehensive I've seen so far is on Wikipedia \cite{Matrixca44:online}, but maybe it is not the most relaible source.

I will use the notation that most Engineers might be used to, and only cover the Hessian formulation since this is the one that matches the derivative results I find in my books. If you are looking for the Jacobian formulation, I highly recommend Professor Randal's classnote, which uses this representation. The unique drawback is that he does not use complex numbers.

Some of the differentiation solutions here were collected from class notes, while others I derived by myself. Obviously, this guide may have errors (I hope not). If you find it, feel free to reach me out through email or simply make a pull request on my \href{https://github.com/tapyu/courses/tree/main/matrix_diff_ref}{Github}.

\section{Notation and nomeclature}

Let
\begin{align}
    \mathbf{A} = \begin{bmatrix}
        a_{11} & a_{12} & \dots & a_{1n} \\
        a_{21} & a_{22} & \dots & a_{2n} \\
        \vdots & \vdots & \ddots & \vdots \\
        a_{m1} & a_{m2} & \dots & a_{mn} \\
    \end{bmatrix} \in \mathbb{C}^{m \times n}
\end{align}
be a complex matrix with dimension equal to \(m \times n\), where \(a_{ij} \in \mathbb{C}\) is its element in the position \((i,j)\). Similarly, a complex vector is defined by
\begin{align}
    \mathbf{x} = \begin{bmatrix}
        x_1 \\
        x_2 \\
        \vdots \\
        x_n
    \end{bmatrix}  \in \mathbb{C}^{n}.
\end{align}

Nonbold Romain and Greek alphabets represent scalars, while bold uppercases and bold lowercases represent matrices and vectors, respectively. In Section \ref{sec:diff}, I will try to use the initial letters of the Romain alphabet (\(a, b, c, \dots\)) to represent knows variables, and the final letters of the Romain alphabet (\(x, y, z, w, \dots\)) to represent unknown variables. The operators \(\cdot^{\trans}\), \(\cdot^{\hermit}\), \(\cdot^*\) \(\text{tr}(\cdot)\), \(\textnormal{adj}(\cdot)\), and \(\abs{\cdot}\) represent, respectively, the transpose, the hermitian, the conjulgate, the trace, the adjoint, and the determinant (or absolute value when the operand is a scalar).

\subsection{Jacobian formulation (numerator layout)}

Consider two vectors \(\mathbf{x} \in \mathbb{C}^n\) and \(\mathbf{y} \in \mathbb{C}^m\). In the Jacobian formulation (also called numerator layout), the partial derivative of each element in \(\mathbf{y}\) by each element in \(\mathbf{x}\) is represented as
\begin{align}
    \label{eq:jacobian-formulation}
    \left[ \dfrac{\partial \mathbf{y}}{\partial \mathbf{x}} \right]_{\textnormal{Num}} = \renewcommand{\arraystretch}{2.6} \begin{bmatrix}
        \dfrac{\partial y_1^\trans}{\partial \mathbf{x}} \\
        \dfrac{\partial y_2^\trans}{\partial \mathbf{x}} \\ 
        \vdots \\ 
        \dfrac{\partial y_m^\trans}{\partial \mathbf{x}}
    \end{bmatrix} = \renewcommand{\arraystretch}{1.8}
    \begin{bmatrix}
        \dfrac{\partial y_1}{\partial x_1} & \dfrac{\partial y_1}{\partial x_2} & \dots & \dfrac{\partial y_1}{\partial x_n} \\
        \dfrac{\partial y_2}{\partial x_1} & \dfrac{\partial y_2}{\partial x_2} & \dots & \dfrac{\partial y_2}{\partial x_n} \\
        \vdots & \vdots & \ddots & \vdots \\
        \dfrac{\partial y_m}{\partial x_1} & \dfrac{\partial y_m}{\partial x_2} & \dots & \dfrac{\partial y_m}{\partial x_n} \\
    \end{bmatrix} \in \mathbb{C}^{m\times n}.
\end{align}

Note that it perfectly matches the Jacobian matrix definition,

\begin{align}
    \mathbf{J} = \renewcommand{\arraystretch}{2.6} \begin{bmatrix}
        \dfrac{\partial f_1^\trans}{\partial \mathbf{x}} \\
        \dfrac{\partial f_2^\trans}{\partial \mathbf{x}} \\ 
        \vdots \\ 
        \dfrac{\partial f_m^\trans}{\partial \mathbf{x}}
    \end{bmatrix} = \begin{bmatrix}
        \dfrac{\partial f_1}{\partial x_1} & \dfrac{\partial f_1}{\partial x_2} & \dots & \dfrac{\partial f_1}{\partial x_n} \\
        \dfrac{\partial f_2}{\partial x_1} & \dfrac{\partial f_2}{\partial x_2} & \dots & \dfrac{\partial f_2}{\partial x_n} \\
        \vdots & \vdots & \ddots & \vdots \\
        \dfrac{\partial f_m}{\partial x_1} & \dfrac{\partial f_m}{\partial x_2} & \dots & \dfrac{\partial f_m}{\partial x_n} \\
    \end{bmatrix},
\end{align}
where \(f: \mathbb{R}^n \rightarrow \mathbb{R}\). Perhaps that is why it is called the ``Jacobian formulation''.

We can infer what is the shape of \(\dfrac{\partial y}{\partial \mathbf{x}}\) and \(\dfrac{\partial \mathbf{y}}{\partial x}\) by changing the respective vectors in Equation \eqref{eq:jacobian-formulation} by scalar, but we cannot infer which shape the derivate results when one of the terms is a matrix. The partial derivative \(\dfrac{\partial \mathbf{Y}}{\partial x}\) (usually called tangent matrix) is defined for the numerator layout as
\begin{align}
    \left[\dfrac{\partial \mathbf{Y}}{\partial x}\right]_{\textnormal{Num}} \renewcommand{\arraystretch}{1.8} = \begin{bmatrix}
        \dfrac{\partial y_{11}}{\partial x} & \dfrac{\partial y_{12}}{\partial x} & \dots & \dfrac{\partial y_{1n}}{\partial x} \\
        \dfrac{\partial y_{21}}{\partial x} & \dfrac{\partial y_{22}}{\partial x} & \dots & \dfrac{\partial y_{2n}}{\partial x} \\
        \vdots & \vdots & \ddots & \vdots \\
        \dfrac{\partial y_{m1}}{\partial x} & \dfrac{\partial y_{m2}}{\partial x} & \dots & \dfrac{\partial y_{mn}}{\partial x} \\
    \end{bmatrix} \in \mathbb{C}^{m \times n},
\end{align}
where \(\mathbf{Y} \in \mathbb{C}^{m \times n}\), and the partial derivative of \(\dfrac{\partial y}{\partial \mathbf{X}}\) (usually called gradient matrix) is given by
\begin{align}
    \renewcommand{\arraystretch}{1.8}
			\left[\dfrac{\partial y}{\partial \mathbf{X}}\right]_{\textnormal{Num}} = \begin{bmatrix}
				\dfrac{\partial y}{\partial x_{11}} & \dfrac{\partial y}{\partial x_{12}} & \dots & \dfrac{\partial y}{\partial x_{1n}} \\
				\dfrac{\partial y}{\partial x_{21}} & \dfrac{\partial y}{\partial x_{22}} & \dots & \dfrac{\partial y}{\partial x_{2n}} \\
				\vdots & \vdots & \ddots & \vdots \\
				\dfrac{\partial y}{\partial x_{m1}} & \dfrac{\partial y}{\partial x_{m2}} & \dots & \dfrac{\partial y}{\partial x_{mn}} \\
			\end{bmatrix} \in \mathbb{C}^{m \times n},
\end{align}
where \(\mathbf{X} \in \mathbb{C}^{m \times n}\).

\subsection{Hessian formulation (denominator layout)}

The Hessian formulation (or denominator layout) has the following notation
\begin{align}
    \left[ \dfrac{\partial \mathbf{y}}{\partial \mathbf{x}} \right]_{\textnormal{Den}} = \begin{bmatrix}
        \dfrac{\partial y_1}{\partial \mathbf{x}} & \dfrac{\partial y_2}{\partial \mathbf{x}} & \cdots & \dfrac{\partial y_3}{\partial \mathbf{x}}
    \end{bmatrix} = \begin{bmatrix}
        \dfrac{\partial y_1}{\partial x_1} & \dfrac{\partial y_2}{\partial x_1} & \dots & \dfrac{\partial y_m}{\partial x_1} \\
        \dfrac{\partial y_1}{\partial x_2} & \dfrac{\partial y_2}{\partial x_2} & \dots & \dfrac{\partial y_m}{\partial x_2} \\
        \vdots & \vdots & \ddots & \vdots \\
        \dfrac{\partial y_1}{\partial x_n} & \dfrac{\partial y_2}{\partial x_n} & \dots & \dfrac{\partial y_m}{\partial x_n} \\
    \end{bmatrix} \in \mathbb{C}^{n\times m}.
\end{align}
I've tried to find some analogy with the Hessian matrix but, unfortunately, I haven't discovered it yet. The tangent matrix is given by
\begin{align}
    \left[\dfrac{\partial \mathbf{Y}}{\partial x}\right]_{\textnormal{Den}} \renewcommand{\arraystretch}{1.8} = \begin{bmatrix}
        \dfrac{\partial y_{11}}{\partial x} & \dfrac{\partial y_{21}}{\partial x} & \dots & \dfrac{\partial y_{m1}}{\partial x} \\
        \dfrac{\partial y_{12}}{\partial x} & \dfrac{\partial y_{22}}{\partial x} & \dots & \dfrac{\partial y_{m2}}{\partial x} \\
        \vdots & \vdots & \ddots & \vdots \\
        \dfrac{\partial y_{1n}}{\partial x} & \dfrac{\partial y_{2n}}{\partial x} & \dots & \dfrac{\partial y_{mn}}{\partial x} \\
    \end{bmatrix} \in \mathbb{C}^{n \times m},
\end{align}
where \(\mathbf{Y} \in \mathbb{C}^{m \times n}\), and the partial derivative of the gradient matrix is given by
\begin{align}
    \renewcommand{\arraystretch}{1.8}
			\left[\dfrac{\partial y}{\partial \mathbf{X}}\right]_{\textnormal{Den}} = \begin{bmatrix}
				\dfrac{\partial y}{\partial x_{11}} & \dfrac{\partial y}{\partial x_{21}} & \dots & \dfrac{\partial y}{\partial x_{m1}} \\
				\dfrac{\partial y}{\partial x_{12}} & \dfrac{\partial y}{\partial x_{22}} & \dots & \dfrac{\partial y}{\partial x_{m2}} \\
				\vdots & \vdots & \ddots & \vdots \\
				\dfrac{\partial y}{\partial x_{1n}} & \dfrac{\partial y}{\partial x_{2n}} & \dots & \dfrac{\partial y}{\partial x_{mn}} \\
			\end{bmatrix} \in \mathbb{C}^{m \times n},
\end{align}
where \(\mathbf{X} \in \mathbb{C}^{n \times m}\).

\subsection{Comparative between Jacobian and Hessian formulations}

As you could have noticed,
\begin{align}
    \left[ \dfrac{\partial \mathbf{y}}{\partial \mathbf{x}} \right]_{\textnormal{Num}} & = \left[ \dfrac{\partial \mathbf{y}}{\partial \mathbf{x}} \right]_{\textnormal{Den}}^{\trans}, \\
    \left[ \dfrac{\partial \mathbf{Y}}{\partial x}\right]_{\textnormal{Num}} & = \left[ \dfrac{\partial \mathbf{Y}}{\partial x}\right]_{\textnormal{Den}}^{\trans}, \\
    \left[\dfrac{\partial y}{\partial \mathbf{X}}\right]_{\textnormal{Num}} & = \left[\dfrac{\partial y}{\partial \mathbf{X}}\right]_{\textnormal{Den}}^{\trans}.
\end{align}

That is the difference when you try to differentiate without paying attention to which representation the author adopted. The good news is that, as long as you differentiate it correctly, you can switch between the Jacobian and Hessian formulations by simply transposing the final result\footnote{First, you should apply \(\left[\cdot\right]_{\textnormal{Den}} = \left[\cdot\right]_{\textnormal{Num}}^{\trans}\) on partial derivatives that you get in the solution. Then, you apply the transpose to the whole solution.}. Fortunately, the denominator layout is the most adopted by authors from areas related to Electrical Engineering. That is why we will focus on the denominator layout hereafter (the notation \(\left[\cdot\right]_{\textnormal{Den}}\) will be dropped out since we do not need it anymore).

\newpage
As a rule of thumb, keep in mind that:
\begin{itemize}
    \item \(\dfrac{\partial \mathbf{y}}{\partial \mathbf{x}}\) will yield a matrix.
    \item \(\dfrac{\partial \mathbf{Y}}{\partial x}\) will yield a matrix.
    \item \(\dfrac{\partial x}{\partial \mathbf{X}}\) will yield a matrix.
    \item \(\dfrac{\partial y}{\partial \mathbf{x}}\) will yield a vector.
    \item \(\dfrac{\partial \mathbf{y}}{\partial x}\) will yield a \(1\times n\) matrix (``row vector'').
\end{itemize}

Note that notations such as \(\dfrac{\partial \mathbf{y}}{\partial \mathbf{X}}, \dfrac{\partial \mathbf{Y}}{\partial \mathbf{X}}\), or \(\dfrac{\partial \mathbf{Y}}{\partial \mathbf{x}}\) do not exist for both Jacobian and Hessian formulations.

\section{Matrix Differentiation}\label{sec:diff}
To solve matrix differentiations, you only need to know that a scalar-vector derivate results in a vector for the Hessian formulation. All other shapes will naturally arise.

\subsection{\(\dfrac{\partial \mathbf{A} \mathbf{x}}{\partial \mathbf{x}} = \mathbf{A}^\trans\)}

Let \(\mathbf{A}\in \mathbb{C}^{m\times n}\) and \(\mathbf{x} \in \mathbb{C}^{n}\), in which \(\mathbf{A}\) does not depend on \(\mathbf{x}\), we have that:
\begin{align}
    \dfrac{\partial \mathbf{A} \mathbf{x}}{\partial \mathbf{x}} = & \dfrac{\partial}{\partial \mathbf{x}} \left(
        \begin{bmatrix}
            a_{11} & a_{12} & \dots & a_{1n} \\
            a_{21} & a_{22} & \dots & a_{2n} \\
            \vdots & \vdots & \ddots & \vdots \\
            a_{m1} & a_{m2} & \dots & a_{mn} \\
        \end{bmatrix} \begin{bmatrix}
            x_{1} \\ x_{2} \\ \vdots \\ x_{n}
        \end{bmatrix} \right)  \\
    %
    = & \dfrac{\partial}{\partial \mathbf{x}} \left(\begin{bmatrix} 
        \sum_{j = 1}^n a_{1j}x_j \\
        \sum_{j = 1}^n a_{2j}x_j \\
        \vdots \\
        \sum_{j = 1}^n a_{mj}x_j
    \end{bmatrix}^\trans \right)  \\
    %
    = & \begin{bmatrix}
        \dfrac{\partial}{\partial \mathbf{x}}\left(\sum_{j = 1}^n {a_{1j}x_j}\right) & \dfrac{\partial}{\partial \mathbf{x}}\left(\sum_{j = 1}^n {a_{2j}x_j}\right) & \dots & \dfrac{\partial}{\partial \mathbf{x}}\left(\sum_{j = 1}^n {a_{mj}x_j}\right)
    \end{bmatrix}
\end{align}

Since a scalar-vector derivative is represented by a vector, we have that
\begin{align}
    %
    \dfrac{\partial \mathbf{A} \mathbf{x}}{\partial \mathbf{x}} = & \begin{bmatrix}
        \dfrac{\partial}{\partial x_1} \left( \sum_{j = 1}^n a_{1j}x_j \right) & 
        \dfrac{\partial}{\partial x_1} \left( \sum_{j = 1}^n a_{21j}x_j \right) & 
        \dots & 
        \dfrac{\partial}{\partial x_1} \left( \sum_{j = 1}^n a_{mj}x_j \right) \\
        \dfrac{\partial}{\partial x_2} \left( \sum_{j = 1}^n a_{1j}x_j \right) & 
        \dfrac{\partial}{\partial x_2} \left( \sum_{j = 1}^n a_{21j}x_j \right) & 
        \dots & 
        \dfrac{\partial}{\partial x_2} \left( \sum_{j = 1}^n a_{mj}x_j \right) \\
        \vdots & \vdots & \ddots & \vdots \\
        \dfrac{\partial}{\partial x_n} \left( \sum_{j = 1}^n a_{1j}x_j \right) & 
        \dfrac{\partial}{\partial x_n} \left( \sum_{j = 1}^n a_{21j}x_j \right) & 
        \dots & 
        \dfrac{\partial}{\partial x_n} \left( \sum_{j = 1}^n a_{mj}x_j \right) \\
    \end{bmatrix}  \\
    %
    = & \begin{bmatrix}
        a_{11} & a_{21} & \dots & a_{n1} \\
        a_{12} & a_{22} & \dots & a_{n2} \\
        \vdots & \vdots & \ddots & \vdots \\
        a_{1m} & a_{2m} & \dots & a_{nm} \\
    \end{bmatrix}
\end{align}
\begin{align}
    \boxed{\dfrac{\partial \mathbf{A} \mathbf{x}}{\partial \mathbf{x}} = \mathbf{A}^\trans \in \mathbb{C}^{n\times m}}
\end{align}

\subsection{\(\dfrac{\partial \mathbf{A}  \mathbf{x}}{\partial \mathbf{z}} = \dfrac{\partial \mathbf{x}}{\partial \mathbf{z}} \mathbf{A}^\trans\)}

Let \(\mathbf{x} \in \mathbb{C}^{n}\), \(\mathbf{z} \in \mathbb{C}^{p}\) and \(\mathbf{A} \in \mathbb{C}^{m\times n}\), where \(\mathbf{x}\) depends on \(\mathbf{z}\), but \(\mathbf{A}\) does not. Then
\begin{align}
    \dfrac{\partial \mathbf{A}  \mathbf{x}}{\partial \mathbf{z}} & =
    %
    \begin{bmatrix}
        \dfrac{\partial}{\partial \mathbf{z}}\sum_{i=1}^{n} a_{1i}x_i & \dfrac{\partial}{\partial \mathbf{z}}\sum_{i=1}^{n} a_{2i}x_i & \cdots & \dfrac{\partial}{\partial \mathbf{z}}\sum_{i=1}^{n} a_{mi}x_i
    \end{bmatrix} \\
    %
    & = \begin{bmatrix}
        \sum_{i=1}^{n} a_{1i}\dfrac{\partial x_i}{\partial \mathbf{z}} & \sum_{i=1}^{n} a_{2i}\dfrac{\partial x_i}{\partial \mathbf{z}} & \cdots & \sum_{i=1}^{n} a_{mi}\dfrac{\partial x_i}{\partial \mathbf{z}}
    \end{bmatrix} \\
    %
    & = \underbrace{\begin{bmatrix}
        \dfrac{\partial x_1}{\partial \mathbf{z}} & \dfrac{\partial x_2}{\partial \mathbf{z}} & \cdots & \dfrac{\partial x_n}{\partial \mathbf{z}}
    \end{bmatrix}}_{p \times n}
    %
    \underbrace{\begin{bmatrix}
        a_{11} & a_{21} & \dots & a_{m1} \\
        a_{12} & a_{22} & \dots & a_{m2} \\
        \vdots & \vdots & \ddots & \vdots \\
        a_{1n} & a_{2n} & \dots & a_{mn} \\
    \end{bmatrix}}_{n \times m}
\end{align}
\begin{align}
    \boxed{\dfrac{\partial \mathbf{A}  \mathbf{x}}{\partial \mathbf{z}} = \dfrac{\partial \mathbf{x}}{\partial \mathbf{z}} \mathbf{A}^\trans \in \mathbb{C}^{p \times m}}
\end{align}

Observe that this result is equivalent to applying the chain rule in the following way
\begin{align}
    \dfrac{\partial \mathbf{A}  \mathbf{x}}{\partial \mathbf{z}} = \dfrac{\partial \mathbf{x}}{\partial \mathbf{z}} \dfrac{\partial \mathbf{A}  \mathbf{x}}{\partial \mathbf{x}} = \dfrac{\partial \mathbf{x}}{\partial \mathbf{z}} \mathbf{A}^\trans.
\end{align}

Note that the chain rule in matrix calculus is placed backward when compared with the standard chain rule of scalar elements.

\subsection{\(\dfrac{\partial \mathbf{a}^\trans \mathbf{x}}{\partial \mathbf{x}} = \mathbf{a}\)}

Let \(\mathbf{a, x} \in \mathbb{C}^{n}\), in which \(\mathbf{a}\) does not depend on \(\mathbf{x}\). You can derive the derivative for the inner product by considering that \(\mathbf{a}^\trans\) is actually a \(1\times n\) matrix that transforms \(\mathbb{R}^{n}\) into \(\mathbb{R}\), and we already know what is the derivate of a \(\mathbf{Ax}\). Thus,
\begin{align}
    \dfrac{\partial \mathbf{a}^\trans \mathbf{x}}{\partial \mathbf{x}} = {\mathbf{a}^\trans}^\trans = \mathbf{a}.
\end{align}

Even though, if you want the step-by-step, here it is:
\begin{align}
    \dfrac{\partial \mathbf{a}^\trans \mathbf{x}}{\partial \mathbf{x}} &= \dfrac{\partial}{\partial \mathbf{x}} \left(
    \begin{bmatrix}
        a_1 & a_2 & \dots & a_n
    \end{bmatrix} \begin{bmatrix}
        x_{1} \\ x_{2} \\ \vdots \\ x_{n}
    \end{bmatrix} \right) \\
    & = \dfrac{\partial}{\partial \mathbf{x}} \left( \sum_{i = 1}^n a_ix_i \right) \\
    & = \begin{bmatrix}
        \dfrac{\partial}{\partial x_1} \left( \sum_{i = 1}^n a_ix_i \right) \\ \dfrac{\partial}{\partial x_2} \left( \sum_{i = 1}^n a_ix_i \right) \\ \vdots \\ \dfrac{\partial}{\partial x_n} \left( \sum_{i = 1}^n a_ix_i \right) 
    \end{bmatrix} = \begin{bmatrix}
        a_1 \\ a_2 \\ \vdots \\ a_n
    \end{bmatrix}
\end{align}
\begin{align}
    \boxed{\dfrac{\partial \mathbf{a}^\trans \mathbf{x}}{\partial \mathbf{x}} = \mathbf{a} \in \mathbb{C}^{n}}
\end{align}

\subsection{\(\dfrac{\partial \mathbf{x}^\trans  \mathbf{a}}{\partial \mathbf{x}} = \mathbf{a}\)}

This one can be solved quickly by noticing that \(\mathbf{x}^\trans  \mathbf{a} = \mathbf{a}^\trans  \mathbf{x}\). Hence,
\begin{align}
    \dfrac{\partial \mathbf{x}^\trans  \mathbf{a}}{\partial \mathbf{x}} = \dfrac{\partial \mathbf{a}^\trans  \mathbf{x}}{\partial \mathbf{x}} = \mathbf{a}
\end{align}

Nevertheless, here is the step-by-step:
\begin{align}
    \dfrac{\partial \mathbf{x}^\trans \mathbf{a}}{\partial \mathbf{x}} &= \dfrac{\partial}{\partial \mathbf{x}} \left(
    \begin{bmatrix}
        x_1 & x_2 & \dots & x_n
    \end{bmatrix} \begin{bmatrix}
        a_{1} \\ a_{2} \\ \vdots \\ a_{n}
    \end{bmatrix} \right) \\
    & = \dfrac{\partial}{\partial \mathbf{x}} \left( \sum_{i = 1}^n x_ia_i \right) \\
    & = \begin{bmatrix}
        \dfrac{\partial}{\partial x_1} \left( \sum_{i = 1}^n x_ia_i \right) \\ \dfrac{\partial}{\partial x_2} \left( \sum_{i = 1}^n x_ia_i \right) \\ \vdots \\ \dfrac{\partial}{\partial x_n} \left( \sum_{i = 1}^n x_ia_i \right) 
    \end{bmatrix} 
    = \begin{bmatrix}
        a_1 \\ a_2 \\ \vdots \\ a_n
    \end{bmatrix}
\end{align}
\begin{align}
    \boxed{\dfrac{\partial \mathbf{x}^\trans \mathbf{a}}{\partial \mathbf{x}} = \mathbf{a} \in \mathbb{C}^{n}}
\end{align}

\subsection{\(\dfrac{\partial \mathbf{a}^\hermit  \mathbf{x}}{\partial \mathbf{x}} = \mathbf{a}^*\)}

Let \(\mathbf{a, x} \in \mathbb{C}^{n}\), in which \(\mathbf{a}\) does not depend on \(\mathbf{x}\). Once again, we could say that
\begin{align}
    \dfrac{\partial \mathbf{a}^\hermit \mathbf{x}}{\partial \mathbf{x}} = {\mathbf{a}^\hermit}^\trans = \mathbf{a}^*
\end{align}

Nevertheless, here is the step-by-step:
\begin{align}
    \dfrac{\partial \mathbf{a}^\hermit \mathbf{x}}{\partial \mathbf{x}} &= \dfrac{\partial}{\partial \mathbf{x}} \left(
    \begin{bmatrix}
        a^*_1 & a^*_2 & \dots & a^*_n
    \end{bmatrix} \begin{bmatrix}
        x_{1} \\ x_{2} \\ \vdots \\ x_{n}
    \end{bmatrix} \right) 
    = \dfrac{\partial}{\partial \mathbf{x}} \left( \sum_{i = 1}^n a^*_ix_i \right) \\
\end{align}

Since a scalar-vector derivative is represented by a vector, we have that
\begin{align}
    \dfrac{\partial \mathbf{a}^\hermit \mathbf{x}}{\partial \mathbf{x}} &= \begin{bmatrix}
        \dfrac{\partial}{\partial x_1} \left( \sum_{i = 1}^n a^*_ix_i \right) \\ \dfrac{\partial}{\partial x_2} \left( \sum_{i = 1}^n a^*_ix_i \right) \\ \vdots \\ \dfrac{\partial}{\partial x_n} \left( \sum_{i = 1}^n a^*_ix_i \right) 
    \end{bmatrix}
    = \begin{bmatrix}
        a^*_1 \\ a^*_2 \\ \vdots \\ a^*_n
    \end{bmatrix}
\end{align}
\begin{align}
    \boxed{\dfrac{\partial \mathbf{a}^\hermit \mathbf{x}}{\partial \mathbf{x}} = \mathbf{a}^* \in \mathbb{C}^{n}}
\end{align}

\subsection{\(\dfrac{\partial \mathbf{x}^\hermit \mathbf{a}}{\partial \mathbf{x}} = \mathbf{0}\)}
Notice that \(\mathbf{x}^\hermit \mathbf{a} \neq \mathbf{a}^\hermit \mathbf{x}\). Therefore, we have no choice but derive it. Let \(\mathbf{a, x} \in \mathbb{C}^{n}\), in which \(\mathbf{a}\) does not depend on \(\mathbf{x}\), we have that
\begin{align}
    \dfrac{\partial \mathbf{x}^\hermit \mathbf{a}}{\mathbf{x}} & = \dfrac{\partial}{\mathbf{x}} \left(
    \begin{bmatrix}
        x^*_1 & x^*_2 & \dots & x^*_n
    \end{bmatrix} \begin{bmatrix}
        a_{1} \\ a_{2} \\ \vdots \\ a_{n}
    \end{bmatrix} \right) \\
    & = \dfrac{\partial}{\mathbf{x}} \left( \sum_{i = 1}^n x^*_ia_i \right) \\
    &= \begin{bmatrix}
            \dfrac{\partial}{\partial x_1} \left( \sum_{i = 1}^n x^*_ia_i \right) \\ \dfrac{\partial}{\partial x_2} \left( \sum_{i = 1}^n x^*_ia_i \right) \\ \vdots \\
            \dfrac{\partial}{\partial x_n} \left( \sum_{i = 1}^n x^*_ia_i \right)
        \end{bmatrix}.
\end{align}
By recalling that \(\dfrac{\partial x^*}{x} = 0\) \obs{reference required}, we have that
\begin{align}
    \dfrac{\partial \mathbf{x}^\hermit \mathbf{a}}{\mathbf{x}} = \begin{bmatrix}
        0 \\ 0 \\ \vdots \\ 0
    \end{bmatrix}
\end{align}
\begin{align}
    \boxed{\dfrac{\partial \mathbf{x}^\hermit \mathbf{a}}{\mathbf{x}} = \mathbf{0} \in \mathbb{C}^{n}}
\end{align}
where \(\mathbf{0}\) is the zero vector.

\subsection{\(\dfrac{\partial \mathbf{x}^\trans \mathbf{y}}{\partial \mathbf{z}} = \dfrac{\partial \mathbf{x}}{\partial \mathbf{z}}\mathbf{y} + \dfrac{\partial \mathbf{y}}{\partial \mathbf{z}}\mathbf{x}\)}
Let \(\mathbf{x},\mathbf{y} \in \mathbb{C}^{n}\) and \(\mathbf{z} \in \mathbb{C}^{m}\). Where \(\mathbf{x}\) and \(\mathbf{y}\) depend on \(\mathbf{z}\). Thus,
\begin{align}
    \dfrac{\partial \mathbf{x}^\trans \mathbf{y}}{\partial \mathbf{z}} & = \dfrac{\partial}{\partial \mathbf{z}}\sum_{i=1}^{n} y_ix_i \\
    & = \sum_{i=1}^{n} \dfrac{\partial y_ix_i}{\partial \mathbf{z}}.
\end{align}
Recalling that \((fg)' = f'g + g'f\), we have
\begin{align}
    \dfrac{\partial \mathbf{x}^\trans \mathbf{y}}{\partial \mathbf{z}} & = \sum_{i=1}^{n} x_i\dfrac{\partial y_i}{\partial \mathbf{z}} + \sum_{i=1}^{n} y_i\dfrac{\partial x_i}{\partial \mathbf{z}} \\
    & = \begin{bmatrix}
        \sum_{i=1}^{n} x_i\dfrac{\partial y_i}{\partial z_1} \\
        \sum_{i=1}^{n} x_i\dfrac{\partial y_i}{\partial z_2} \\
        \vdots \\
        \sum_{i=1}^{n} x_i\dfrac{\partial y_i}{\partial z_m} \\
    \end{bmatrix} +
    \begin{bmatrix}
        \sum_{i=1}^{n} y_i\dfrac{\partial x_i}{\partial z_1} \\
        \sum_{i=1}^{n} y_i\dfrac{\partial x_i}{\partial z_2} \\
        \vdots \\
        \sum_{i=1}^{n} y_i\dfrac{\partial x_i}{\partial z_m}
    \end{bmatrix} \\
    %
    & = \begin{bmatrix}
        \dfrac{\partial y_1}{\partial z_1} & \dfrac{\partial y_2}{\partial z_1} & \cdots & \dfrac{\partial y_n}{\partial z_1} \\
        \dfrac{\partial y_1}{\partial z_2} & \dfrac{\partial y_2}{\partial z_2} & \cdots & \dfrac{\partial y_n}{\partial z_2} \\
        \vdots & \ddots & \vdots & \vdots \\
        \dfrac{\partial y_1}{\partial z_m} & \dfrac{\partial y_2}{\partial z_m} & \cdots & \dfrac{\partial y_n}{\partial z_m} \\
    \end{bmatrix} \mathbf{x} \\
    & \hspace{2.5ex} + \begin{bmatrix}
        \dfrac{\partial x_1}{\partial z_1} & \dfrac{\partial x_2}{\partial z_1} & \cdots & \dfrac{\partial x_n}{\partial z_1} \\
        \dfrac{\partial x_1}{\partial z_2} & \dfrac{\partial x_2}{\partial z_2} & \cdots & \dfrac{\partial x_n}{\partial z_2} \\
        \vdots & \ddots & \vdots & \vdots \\
        \dfrac{\partial x_1}{\partial z_m} & \dfrac{\partial x_2}{\partial z_m} & \cdots & \dfrac{\partial x_n}{\partial z_m} \\
    \end{bmatrix} \mathbf{y}
\end{align}
\begin{align}
    \boxed{\dfrac{\partial \mathbf{x}^\trans \mathbf{y}}{\partial \mathbf{z}} = \dfrac{\partial \mathbf{x}}{\partial \mathbf{z}}\mathbf{y} + \dfrac{\partial \mathbf{y}}{\partial \mathbf{z}}\mathbf{x} \in \mathbb{C}^{m}}
\end{align}
Note that, if either \(\mathbf{x}\) or \(\mathbf{y}\) does not depend on \(\mathbf{z}\), just disconsider \(\dfrac{\partial \mathbf{x}}{\partial \mathbf{z}}\mathbf{y}\) or \(\dfrac{\partial \mathbf{y}}{\partial \mathbf{z}}\mathbf{x}\), respectively.

\subsection{\(\dfrac{\partial \mathbf{x}^\trans \mathbf{x}}{\partial \mathbf{x}} = 2\mathbf{x}\)}
Let \(\mathbf{x} \in \mathbb{C}^{n}\) and \(\mathbf{z}\in \mathbb{C}^m\), where \(\mathbf{x}\) depends on \(\mathbf{z}\). Thus,
\begin{align}
    \dfrac{\partial \mathbf{x}^\trans \mathbf{x}}{\partial \mathbf{x}} & = \dfrac{\partial}{\partial \mathbf{x}}\sum_{i=1}^{n} x_i^2 \\
    & = \sum_{i=1}^{n} \dfrac{\partial x_i^2}{\partial \mathbf{x}} \\
    %
    & = \renewcommand{\arraystretch}{2.2}\begin{bmatrix}
        \sum_{i=1}^{n} \dfrac{\partial x_i^2}{\partial x_1} \\
        \sum_{i=1}^{n} \dfrac{\partial x_i^2}{\partial x_2} \\
        \vdots \\
        \sum_{i=1}^{n} \dfrac{\partial x_i^2}{\partial x_m} \\
    \end{bmatrix} \\
    %
    & = \renewcommand{\arraystretch}{2.2}\begin{bmatrix}
        2x_1 \\
        2x_2 \\
        \vdots \\
        2x_n \\
    \end{bmatrix}
\end{align}
\begin{align}
    \boxed{\dfrac{\partial \mathbf{x}^\trans \mathbf{x}}{\partial \mathbf{x}} = 2\mathbf{x} \in \mathbb{C}^n}
\end{align}

Note that this perfectly matches with the derivate of a quadratic scalar value, i.e., \(\frac{\diff x^2}{\diff x} = 2x\).

\subsection{\(\dfrac{\partial \mathbf{x}^\trans \mathbf{x}}{\partial \mathbf{z}} = 2\dfrac{\partial \mathbf{x}}{\partial \mathbf{z}}\mathbf{x}\)}
Let \(\mathbf{x} \in \mathbb{C}^{n}\) and \(\mathbf{z}\in \mathbb{C}^m\), where \(\mathbf{x}\) depends on \(\mathbf{z}\). Thus,
\begin{align}
    \dfrac{\partial \mathbf{x}^\trans \mathbf{x}}{\partial \mathbf{z}} & = \dfrac{\partial}{\partial \mathbf{z}}\sum_{i=1}^{n} x_i^2 \\
    & = \sum_{i=1}^{n} \dfrac{\partial x_i^2}{\partial \mathbf{z}} \\
    %
    & = \renewcommand{\arraystretch}{2.2}\begin{bmatrix}
        \sum_{i=1}^{n} \dfrac{\partial x_i^2}{\partial z_1} \\
        \sum_{i=1}^{n} \dfrac{\partial x_i^2}{\partial z_2} \\
        \vdots \\
        \sum_{i=1}^{n} \dfrac{\partial x_i^2}{\partial z_m} \\
    \end{bmatrix} \\
    %
    & = \renewcommand{\arraystretch}{2.2}\begin{bmatrix}
        \sum_{i=1}^{n} 2x_i\dfrac{\partial x_i}{\partial z_1} \\
        \sum_{i=1}^{n} 2x_i\dfrac{\partial x_i}{\partial z_2} \\
        \vdots \\
        \sum_{i=1}^{n} 2x_i\dfrac{\partial x_i}{\partial z_m} \\
    \end{bmatrix} \\
    & = 2 \renewcommand{\arraystretch}{2.2}\begin{bmatrix}
        \dfrac{\partial x_1}{\partial \mathbf{z}} & \dfrac{\partial x_2}{\partial \mathbf{z}} & \cdots & \dfrac{\partial x_n}{\partial \mathbf{z}}
    \end{bmatrix} \mathbf{x}
\end{align}
\begin{align}
    \boxed{\dfrac{\partial \mathbf{x}^\trans \mathbf{x}}{\partial \mathbf{z}} = 2\dfrac{\partial \mathbf{x}}{\partial \mathbf{z}}\mathbf{x} \in \mathbb{C}^m}
\end{align}

Note that this solution could also be solved by the chain rule as follows
\begin{align}
    \boxed{\dfrac{\partial \mathbf{x}^\trans \mathbf{x}}{\partial \mathbf{z}} = \dfrac{\partial \mathbf{x}}{\partial \mathbf{z}} \dfrac{\partial \mathbf{x}^\trans \mathbf{x}}{\partial \mathbf{x}} = 2\dfrac{\partial \mathbf{x}}{\partial \mathbf{z}}\mathbf{x} \in \mathbb{C}^m}
\end{align}

\subsection{\(\dfrac{\partial \mathbf{x}^\trans \mathbf{A} \mathbf{x}}{\partial \mathbf{x}} = \left(\mathbf{A}^\trans + \mathbf{A}\right) \mathbf{x}\)}
Let \(\mathbf{A}\in \mathbb{C}^{n\times n}\) and \(\mathbf{x} \in \mathbb{C}^{n}\), in which \(\mathbf{A}\) does not depend on \(\mathbf{x}\). For the quadratic form, it follows that
\begin{align}
    \dfrac{\partial \mathbf{x}^\trans \mathbf{A} \mathbf{x}}{\partial \mathbf{x}} &= \dfrac{\partial}{\partial \mathbf{x}} \left(
    \begin{bmatrix}
        x_{1} & x_{2} & \dots & x_{n}
    \end{bmatrix}
    \begin{bmatrix}
        a_{11} & a_{12} & \dots & a_{1n} \\
        a_{21} & a_{22} & \dots & a_{2n} \\
        \vdots & \vdots & \ddots & \vdots \\
        a_{n1} & a_{n2} & \dots & a_{nn} \\
    \end{bmatrix} \begin{bmatrix}
        x_{1} \\ x_{2} \\ \vdots \\ x_{n}
    \end{bmatrix} \right) \\
    &= \dfrac{\partial}{\partial \mathbf{x}} \left(
			\begin{bmatrix}
				\displaystyle \sum_{i = 1}^{n} x_{i}a_{i1} & 
				\displaystyle \sum_{i = 1}^{n} x_{i}a_{i2} & 
				\dots & 
				\displaystyle \sum_{i = 1}^{n} x_{i}a_{in}
			\end{bmatrix} \begin{bmatrix}
				x_{1} \\ x_{2} \\ \vdots \\ x_{n}
			\end{bmatrix} \right) \\
            &= \dfrac{\partial}{\partial \mathbf{x}} \left(
				\sum_{i = 1}^{n}\sum_{j = 1}^{n} a_{ij} x_{i} x_{j}
			\right).
\end{align}

Note that the element inside the parentheses is a scalar and that a scalar-vector derivative results in a vector, that is,
\begin{align}
    \dfrac{\partial \mathbf{x}^\trans \mathbf{A} \mathbf{x}}{\partial \mathbf{x}} &= \begin{bmatrix}
        \displaystyle \dfrac{\partial}{\partial x_1} \left( \sum_{i = 1}^{n}\sum_{j = 1}^{n} x_{i} a_{ij} x_{j} \right) \\ 
        \displaystyle  \dfrac{\partial}{\partial x_2} \left( \sum_{i = 1}^{n}\sum_{j = 1}^{n} x_{i} a_{ij} x_{j} \right) \\ 
        \vdots \\ 
        \displaystyle \dfrac{\partial}{\partial x_n} \left( \sum_{i = 1}^{n}\sum_{j = 1}^{n} x_{i} a_{ij} x_{j} \right) 
    \end{bmatrix} \\
    & = \begin{bmatrix}
        \displaystyle 2x_1a_{11} + \sum_{\substack{j = 1 \\ j \neq 1}}^{n} a_{1j} x_{j} + \sum_{\substack{i = 1 \\ i \neq 1}}^{n} a_{i1} x_{i} \\
        \displaystyle 2x_2a_{22} + \sum_{\substack{j = 1 \\ j \neq 2}}^{n} a_{2j} x_{j} + \sum_{\substack{i = 1 \\ i \neq 2}}^{n} a_{i2} x_{i} \\
        \vdots \\
        \displaystyle 2x_na_{nn} + \sum_{\substack{j = 1 \\ j \neq n}}^{n} a_{nj} x_{j} + \sum_{\substack{i = 1 \\ i \neq n}}^{n} a_{in} x_{i} 
    \end{bmatrix} \\
    &= \begin{bmatrix}
        \displaystyle \sum_{j = 1}^{n} a_{1j} x_{j} \\
        \displaystyle \sum_{j = 1}^{n} a_{2j} x_{j} \\
        \vdots \\
        \displaystyle \sum_{j = 1}^{n} a_{nj} x_{j} 
    \end{bmatrix} +
    \begin{bmatrix}
        \displaystyle \sum_{i = 1}^{n} a_{i1} x_{i} \\
        \displaystyle \sum_{i = 1}^{n} a_{i2} x_{i} \\
        \vdots \\
        \displaystyle \sum_{i = 1}^{n} a_{in} x_{i} 
    \end{bmatrix} \\
    & = \mathbf{A}^\trans \mathbf{x} + \mathbf{A} \mathbf{x}
\end{align}
\begin{align}
    \label{eq:quadratic-solution}
    \boxed{\dfrac{\partial \mathbf{x}^\trans \mathbf{A} \mathbf{x}}{\partial \mathbf{x}} = \left(\mathbf{A}^\trans + \mathbf{A}\right) \mathbf{x} \in \mathbb{C}^{n}}
\end{align}
For the special case where \(\mathbf{A}\) is symmetric, we obtain
\begin{align}
    \boxed{\dfrac{\partial \mathbf{x}^\trans \mathbf{A} \mathbf{x}}{\partial \mathbf{x}} = 2\mathbf{A} \mathbf{x} \in \mathbb{C}^{n}}
\end{align}

\subsection{\(\dfrac{\partial \mathbf{x}^\trans \mathbf{A} \mathbf{x}}{\partial \mathbf{z}} = \dfrac{\partial \mathbf{x}}{\partial \mathbf{z}}\left( \mathbf{A} + \mathbf{A}^\trans \right) \mathbf{x}\)}
Let \(\mathbf{z} \in \mathbb{C}^{m}, \mathbf{x} \in \mathbb{C}^{n}\) and \(\mathbf{A}\in \mathbb{C}^{n\times n}\), where \(\mathbf{x}\) depends on \(\mathbf{z}\), but \(\mathbf{A}\) does not. For the quadratic form, it follows that
\begin{align}
    \dfrac{\partial \mathbf{x}^\trans \mathbf{A} \mathbf{x}}{\partial \mathbf{z}} &= \dfrac{\partial}{\partial \mathbf{z}} \left(
    \begin{bmatrix}
        x_{1} & x_{2} & \dots & x_{n}
    \end{bmatrix}
    \begin{bmatrix}
        a_{11} & a_{12} & \dots & a_{1n} \\
        a_{21} & a_{22} & \dots & a_{2n} \\
        \vdots & \vdots & \ddots & \vdots \\
        a_{n1} & a_{n2} & \dots & a_{nn} \\
    \end{bmatrix} \begin{bmatrix}
        x_{1} \\ x_{2} \\ \vdots \\ x_{n}
    \end{bmatrix} \right) \\
    &= \dfrac{\partial}{\partial \mathbf{z}} \left(
			\begin{bmatrix}
				\displaystyle \sum_{i = 1}^{n} x_{i}a_{i1} & 
				\displaystyle \sum_{i = 1}^{n} x_{i}a_{i2} & 
				\dots & 
				\displaystyle \sum_{i = 1}^{n} x_{i}a_{in}
			\end{bmatrix} \begin{bmatrix}
				x_{1} \\ x_{2} \\ \vdots \\ x_{n}
			\end{bmatrix} \right) \\
            &= \dfrac{\partial}{\partial \mathbf{z}} \left(
				\sum_{i = 1}^{n}\sum_{j = 1}^{n} a_{ij} x_{i} x_{j}
			\right) \\
    &= \begin{bmatrix}
        \displaystyle \dfrac{\partial}{\partial z_1} \left( \sum_{i = 1}^{n}\sum_{j = 1}^{n} x_{i} a_{ij} x_{j} \right) \\ 
        \displaystyle  \dfrac{\partial}{\partial z_2} \left( \sum_{i = 1}^{n}\sum_{j = 1}^{n} x_{i} a_{ij} x_{j} \right) \\ 
        \vdots \\ 
        \displaystyle \dfrac{\partial}{\partial z_n} \left( \sum_{i = 1}^{n}\sum_{j = 1}^{n} x_{i} a_{ij} x_{j} \right) 
    \end{bmatrix} \\
    &= \begin{bmatrix}
        \displaystyle \sum_{i = 1}^{n}\sum_{j = 1}^{n} a_{ij} \dfrac{\partial x_{i}x_{j}}{\partial z_1} \\ 
        \displaystyle \sum_{i = 1}^{n}\sum_{j = 1}^{n} a_{ij} \dfrac{\partial x_{i}x_{j}}{\partial z_2} \\ 
        \vdots \\ 
        \displaystyle \sum_{i = 1}^{n}\sum_{j = 1}^{n} a_{ij} \dfrac{\partial x_{i}x_{j}}{\partial z_n} 
    \end{bmatrix}
\end{align}

Recalling that \((fg)' = f'g + g'f\), we have that
\begin{align}
    \dfrac{\partial \mathbf{x}^\trans \mathbf{A} \mathbf{x}}{\partial \mathbf{z}} &= \begin{bmatrix}
        \displaystyle \sum_{i = 1}^{n}\sum_{j = 1}^{n} x_{j}a_{ij} \dfrac{\partial x_{i}}{\partial z_1} +
        \sum_{i = 1}^{n}\sum_{j = 1}^{n} x_{i}a_{ij} \dfrac{\partial x_{j}}{\partial z_1} \\ 
        \displaystyle \sum_{i = 1}^{n}\sum_{j = 1}^{n} x_{j}a_{ij} \dfrac{\partial x_{i}}{\partial z_2} +
        \sum_{i = 1}^{n}\sum_{j = 1}^{n} x_{i}a_{ij} \dfrac{\partial x_{j}}{\partial z_2} \\ 
        \vdots \\ 
        \displaystyle \sum_{i = 1}^{n}\sum_{j = 1}^{n} x_{j}a_{ij} \dfrac{\partial x_{i}}{\partial z_n} +
        \sum_{i = 1}^{n}\sum_{j = 1}^{n} x_{i}a_{ij} \dfrac{\partial x_{j}}{\partial z_n} 
    \end{bmatrix} \\
    &= \begin{bmatrix}
        \displaystyle \sum_{i = 1}^{n}\sum_{j = 1}^{n} x_{j}a_{ij} \dfrac{\partial x_{i}}{\partial z_1} \\
        \displaystyle \sum_{i = 1}^{n}\sum_{j = 1}^{n} x_{j}a_{ij} \dfrac{\partial x_{i}}{\partial z_2} \\
        \vdots \\ 
        \displaystyle \sum_{i = 1}^{n}\sum_{j = 1}^{n} x_{j}a_{ij} \dfrac{\partial x_{i}}{\partial z_n}
    \end{bmatrix} +
    \begin{bmatrix}
        \displaystyle \sum_{i = 1}^{n}\sum_{j = 1}^{n} x_{i}a_{ij} \dfrac{\partial x_{j}}{\partial z_1} \\ 
        \displaystyle \sum_{i = 1}^{n}\sum_{j = 1}^{n} x_{i}a_{ij} \dfrac{\partial x_{j}}{\partial z_2} \\ 
        \vdots \\ 
        \displaystyle \sum_{i = 1}^{n}\sum_{j = 1}^{n} x_{i}a_{ij} \dfrac{\partial x_{j}}{\partial z_n} 
    \end{bmatrix} \\
    &= \begin{bmatrix}
       \dfrac{\partial x_{1}}{\partial \mathbf{z}} & \dfrac{\partial x_{2}}{\partial \mathbf{z}} & \cdots  & \dfrac{\partial x_{n}}{\partial \mathbf{z}}
    \end{bmatrix}
    \begin{bmatrix}
        a_{11} & a_{12} & \dots & a_{1n} \\
        a_{21} & a_{22} & \dots & a_{2n} \\
        \vdots & \vdots & \ddots & \vdots \\
        a_{m1} & a_{m2} & \dots & a_{mn} \\
    \end{bmatrix}
    \begin{bmatrix}
        x_{1} \\ x_{2} \\ \vdots \\ x_{n}
    \end{bmatrix} + \nonumber \\
    & \hspace{2.5ex} \begin{bmatrix}
        \dfrac{\partial x_{1}}{\partial \mathbf{z}} & \dfrac{\partial x_{2}}{\partial \mathbf{z}} & \cdots  & \dfrac{\partial x_{n}}{\partial \mathbf{z}}
     \end{bmatrix}
     \begin{bmatrix}
         a_{11} & a_{21} & \dots & a_{m1} \\
         a_{12} & a_{22} & \dots & a_{m2} \\
         \vdots & \vdots & \ddots & \vdots \\
         a_{1n} & a_{2n} & \dots & a_{mn} \\
     \end{bmatrix}
     \begin{bmatrix}
         x_{1} \\ x_{2} \\ \vdots \\ x_{n}
     \end{bmatrix} \\
     & = \dfrac{\partial \mathbf{x}}{\partial \mathbf{z}}\mathbf{A} \mathbf{x} + \dfrac{\partial \mathbf{x}}{\partial \mathbf{z}}\mathbf{A}^\trans \mathbf{x}
\end{align}
\begin{align}
    \boxed{\dfrac{\partial \mathbf{x}^\trans \mathbf{A} \mathbf{x}}{\partial \mathbf{z}} = \dfrac{\partial \mathbf{x}}{\partial \mathbf{z}}\left( \mathbf{A} + \mathbf{A}^\trans \right) \mathbf{x} \in \mathbb{C}^{m}}
\end{align}

For the special case where \(\mathbf{A}\) is symmetric, we get
\begin{align}
    \boxed{\dfrac{\partial \mathbf{x}^\trans \mathbf{A} \mathbf{x}}{\partial \mathbf{z}} = 2\dfrac{\partial \mathbf{x}}{\partial \mathbf{z}}\mathbf{A} \mathbf{x} \in \mathbb{C}^{m}}
\end{align}

Note that the solution is much easier if we maintain the matrix calculus notation and apply the chain rule, that is,
\begin{align}
    \dfrac{\partial \mathbf{x}^\trans \mathbf{A} \mathbf{x}}{\partial \mathbf{z}} = \dfrac{\partial \mathbf{x}}{\partial \mathbf{z}} \dfrac{\partial \mathbf{x}^\trans \mathbf{A} \mathbf{x}}{\partial \mathbf{x}} = \dfrac{\partial \mathbf{x}}{\partial \mathbf{z}} \left(\mathbf{A}^\trans + \mathbf{A}\right) \mathbf{x},
\end{align}
where the last equality comes from the Equation \eqref{eq:quadratic-solution}.

\subsection{\(\dfrac{\partial \mathbf{y}^\trans \mathbf{A} \mathbf{x}}{\partial \mathbf{x}}\)}

\subsection{\(\dfrac{\partial \mathbf{y}^\trans \mathbf{A} \mathbf{x}}{\partial \mathbf{y}}\)}

\subsection{\(\dfrac{\partial \mathbf{y}^\trans \mathbf{A} \mathbf{x}}{\partial \mathbf{z}}\)}

\subsection{\(\dfrac{\partial \textnormal{tr}\left(\mathbf{A} \mathbf{X}\right)}{\partial \mathbf{X}} = \mathbf{A}^\trans\)}

Let \(\mathbf{A}, \mathbf{X} \in \mathbb{R}^{n\times n}\), where \(\mathbf{A}\) does not depend on the elements in \(\mathbf{X}\).
\begin{align*}
    \dfrac{\partial \textnormal{tr}\left(\mathbf{A} \mathbf{X}\right)}{\partial \mathbf{X}} &= \dfrac{\partial}{\partial \mathbf{X}} \left( \tr{\begin{bmatrix}
        a_{11} & a_{12} & \dots & a_{1n} \\
        a_{21} & a_{22} & \dots & a_{2n} \\
        \vdots & \vdots & \ddots & \vdots \\
        a_{n1} & a_{n2} & \dots & a_{nn} \\
    \end{bmatrix}
    \begin{bmatrix}
        x_{11} & x_{12} & \dots & x_{1n} \\
        x_{21} & x_{22} & \dots & x_{2n} \\
        \vdots & \vdots & \ddots & \vdots \\
        x_{n1} & x_{n2} & \dots & x_{nn} \\
    \end{bmatrix} \right)} \\
    %
    &= \dfrac{\partial}{\partial \mathbf{X}} \left( \sum_{i = 1}^{n} \sum_{j = 1}^{n} a_{ij}x_{ji} \right) \\
    %
    &= \begin{bmatrix}
        \displaystyle \dfrac{\partial}{\partial x_{11}} \left( \sum_{i = 1}^{n} \sum_{j = 1}^{n} a_{ij}x_{ji} \right) & 
        \displaystyle \dfrac{\partial}{\partial x_{12}} \left( \sum_{i = 1}^{n} \sum_{j = 1}^{n} a_{ij}x_{ji} \right) & 
        \dots & 
        \displaystyle \dfrac{\partial}{\partial x_{1n}} \left( \sum_{i = 1}^{n} \sum_{j = 1}^{n} a_{ij}x_{ji} \right) \\
        \displaystyle \dfrac{\partial}{\partial x_{21}} \left( \sum_{i = 1}^{n} \sum_{j = 1}^{n} a_{ij}x_{ji} \right) & 
        \displaystyle \dfrac{\partial}{\partial x_{22}} \left( \sum_{i = 1}^{n} \sum_{j = 1}^{n} a_{ij}x_{ji} \right) & 
        \dots & 
        \displaystyle \dfrac{\partial}{\partial x_{2n}} \left( \sum_{i = 1}^{n} \sum_{j = 1}^{n} a_{ij}x_{ji} \right) \\
        \vdots & \vdots & \ddots & \vdots \\
        \displaystyle \dfrac{\partial}{\partial x_{n1}} \left( \sum_{i = 1}^{n} \sum_{j = 1}^{n} a_{ij}x_{ji} \right) & 
        \displaystyle \dfrac{\partial}{\partial x_{n2}} \left( \sum_{i = 1}^{n} \sum_{j = 1}^{n} a_{ij}x_{ji} \right) & 
        \dots & 
        \displaystyle \dfrac{\partial}{\partial x_{nn}} \left( \sum_{i = 1}^{n} \sum_{j = 1}^{n} a_{ij}x_{ji} \right) \\
    \end{bmatrix} \\
    &= \begin{bmatrix}
            a_{11} & a_{21} & \dots & a_{n1} \\
            a_{12} & a_{22} & \dots & a_{n2} \\
            \vdots & \vdots & \ddots & \vdots \\
            a_{1n} & a_{2n} & \dots & a_{nn} \\
        \end{bmatrix}
\end{align*}
\begin{align}
    \boxed{\dfrac{\partial \textnormal{tr}\left(\mathbf{A} \mathbf{X}\right)}{\partial \mathbf{X}} = \mathbf{A}^\trans}
\end{align}
\subsection{\(\dfrac{\partial \abs{\mathbf{X}}}{\partial \mathbf{X}} = \adj{\mathbf{X}}\)}
Let \(\mathbf{X} \in \mathbb{R}^{n\times n}\). Through Laplace expansion (cofactor expansion), we can rewrite the determinant of \(\mathbf{X}\) as the sum of the cofactors of any row or column, multiplied by its generating element, that is
\begin{align}
    \abs{\mathbf{X}} = \sum_{i = 1}^{n} x_{ki} \abs{\mathbf{C}_{ki}} = \sum_{i = 1}^{n} x_{ik} \abs{\mathbf{C}_{ik}} \,\,\,\,\,\, \forall \,\, k \in \left\{ 1, 2, ..., n \right\},
\end{align}
where \(\mathbf{C}_{ij}\) denotes the cofactor matrix of \(\mathbf{X}\) generated from element \(x_{ij}\). It is worth noting that the cofactor of \(\mathbf{C}_{ij}\) is independent of the value of any element \((i,j)\) in \(\mathbf{X}\). Therefore, it follows that
\begin{align}
    \dfrac{\partial \abs{\mathbf{X}}}{\partial \mathbf{X}} &= \dfrac{\partial}{\partial \mathbf{X}} \left( \sum_{i = 1}^{n} x_{ki} \abs{\mathbf{C}_{ki}} \right) \,\,\,\,\,\, \forall \,\, k \in \left\{ 1, 2, ..., n \right\} \\
    %
    & = \dfrac{\partial}{\partial \mathbf{X}} \left( \begin{bmatrix}
        \displaystyle \sum_{i = 1}^{n} x_{1i} \abs{\mathbf{C}_{1i}} & 
        \displaystyle \sum_{i = 1}^{n} x_{1i} \abs{\mathbf{C}_{1i}} & 
        \dots & 
        \displaystyle \sum_{i = 1}^{n} x_{1i} \abs{\mathbf{C}_{1i}} \\
        \displaystyle \sum_{i = 1}^{n} x_{2i} \abs{\mathbf{C}_{2i}} & 
        \displaystyle \sum_{i = 1}^{n} x_{2i} \abs{\mathbf{C}_{2i}} & 
        \dots & 
        \displaystyle \sum_{i = 1}^{n} x_{2i} \abs{\mathbf{C}_{2i}} \\
        \vdots & \vdots & \ddots & \vdots \\
        \displaystyle \sum_{i = 1}^{n} x_{ni} \abs{\mathbf{C}_{ni}} & 
        \displaystyle \sum_{i = 1}^{n} x_{ni} \abs{\mathbf{C}_{ni}} & 
        \dots & 
        \displaystyle \sum_{i = 1}^{n} x_{ni} \abs{\mathbf{C}_{ni}}
    \end{bmatrix} \right) \\
    %
    & = \begin{bmatrix}
        \displaystyle \dfrac{\partial}{\partial x_{11}} \left( \sum_{i = 1}^{n} x_{1i} \abs{\mathbf{C}_{1i}} \right) & 
        \displaystyle \dfrac{\partial}{\partial x_{12}} \left( \sum_{i = 1}^{n} x_{1i} \abs{\mathbf{C}_{1i}} \right) & 
        \dots & 
        \displaystyle \dfrac{\partial}{\partial x_{13}} \left( \sum_{i = 1}^{n} x_{1i} \abs{\mathbf{C}_{1i}} \right) \\
        \displaystyle \dfrac{\partial}{\partial x_{21}} \left( \sum_{i = 1}^{n} x_{2i} \abs{\mathbf{C}_{2i}} \right) & 
        \displaystyle \dfrac{\partial}{\partial x_{22}} \left( \sum_{i = 1}^{n} x_{2i} \abs{\mathbf{C}_{2i}} \right) & 
        \dots & 
        \displaystyle \dfrac{\partial}{\partial x_{33}} \left( \sum_{i = 1}^{n} x_{2i} \abs{\mathbf{C}_{2i}} \right) \\
        \vdots & \vdots & \ddots & \vdots \\
        \displaystyle \dfrac{\partial}{\partial x_{n1}} \left( \sum_{i = 1}^{n} x_{ni} \abs{\mathbf{C}_{ni}} \right) & 
        \displaystyle \dfrac{\partial}{\partial x_{n2}} \left( \sum_{i = 1}^{n} x_{ni} \abs{\mathbf{C}_{ni}} \right) & 
        \dots & 
        \displaystyle \dfrac{\partial}{\partial x_{n3}} \left( \sum_{i = 1}^{n} x_{ni} \abs{\mathbf{C}_{ni}} \right) \\
    \end{bmatrix} \\
    &= \begin{bmatrix}
        \abs{\mathbf{C}_{11}} & \abs{\mathbf{C}_{12}} & \dots & \abs{\mathbf{C}_{1n}} \\
        \abs{\mathbf{C}_{21}} & \abs{\mathbf{C}_{22}} & \dots & \abs{\mathbf{C}_{2n}} \\
        \vdots & \vdots & \ddots & \vdots \\
        \abs{\mathbf{C}_{n1}} & \abs{\mathbf{C}_{n2}} & \dots & \abs{\mathbf{C}_{nn}} \\
    \end{bmatrix}
\end{align}
\begin{align}
    \boxed{\dfrac{\partial \abs{\mathbf{X}}}{\partial \mathbf{X}} = \adj{\mathbf{X}}}
\end{align}

\subsection{\(\dfrac{\partial \mathbf{A}^{-1}}{\partial \alpha }\)}

\nocite{*}
\printbibliography

\end{document}