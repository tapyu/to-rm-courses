\documentclass{article}

% Language setting
% Replace `english' with e.g. `spanish' to change the document language
\usepackage[english]{babel}
\usepackage{float}
\usepackage{multirow}

% math
\usepackage{amsfonts}
\newcommand{\trans}{\mathsf{T}}
\newcommand{\hermit}{\mathsf{H}}
\newcommand{\tr}[1]{\ensuremath{\textnormal{tr}\left(#1\right)}} % trace
\newcommand{\adj}[1]{\ensuremath{\textnormal{adj}\left(#1\right)}} % adjoint
\newcommand{\obs}[1]{\textcolor{red}{(#1)}}
\newcommand*\diff{\mathop{}\!\mathrm{d}}

% |a| -> absolute value of a, which is a scalar
\newcommand\abs[1]{\left\lvert#1\right\rvert}

% Set page size and margins
% Replace `letterpaper' with`a4paper' for UK/EU standard size
\usepackage[letterpaper,top=2cm,bottom=2cm,left=3cm,right=3cm,marginparwidth=1.75cm]{geometry}

% Useful packages
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}

% title packages
\usepackage{authblk}

% bmatrix with additional gaps
\usepackage{tabstackengine}
\stackMath
\setstackgap{L}{30pt} % vertical gap
\setstacktabbedgap{10pt} % horizontal gap
\def\lrgap{\kern6pt}
\def\xbracketVectorstack#1{\left[\lrgap\Vectorstack{#1}\lrgap\right]} % use it for vectors
\def\xbracketMatrixstack#1{\left[\lrgap\tabbedCenterstack{#1}\lrgap\right]} % use it for matrices

% biblatex
\usepackage[backend=bibtex, sorting=none, style=numeric-comp, defernumbers=true]{biblatex} % using biblatex
\addbibresource{refs.bib} % add reference file
% for each cited reference create a category named "cited"

% strike out texts
\usepackage{soul}

% begin
\title{\textbf{Solutions of the most common Matrix Differentiations}  \vspace{-.3cm}}
\author{Rubem Vasconcelos Pacelli\\
  {\tt rubem.engenharia@gmail.com}}
\affil{Department of Teleinformatics Engineering, Federal University of Ceará.\\Fortaleza, Ceará, Brazil. \vspace{-.5cm}}

\begin{document}
\maketitle
\tableofcontents

\section{Introduction}
Since my Master's degree, I've been struggling with matrix differentiation as I could not find good references that cover it nicely. The bibliographies I found at that time were books from Economics \cite{dhrymes1978mathematics}, and they use a \st{weird} distinct notation.

After delving a lot, I finally found a good reference from Professor Randal's classnote \cite{barnes2006matrix} (honorable mention for the Matrix Cookbook too \cite{petersen2008matrix}). However, to my surprise, when I tried to apply these matrix differentiation propositions, I got ``wrong'' answers! After a while, I discovered what was going on: \emph{there are two ways to represent a derivative of a vector} \cite{Singh}. If you do not select the author's representation, you will end up with the same result, but in a row vector\footnote{Although the expression ``row vector'' is quite common, I really advocate to avoid it since, once defined a vector as a column, \(\mathbf{y}^{\trans} \in \mathbb{C}^{1\times n}\) is actually a linear transformation from \(\mathbb{R}^{n}\) to \(\mathbb{R}\). That is, it has nothing to do with a vector, which is a numerical entity in a \(n\)-dimensional space. Therefore, throughout this note, I will refer to it as \(1\times n\) matrix.} instead of a column vector and vice-versa. For the cases where the resulting derivative is a matrix, you will get its transpose. The first representation is called Jacobian formulation or numerator layout, while the second one is called Hessian formulation or denominator layout.

Due to the lack of references and the need to get my own guide, I decided to make this quick guide. The main goal is to derive the partial derivatives for the most common matrix calculus expressions you came across. If you are only looking for a quick table of results, the most comprehensive I've seen so far is on Wikipedia \cite{Matrixca44:online}, but maybe it is not the most reliable source.

I will use the notation that most Engineers might be used to, and only cover the Hessian formulation since this is the one that matches the derivative results I find in my books. If you are looking for the Jacobian formulation, I highly recommend Professor Randal's classnote, which uses this representation. The unique drawback is that he does not use complex numbers.

Some of the differentiation solutions here were collected from class notes, while others I derived by myself. Obviously, this guide may have errors (I hope not). If you find it, feel free to reach me out through email or simply make a pull request on my \href{https://github.com/tapyu/courses/tree/main/matrix_diff_ref}{Github}.

\section{Notation and nomeclature}

Let
\begin{align}
    \mathbf{A} = \begin{bmatrix}
        a_{11} & a_{12} & \dots & a_{1n} \\
        a_{21} & a_{22} & \dots & a_{2n} \\
        \vdots & \vdots & \ddots & \vdots \\
        a_{m1} & a_{m2} & \dots & a_{mn} \\
    \end{bmatrix} \in \mathbb{C}^{m \times n}
\end{align}
be a complex matrix with dimension equal to \(m \times n\), where \(a_{ij} \in \mathbb{C}\) is its element in the position \((i,j)\). Similarly, a complex vector is defined by
\begin{align}
    \mathbf{x} = \begin{bmatrix}
        x_1 \\
        x_2 \\
        \vdots \\
        x_n
    \end{bmatrix}  \in \mathbb{C}^{n}.
\end{align}

Nonbold Romain and Greek alphabets represent scalars, while bold uppercase and bold lowercase represent matrices and vectors, respectively. In Section \ref{sec:diff}, I will try to use the initial letters of the Romain alphabet (\(a, b, c, \dots\)) to represent known variables, and the final letters of the Romain alphabet (\(x, y, z, w, \dots\)) to represent unknown variables. The operators \(\cdot^{\trans}\), \(\cdot^{\hermit}\), \(\cdot^*\) \(\text{tr}(\cdot)\), \(\textnormal{adj}(\cdot)\), and \(\abs{\cdot}\) represent, respectively, the transpose, the hermitian, the conjugate, the trace, the adjoint, and the determinant (or absolute value when the operand is a scalar). Greek letters will be preferred to represent independent and unknown scalars that are not within a vector or matrix.

\subsection{Jacobian formulation (numerator layout)}

Consider two vectors \(\mathbf{x} \in \mathbb{C}^n\) and \(\mathbf{y} \in \mathbb{C}^m\). In the Jacobian formulation (also called numerator layout), the partial derivative of each element in \(\mathbf{y}\) by each element in \(\mathbf{x}\) is represented as
\begin{align}
    \label{eq:jacobian-formulation}
    \left[ \dfrac{\partial \mathbf{y}}{\partial \mathbf{x}} \right]_{\textnormal{Num}} = \renewcommand{\arraystretch}{2.6} \begin{bmatrix}
        \dfrac{\partial y_1^\trans}{\partial \mathbf{x}} \\
        \dfrac{\partial y_2^\trans}{\partial \mathbf{x}} \\ 
        \vdots \\ 
        \dfrac{\partial y_m^\trans}{\partial \mathbf{x}}
    \end{bmatrix} = \renewcommand{\arraystretch}{1.8}
    \begin{bmatrix}
        \dfrac{\partial y_1}{\partial x_1} & \dfrac{\partial y_1}{\partial x_2} & \dots & \dfrac{\partial y_1}{\partial x_n} \\
        \dfrac{\partial y_2}{\partial x_1} & \dfrac{\partial y_2}{\partial x_2} & \dots & \dfrac{\partial y_2}{\partial x_n} \\
        \vdots & \vdots & \ddots & \vdots \\
        \dfrac{\partial y_m}{\partial x_1} & \dfrac{\partial y_m}{\partial x_2} & \dots & \dfrac{\partial y_m}{\partial x_n} \\
    \end{bmatrix} \in \mathbb{C}^{m\times n}.
\end{align}

Note that it perfectly matches the Jacobian matrix definition,

\begin{align}
    \mathbf{J} = \renewcommand{\arraystretch}{2.6} \begin{bmatrix}
        \dfrac{\partial f_1^\trans}{\partial \mathbf{x}} \\
        \dfrac{\partial f_2^\trans}{\partial \mathbf{x}} \\ 
        \vdots \\ 
        \dfrac{\partial f_m^\trans}{\partial \mathbf{x}}
    \end{bmatrix} = \begin{bmatrix}
        \dfrac{\partial f_1}{\partial x_1} & \dfrac{\partial f_1}{\partial x_2} & \dots & \dfrac{\partial f_1}{\partial x_n} \\
        \dfrac{\partial f_2}{\partial x_1} & \dfrac{\partial f_2}{\partial x_2} & \dots & \dfrac{\partial f_2}{\partial x_n} \\
        \vdots & \vdots & \ddots & \vdots \\
        \dfrac{\partial f_m}{\partial x_1} & \dfrac{\partial f_m}{\partial x_2} & \dots & \dfrac{\partial f_m}{\partial x_n} \\
    \end{bmatrix},
\end{align}
where \(f: \mathbb{R}^n \rightarrow \mathbb{R}\). Perhaps that is why it is called the ``Jacobian formulation''.

We can infer what is the shape of \(\dfrac{\partial y}{\partial \mathbf{x}}\) and \(\dfrac{\partial \mathbf{y}}{\partial x}\) by changing the respective vectors in Equation \eqref{eq:jacobian-formulation} by scalar, but we cannot infer which shape the derivate results when one of the terms is a matrix. The partial derivative \(\dfrac{\partial \mathbf{Y}}{\partial x}\) (usually called tangent matrix) is defined for the numerator layout as
\begin{align}
    \left[\dfrac{\partial \mathbf{Y}}{\partial x}\right]_{\textnormal{Num}} \renewcommand{\arraystretch}{1.8} = \begin{bmatrix}
        \dfrac{\partial y_{11}}{\partial x} & \dfrac{\partial y_{12}}{\partial x} & \dots & \dfrac{\partial y_{1n}}{\partial x} \\
        \dfrac{\partial y_{21}}{\partial x} & \dfrac{\partial y_{22}}{\partial x} & \dots & \dfrac{\partial y_{2n}}{\partial x} \\
        \vdots & \vdots & \ddots & \vdots \\
        \dfrac{\partial y_{m1}}{\partial x} & \dfrac{\partial y_{m2}}{\partial x} & \dots & \dfrac{\partial y_{mn}}{\partial x} \\
    \end{bmatrix} \in \mathbb{C}^{m \times n},
\end{align}
where \(\mathbf{Y} \in \mathbb{C}^{m \times n}\), and the partial derivative of \(\dfrac{\partial y}{\partial \mathbf{X}}\) (usually called gradient matrix) is given by
\begin{align}
    \renewcommand{\arraystretch}{1.8}
			\left[\dfrac{\partial y}{\partial \mathbf{X}}\right]_{\textnormal{Num}} = \begin{bmatrix}
				\dfrac{\partial y}{\partial x_{11}} & \dfrac{\partial y}{\partial x_{12}} & \dots & \dfrac{\partial y}{\partial x_{1n}} \\
				\dfrac{\partial y}{\partial x_{21}} & \dfrac{\partial y}{\partial x_{22}} & \dots & \dfrac{\partial y}{\partial x_{2n}} \\
				\vdots & \vdots & \ddots & \vdots \\
				\dfrac{\partial y}{\partial x_{m1}} & \dfrac{\partial y}{\partial x_{m2}} & \dots & \dfrac{\partial y}{\partial x_{mn}} \\
			\end{bmatrix} \in \mathbb{C}^{m \times n},
\end{align}
where \(\mathbf{X} \in \mathbb{C}^{m \times n}\).

\subsection{Hessian formulation (denominator layout)}

The Hessian formulation (or denominator layout) has the following notation
\begin{align}
    \left[ \dfrac{\partial \mathbf{y}}{\partial \mathbf{x}} \right]_{\textnormal{Den}} = \begin{bmatrix}
        \dfrac{\partial y_1}{\partial \mathbf{x}} & \dfrac{\partial y_2}{\partial \mathbf{x}} & \cdots & \dfrac{\partial y_3}{\partial \mathbf{x}}
    \end{bmatrix} = \begin{bmatrix}
        \dfrac{\partial y_1}{\partial x_1} & \dfrac{\partial y_2}{\partial x_1} & \dots & \dfrac{\partial y_m}{\partial x_1} \\
        \dfrac{\partial y_1}{\partial x_2} & \dfrac{\partial y_2}{\partial x_2} & \dots & \dfrac{\partial y_m}{\partial x_2} \\
        \vdots & \vdots & \ddots & \vdots \\
        \dfrac{\partial y_1}{\partial x_n} & \dfrac{\partial y_2}{\partial x_n} & \dots & \dfrac{\partial y_m}{\partial x_n} \\
    \end{bmatrix} \in \mathbb{C}^{n\times m}.
\end{align}
I've tried to find some analogy with the Hessian matrix but, unfortunately, I haven't discovered it yet. The tangent matrix is given by
\begin{align}
    \left[\dfrac{\partial \mathbf{Y}}{\partial x}\right]_{\textnormal{Den}} \renewcommand{\arraystretch}{1.8} = \begin{bmatrix}
        \dfrac{\partial y_{11}}{\partial x} & \dfrac{\partial y_{21}}{\partial x} & \dots & \dfrac{\partial y_{m1}}{\partial x} \\
        \dfrac{\partial y_{12}}{\partial x} & \dfrac{\partial y_{22}}{\partial x} & \dots & \dfrac{\partial y_{m2}}{\partial x} \\
        \vdots & \vdots & \ddots & \vdots \\
        \dfrac{\partial y_{1n}}{\partial x} & \dfrac{\partial y_{2n}}{\partial x} & \dots & \dfrac{\partial y_{mn}}{\partial x} \\
    \end{bmatrix} \in \mathbb{C}^{n \times m},
\end{align}
where \(\mathbf{Y} \in \mathbb{C}^{m \times n}\), and the partial derivative of the gradient matrix is given by
\begin{align}
    \renewcommand{\arraystretch}{1.8}
			\left[\dfrac{\partial y}{\partial \mathbf{X}}\right]_{\textnormal{Den}} = \begin{bmatrix}
				\dfrac{\partial y}{\partial x_{11}} & \dfrac{\partial y}{\partial x_{21}} & \dots & \dfrac{\partial y}{\partial x_{m1}} \\
				\dfrac{\partial y}{\partial x_{12}} & \dfrac{\partial y}{\partial x_{22}} & \dots & \dfrac{\partial y}{\partial x_{m2}} \\
				\vdots & \vdots & \ddots & \vdots \\
				\dfrac{\partial y}{\partial x_{1n}} & \dfrac{\partial y}{\partial x_{2n}} & \dots & \dfrac{\partial y}{\partial x_{mn}} \\
			\end{bmatrix} \in \mathbb{C}^{m \times n},
\end{align}
where \(\mathbf{X} \in \mathbb{C}^{n \times m}\).

\subsection{Comparative between Jacobian and Hessian formulations}

As you could have noticed,
\begin{align}
    \left[ \dfrac{\partial \mathbf{y}}{\partial \mathbf{x}} \right]_{\textnormal{Num}} & = \left[ \dfrac{\partial \mathbf{y}}{\partial \mathbf{x}} \right]_{\textnormal{Den}}^{\trans}, \\
    \left[ \dfrac{\partial \mathbf{Y}}{\partial x}\right]_{\textnormal{Num}} & = \left[ \dfrac{\partial \mathbf{Y}}{\partial x}\right]_{\textnormal{Den}}^{\trans}, \\
    \left[\dfrac{\partial y}{\partial \mathbf{X}}\right]_{\textnormal{Num}} & = \left[\dfrac{\partial y}{\partial \mathbf{X}}\right]_{\textnormal{Den}}^{\trans}.
\end{align}

That is the difference when you try to differentiate without paying attention to which representation the author adopted. The good news is that, as long as you differentiate it correctly, you can switch between the Jacobian and Hessian formulations by simply transposing the final result\footnote{First, you should apply \(\left[\cdot\right]_{\textnormal{Den}} = \left[\cdot\right]_{\textnormal{Num}}^{\trans}\) on partial derivatives that you get in the solution. Then, you apply the transpose to the whole solution.}. Fortunately, the denominator layout is the most adopted by authors from areas related to Electrical Engineering. That is why we will focus on the denominator layout hereafter (the notation \(\left[\cdot\right]_{\textnormal{Den}}\) will be dropped out since we do not need it anymore).

As a rule of thumb, keep in mind that:
\begin{itemize}
    \item \(\dfrac{\partial \mathbf{y}}{\partial \mathbf{x}}\) will yield a matrix.
    \item \(\dfrac{\partial \mathbf{Y}}{\partial x}\) will yield a matrix.
    \item \(\dfrac{\partial x}{\partial \mathbf{X}}\) will yield a matrix.
    \item \(\dfrac{\partial y}{\partial \mathbf{x}}\) will yield a vector.
    \item \(\dfrac{\partial \mathbf{y}}{\partial x}\) will yield a \(1\times n\) matrix (``row vector'').
\end{itemize}

Note that notations such as \(\dfrac{\partial \mathbf{y}}{\partial \mathbf{X}}, \dfrac{\partial \mathbf{Y}}{\partial \mathbf{X}}\), or \(\dfrac{\partial \mathbf{Y}}{\partial \mathbf{x}}\) do not exist for both Jacobian and Hessian formulations.

\section{Identities}

We usually have two ways to solve matrix differentiation:
\begin{enumerate}
    \item Performing element-by-element operations in matrices and vectors;
    \item Preserving the matrix calculus notation, performing operations on the whole matrix/vector and, eventually, using some identities.
\end{enumerate}
The latter is usually more straightforward and less toilsome than the former and is therefore preferable.

However, in order to only use matrix calculus notations, we need to be cautious when applying the matrix differentiation identities since the element orders matter. For instance, for scalar elements, the product rule may be written as either \((fg)' = f'g + g'f\) or \((fg)' = g f' + f g'\). In matrix calculus, we do not have such a privilege.

\subsection{Chain rule}
\subsubsection{One intermediate variable}
For scalar elements, the chain rule is given by
\begin{align}
    \dfrac{\partial w}{\partial z} = \dfrac{\partial w}{\partial x} \dfrac{\partial x}{\partial y} \dfrac{\partial y}{\partial z}.
\end{align}
Similarly, in matrix notation, we have
\begin{align}
    \label{eq:chain-1inter}
    \dfrac{\partial \mathbf{w}}{\partial \mathbf{z}} = \dfrac{\partial \mathbf{y}}{\partial \mathbf{z}} \dfrac{\partial \mathbf{x}}{\partial \mathbf{y}} \dfrac{\partial \mathbf{w}}{\partial \mathbf{x}},
\end{align}
where \(\mathbf{x} \in \mathbb{C}^{n}, \mathbf{y} \in \mathbb{C}^{m}, \mathbf{z} \in \mathbb{C}^{p}\), and \(\mathbf{w} \in \mathbb{R}^q\). In this expression, \(\mathbf{w}\) depends on \(\mathbf{x}\), \(\mathbf{x}\) depends on \(\mathbf{y}\), and \(\mathbf{y}\) depends on \(\mathbf{z}\). The number of elements in the chain rule can be increased indiscriminately. The main point here is that \emph{the chain rule in matrix calculus notation must be placed backward when compared with the standard chain rule of scalar elements}.

\subsubsection{Multi-intermediate variables}

In the previous section, we needed the partial derivative of \(\mathbf{w}\) by \(\mathbf{z}\). We could solve it by applying the chain rule. \(\mathbf{w}\) is called \emph{dependent} variable, \(\mathbf{z}\) is called \emph{independent} variable, and \(\mathbf{x}\) and \(\mathbf{y}\) are called \emph{intermediate} variables.

We had the luck of having only one intermediate variable by time, that is, \(\mathbf{w}\) depends on \(\mathbf{x}\), which depends on \(\mathbf{y}\), which depends on \(\mathbf{z}\). However, we might find a situation where \(\mathbf{w}\) depends on both \(\mathbf{x}\) and \(\mathbf{y}\) which, in turn, depend on \(\mathbf{z}\).

For scalar elements, we could solve this kind of problem by considering that \(w = f(x, y)\) is differentiable on \(x\) and \(y\). The chain rule becomes
\begin{align}
    \dfrac{\partial w}{\partial z} = \dfrac{\partial w}{\partial x} \dfrac{\partial x}{\partial z} + \dfrac{\partial w}{\partial y} \dfrac{\partial y}{\partial z}.
\end{align}

Similarly, for matrix calculus notation, we have
\begin{align}
    \label{eq:chain-multi-inter}
    \dfrac{\partial \mathbf{w}}{\partial \mathbf{z}} = \dfrac{\partial \mathbf{x}}{\partial \mathbf{z}} \dfrac{\partial \mathbf{w}}{\partial \mathbf{x}} + \dfrac{\partial \mathbf{y}}{\partial \mathbf{z}} \dfrac{\partial \mathbf{w}}{\partial \mathbf{y}}
\end{align}

Note the backward placement of each summation term. This expression can be used for an unrestricted number of intermediate variables.

\subsection{Sum (or minus) rule}
\subsubsection{Vector-vector derivative}
Let \(\mathbf{x}, \mathbf{y} \in \mathbb{C}^{m}\) and \(a, b \in \mathbb{C}\), where \(\mathbf{x}\) and \(\mathbf{y}\) depend on \(\mathbf{w} \in \mathbb{C}^{n}\), but \(a\) and \(b\) do not. Therefore,
\begin{align}
    \dfrac{\partial (a\mathbf{x} \pm b\mathbf{y})}{\partial\mathbf{w}} = a\dfrac{\partial \mathbf{x}}{\partial\mathbf{w}} \pm b\dfrac{\partial \mathbf{y}}{\partial\mathbf{w}}
\end{align}
\subsubsection{Matrix-scalar derivative}
Another is when you have
\begin{align}
    \dfrac{\partial \left( a\mathbf{X} \pm b\mathbf{Y} \right)}{\partial \alpha},
\end{align}
where \(\mathbf{X}, \mathbf{Y} \in \mathbb{C}^{m \times n}\) depend on \(\alpha \in \mathbb{C}\). The solution is
\begin{align}
    \dfrac{\partial (a\mathbf{X} \pm b\mathbf{Y})}{\partial\mathbf{\alpha}} = a\dfrac{\partial \mathbf{X}}{\partial \alpha} \pm b \dfrac{\partial \mathbf{Y}}{\partial \alpha}.
\end{align}
\subsubsection{Scalar-matrix derivative}
The scalar-matrix derivative has a similar result, i.e.,
\begin{align}
    \dfrac{\partial \left( ax \pm by \right)}{\partial \mathbf{W}} = a\dfrac{\partial x}{\partial \mathbf{W}} \pm b \dfrac{\partial y}{\partial \mathbf{W}},
\end{align}
where \(x, y \in \mathbb{C}\) depend on \(\mathbf{W} \in \mathbb{C}^{m\times n}\), but \(a,b \in \mathbb{C}\) do not.

\subsection{Product rule}
\subsubsection{Vector-vector derivative}
Let \(w \in \mathbb{C}\) and \(\mathbf{z} \in \mathbb{C}^{m}\), where both depend on \(\mathbf{x} \in \mathbb{C}^{n}\). Then,
\begin{align}
    \dfrac{\partial w \mathbf{z}}{\partial \mathbf{x}} = w \dfrac{\partial \mathbf{z}}{\partial \mathbf{x}} + \dfrac{\partial w}{\partial \mathbf{x}} \mathbf{z}^\trans.
\end{align}

Note that is not possible to apply the product rule when you have \(\mathbf{Wz}\), where \(\mathbf{W} \in \mathbb{C}^{n \times m}\) also depends on \(\mathbf{x}\). If you tried, you would get \(\partial\mathbf{W}/\partial\mathbf{x}\), which does not exist.
\subsubsection{Scalar-vector derivative}
Another possibility of applying the product rule is when you have \(\mathbf{w}^{\trans} \mathbf{z}\), where \(\mathbf{w} \in \mathbb{C}^{m}\) also depends on \(\mathbf{x} \in \mathbb{C}^{n}\). In this case, the dot product is given by
\begin{align}
    \dfrac{\partial \mathbf{w}^{\trans} \mathbf{z}}{\partial \mathbf{x}} = \dfrac{\partial \mathbf{z}}{\partial \mathbf{x}} \mathbf{w} + \dfrac{\partial \mathbf{w}}{\partial \mathbf{x}} \mathbf{z}.
\end{align}

\subsubsection{Scalar-matrix derivative}
It is still possible to apply the product rule to
\begin{align}
    \dfrac{\partial wz}{\partial \mathbf{X}},
\end{align}
where \(w,z \in \mathbb{C}\) depend on \(\mathbf{X} \in \mathbb{C}^{m \times n}\). In this case, we have
\begin{align}
    \dfrac{\partial wz}{\partial \mathbf{X}} = w \dfrac{\partial z}{\partial \mathbf{X}} + z \dfrac{\partial w}{\partial \mathbf{X}}.
\end{align}

\subsubsection{Matrix-scalar derivative}
The last case is when you have
\begin{align}
    \dfrac{\partial \mathbf{W}\mathbf{Z}}{\partial \alpha},
\end{align}
where both \(\mathbf{W} \in \mathbb{C}^{m \times p}\) and \(\mathbf{Z} \in \mathbb{C}^{p\times n}\) depend on \(\alpha \in \mathbb{C}\). In this case, we have
\begin{align}
    \label{eq:matrix-matrix-product-rule}
    \dfrac{\partial \mathbf{W}\mathbf{Z}}{\partial \alpha} = \dfrac{\partial \mathbf{Z}}{\partial \alpha}\mathbf{W}^{\trans} + \mathbf{Z}^{T} \dfrac{\mathbf{W}}{\partial \alpha}
\end{align}


\section{Solution of Matrix Differentiations}\label{sec:diff}
The solutions in this Section will usually show the element-by-element solution and the solution by preserving the matrix calculus notation. For the element-by-element solutions, you only need to know that a scalar-vector derivate results in a vector for the Hessian formulation. All other shapes will naturally arise. For solutions with matrix calculus notation, you need to be acquainted with some of their identities.

\subsection{\(\dfrac{\partial \mathbf{A} \mathbf{x}}{\partial \mathbf{x}} = \mathbf{A}^\trans\)}

Let \(\mathbf{A}\in \mathbb{C}^{m\times n}\) and \(\mathbf{x} \in \mathbb{C}^{n}\), in which \(\mathbf{A}\) does not depend on \(\mathbf{x}\), we have that:
\begin{align}
    \dfrac{\partial \mathbf{A} \mathbf{x}}{\partial \mathbf{x}} = & \dfrac{\partial}{\partial \mathbf{x}} \left(
        \begin{bmatrix}
            a_{11} & a_{12} & \dots & a_{1n} \\
            a_{21} & a_{22} & \dots & a_{2n} \\
            \vdots & \vdots & \ddots & \vdots \\
            a_{m1} & a_{m2} & \dots & a_{mn} \\
        \end{bmatrix} \begin{bmatrix}
            x_{1} \\ x_{2} \\ \vdots \\ x_{n}
        \end{bmatrix} \right)  \\
    %
    = & \dfrac{\partial}{\partial \mathbf{x}} \left(\begin{bmatrix} 
        \sum_{j = 1}^n a_{1j}x_j \\
        \sum_{j = 1}^n a_{2j}x_j \\
        \vdots \\
        \sum_{j = 1}^n a_{mj}x_j
    \end{bmatrix}^\trans \right)  \\
    %
    = & \begin{bmatrix}
        \dfrac{\partial}{\partial \mathbf{x}}\left(\sum_{j = 1}^n {a_{1j}x_j}\right) & \dfrac{\partial}{\partial \mathbf{x}}\left(\sum_{j = 1}^n {a_{2j}x_j}\right) & \dots & \dfrac{\partial}{\partial \mathbf{x}}\left(\sum_{j = 1}^n {a_{mj}x_j}\right)
    \end{bmatrix}
\end{align}

Since a scalar-vector derivative is represented by a vector, we have that
\begin{align}
    %
    \dfrac{\partial \mathbf{A} \mathbf{x}}{\partial \mathbf{x}} = & \begin{bmatrix}
        \dfrac{\partial}{\partial x_1} \left( \sum_{j = 1}^n a_{1j}x_j \right) & 
        \dfrac{\partial}{\partial x_1} \left( \sum_{j = 1}^n a_{21j}x_j \right) & 
        \dots & 
        \dfrac{\partial}{\partial x_1} \left( \sum_{j = 1}^n a_{mj}x_j \right) \\
        \dfrac{\partial}{\partial x_2} \left( \sum_{j = 1}^n a_{1j}x_j \right) & 
        \dfrac{\partial}{\partial x_2} \left( \sum_{j = 1}^n a_{21j}x_j \right) & 
        \dots & 
        \dfrac{\partial}{\partial x_2} \left( \sum_{j = 1}^n a_{mj}x_j \right) \\
        \vdots & \vdots & \ddots & \vdots \\
        \dfrac{\partial}{\partial x_n} \left( \sum_{j = 1}^n a_{1j}x_j \right) & 
        \dfrac{\partial}{\partial x_n} \left( \sum_{j = 1}^n a_{21j}x_j \right) & 
        \dots & 
        \dfrac{\partial}{\partial x_n} \left( \sum_{j = 1}^n a_{mj}x_j \right) \\
    \end{bmatrix}  \\
    %
    = & \begin{bmatrix}
        a_{11} & a_{21} & \dots & a_{n1} \\
        a_{12} & a_{22} & \dots & a_{n2} \\
        \vdots & \vdots & \ddots & \vdots \\
        a_{1m} & a_{2m} & \dots & a_{nm} \\
    \end{bmatrix}
\end{align}
\begin{align}
    \label{eq:lt-slution}
    \boxed{\dfrac{\partial \mathbf{A} \mathbf{x}}{\partial \mathbf{x}} = \mathbf{A}^\trans \in \mathbb{C}^{n\times m}}
\end{align}

\subsection{\(\dfrac{\partial \mathbf{A}  \mathbf{x}}{\partial \mathbf{z}} = \dfrac{\partial \mathbf{x}}{\partial \mathbf{z}} \mathbf{A}^\trans\)}

Let \(\mathbf{x} \in \mathbb{C}^{n}\), \(\mathbf{z} \in \mathbb{C}^{p}\) and \(\mathbf{A} \in \mathbb{C}^{m\times n}\), where \(\mathbf{x}\) depends on \(\mathbf{z}\), but \(\mathbf{A}\) does not. Then
\begin{align}
    \dfrac{\partial \mathbf{A}  \mathbf{x}}{\partial \mathbf{z}} & =
    %
    \begin{bmatrix}
        \dfrac{\partial}{\partial \mathbf{z}}\sum_{i=1}^{n} a_{1i}x_i & \dfrac{\partial}{\partial \mathbf{z}}\sum_{i=1}^{n} a_{2i}x_i & \cdots & \dfrac{\partial}{\partial \mathbf{z}}\sum_{i=1}^{n} a_{mi}x_i
    \end{bmatrix} \\
    %
    & = \begin{bmatrix}
        \sum_{i=1}^{n} a_{1i}\dfrac{\partial x_i}{\partial \mathbf{z}} & \sum_{i=1}^{n} a_{2i}\dfrac{\partial x_i}{\partial \mathbf{z}} & \cdots & \sum_{i=1}^{n} a_{mi}\dfrac{\partial x_i}{\partial \mathbf{z}}
    \end{bmatrix} \\
    %
    & = \underbrace{\begin{bmatrix}
        \dfrac{\partial x_1}{\partial \mathbf{z}} & \dfrac{\partial x_2}{\partial \mathbf{z}} & \cdots & \dfrac{\partial x_n}{\partial \mathbf{z}}
    \end{bmatrix}}_{p \times n}
    %
    \underbrace{\begin{bmatrix}
        a_{11} & a_{21} & \dots & a_{m1} \\
        a_{12} & a_{22} & \dots & a_{m2} \\
        \vdots & \vdots & \ddots & \vdots \\
        a_{1n} & a_{2n} & \dots & a_{mn} \\
    \end{bmatrix}}_{n \times m}
\end{align}
\begin{align}
    \boxed{\dfrac{\partial \mathbf{A}  \mathbf{x}}{\partial \mathbf{z}} = \dfrac{\partial \mathbf{x}}{\partial \mathbf{z}} \mathbf{A}^\trans \in \mathbb{C}^{p \times m}}
\end{align}

Observe that this result is equivalent to applying the chain rule (c.f. Equation \eqref{eq:chain-1inter}), that is,
\begin{align}
    \dfrac{\partial \mathbf{A}  \mathbf{x}}{\partial \mathbf{z}} = \dfrac{\partial \mathbf{x}}{\partial \mathbf{z}} \dfrac{\partial \mathbf{A}  \mathbf{x}}{\partial \mathbf{x}} = \dfrac{\partial \mathbf{x}}{\partial \mathbf{z}} \mathbf{A}^\trans.
\end{align}

\subsection{\(\dfrac{\partial \mathbf{a}^\trans \mathbf{x}}{\partial \mathbf{x}} = \mathbf{a}\)}

Let \(\mathbf{a, x} \in \mathbb{C}^{n}\), in which \(\mathbf{a}\) does not depend on \(\mathbf{x}\). You can derive the derivative for the inner product by considering that \(\mathbf{a}^\trans\) is actually a \(1\times n\) matrix that transforms \(\mathbb{R}^{n}\) into \(\mathbb{R}\), and we already know what is the derivate of a \(\mathbf{Ax}\). Thus,
\begin{align}
    \dfrac{\partial \mathbf{a}^\trans \mathbf{x}}{\partial \mathbf{x}} = {\mathbf{a}^\trans}^\trans = \mathbf{a}.
\end{align}

Even though, if you want the step-by-step, here it is:
\begin{align}
    \dfrac{\partial \mathbf{a}^\trans \mathbf{x}}{\partial \mathbf{x}} &= \dfrac{\partial}{\partial \mathbf{x}} \left(
    \begin{bmatrix}
        a_1 & a_2 & \dots & a_n
    \end{bmatrix} \begin{bmatrix}
        x_{1} \\ x_{2} \\ \vdots \\ x_{n}
    \end{bmatrix} \right) \\
    & = \dfrac{\partial}{\partial \mathbf{x}} \left( \sum_{i = 1}^n a_ix_i \right) \\
    & = \begin{bmatrix}
        \dfrac{\partial}{\partial x_1} \left( \sum_{i = 1}^n a_ix_i \right) \\ \dfrac{\partial}{\partial x_2} \left( \sum_{i = 1}^n a_ix_i \right) \\ \vdots \\ \dfrac{\partial}{\partial x_n} \left( \sum_{i = 1}^n a_ix_i \right) 
    \end{bmatrix} = \begin{bmatrix}
        a_1 \\ a_2 \\ \vdots \\ a_n
    \end{bmatrix}
\end{align}
\begin{align}
    \boxed{\dfrac{\partial \mathbf{a}^\trans \mathbf{x}}{\partial \mathbf{x}} = \mathbf{a} \in \mathbb{C}^{n}}
\end{align}

\subsection{\(\dfrac{\partial \mathbf{x}^\trans  \mathbf{a}}{\partial \mathbf{x}} = \mathbf{a}\)}

This one can be solved quickly by noticing that \(\mathbf{x}^\trans  \mathbf{a} = \mathbf{a}^\trans  \mathbf{x}\). Hence,
\begin{align}
    \dfrac{\partial \mathbf{x}^\trans  \mathbf{a}}{\partial \mathbf{x}} = \dfrac{\partial \mathbf{a}^\trans  \mathbf{x}}{\partial \mathbf{x}} = \mathbf{a}
\end{align}

Nevertheless, here is the step-by-step:
\begin{align}
    \dfrac{\partial \mathbf{x}^\trans \mathbf{a}}{\partial \mathbf{x}} &= \dfrac{\partial}{\partial \mathbf{x}} \left(
    \begin{bmatrix}
        x_1 & x_2 & \dots & x_n
    \end{bmatrix} \begin{bmatrix}
        a_{1} \\ a_{2} \\ \vdots \\ a_{n}
    \end{bmatrix} \right) \\
    & = \dfrac{\partial}{\partial \mathbf{x}} \left( \sum_{i = 1}^n x_ia_i \right) \\
    & = \begin{bmatrix}
        \dfrac{\partial}{\partial x_1} \left( \sum_{i = 1}^n x_ia_i \right) \\ \dfrac{\partial}{\partial x_2} \left( \sum_{i = 1}^n x_ia_i \right) \\ \vdots \\ \dfrac{\partial}{\partial x_n} \left( \sum_{i = 1}^n x_ia_i \right) 
    \end{bmatrix} 
    = \begin{bmatrix}
        a_1 \\ a_2 \\ \vdots \\ a_n
    \end{bmatrix}
\end{align}
\begin{align}
    \boxed{\dfrac{\partial \mathbf{x}^\trans \mathbf{a}}{\partial \mathbf{x}} = \mathbf{a} \in \mathbb{C}^{n}}
\end{align}

\subsection{\(\dfrac{\partial \mathbf{a}^\hermit  \mathbf{x}}{\partial \mathbf{x}} = \mathbf{a}^*\)}

Let \(\mathbf{a, x} \in \mathbb{C}^{n}\), in which \(\mathbf{a}\) does not depend on \(\mathbf{x}\). Once again, we could say that
\begin{align}
    \dfrac{\partial \mathbf{a}^\hermit \mathbf{x}}{\partial \mathbf{x}} = {\mathbf{a}^\hermit}^\trans = \mathbf{a}^*
\end{align}

Nevertheless, here is the step-by-step:
\begin{align}
    \dfrac{\partial \mathbf{a}^\hermit \mathbf{x}}{\partial \mathbf{x}} &= \dfrac{\partial}{\partial \mathbf{x}} \left(
    \begin{bmatrix}
        a^*_1 & a^*_2 & \dots & a^*_n
    \end{bmatrix} \begin{bmatrix}
        x_{1} \\ x_{2} \\ \vdots \\ x_{n}
    \end{bmatrix} \right) 
    = \dfrac{\partial}{\partial \mathbf{x}} \left( \sum_{i = 1}^n a^*_ix_i \right) \\
\end{align}

Since a scalar-vector derivative is represented by a vector, we have that
\begin{align}
    \dfrac{\partial \mathbf{a}^\hermit \mathbf{x}}{\partial \mathbf{x}} &= \begin{bmatrix}
        \dfrac{\partial}{\partial x_1} \left( \sum_{i = 1}^n a^*_ix_i \right) \\ \dfrac{\partial}{\partial x_2} \left( \sum_{i = 1}^n a^*_ix_i \right) \\ \vdots \\ \dfrac{\partial}{\partial x_n} \left( \sum_{i = 1}^n a^*_ix_i \right) 
    \end{bmatrix}
    = \begin{bmatrix}
        a^*_1 \\ a^*_2 \\ \vdots \\ a^*_n
    \end{bmatrix}
\end{align}
\begin{align}
    \boxed{\dfrac{\partial \mathbf{a}^\hermit \mathbf{x}}{\partial \mathbf{x}} = \mathbf{a}^* \in \mathbb{C}^{n}}
\end{align}

\subsection{\(\dfrac{\partial \mathbf{x}^\hermit \mathbf{a}}{\partial \mathbf{x}} = \mathbf{0}\)}
Notice that \(\mathbf{x}^\hermit \mathbf{a} \neq \mathbf{a}^\hermit \mathbf{x}\). Therefore, we have no choice but derive it. Let \(\mathbf{a, x} \in \mathbb{C}^{n}\), in which \(\mathbf{a}\) does not depend on \(\mathbf{x}\), we have that
\begin{align}
    \dfrac{\partial \mathbf{x}^\hermit \mathbf{a}}{\mathbf{x}} & = \dfrac{\partial}{\mathbf{x}} \left(
    \begin{bmatrix}
        x^*_1 & x^*_2 & \dots & x^*_n
    \end{bmatrix} \begin{bmatrix}
        a_{1} \\ a_{2} \\ \vdots \\ a_{n}
    \end{bmatrix} \right) \\
    & = \dfrac{\partial}{\mathbf{x}} \left( \sum_{i = 1}^n x^*_ia_i \right) \\
    &= \begin{bmatrix}
            \dfrac{\partial}{\partial x_1} \left( \sum_{i = 1}^n x^*_ia_i \right) \\ \dfrac{\partial}{\partial x_2} \left( \sum_{i = 1}^n x^*_ia_i \right) \\ \vdots \\
            \dfrac{\partial}{\partial x_n} \left( \sum_{i = 1}^n x^*_ia_i \right)
        \end{bmatrix}.
\end{align}
By recalling that \(\dfrac{\partial x^*}{x} = 0\) \obs{reference required}, we have that
\begin{align}
    \dfrac{\partial \mathbf{x}^\hermit \mathbf{a}}{\mathbf{x}} = \begin{bmatrix}
        0 \\ 0 \\ \vdots \\ 0
    \end{bmatrix}
\end{align}
\begin{align}
    \boxed{\dfrac{\partial \mathbf{x}^\hermit \mathbf{a}}{\mathbf{x}} = \mathbf{0} \in \mathbb{C}^{n}}
\end{align}
where \(\mathbf{0}\) is the zero vector.

\subsection{\(\dfrac{\partial \mathbf{x}^\trans \mathbf{y}}{\partial \mathbf{z}} = \dfrac{\partial \mathbf{x}}{\partial \mathbf{z}}\mathbf{y} + \dfrac{\partial \mathbf{y}}{\partial \mathbf{z}}\mathbf{x}\)}
Let \(\mathbf{x},\mathbf{y} \in \mathbb{C}^{n}\) and \(\mathbf{z} \in \mathbb{C}^{m}\). Where \(\mathbf{x}\) and \(\mathbf{y}\) depend on \(\mathbf{z}\). Thus,
\begin{align}
    \dfrac{\partial \mathbf{x}^\trans \mathbf{y}}{\partial \mathbf{z}} & = \dfrac{\partial}{\partial \mathbf{z}}\sum_{i=1}^{n} y_ix_i \\
    & = \sum_{i=1}^{n} \dfrac{\partial y_ix_i}{\partial \mathbf{z}}.
\end{align}
Recalling that \((fg)' = f'g + g'f\), we have
\begin{align}
    \dfrac{\partial \mathbf{x}^\trans \mathbf{y}}{\partial \mathbf{z}} & = \sum_{i=1}^{n} x_i\dfrac{\partial y_i}{\partial \mathbf{z}} + \sum_{i=1}^{n} y_i\dfrac{\partial x_i}{\partial \mathbf{z}} \\
    & = \begin{bmatrix}
        \sum_{i=1}^{n} x_i\dfrac{\partial y_i}{\partial z_1} \\
        \sum_{i=1}^{n} x_i\dfrac{\partial y_i}{\partial z_2} \\
        \vdots \\
        \sum_{i=1}^{n} x_i\dfrac{\partial y_i}{\partial z_m} \\
    \end{bmatrix} +
    \begin{bmatrix}
        \sum_{i=1}^{n} y_i\dfrac{\partial x_i}{\partial z_1} \\
        \sum_{i=1}^{n} y_i\dfrac{\partial x_i}{\partial z_2} \\
        \vdots \\
        \sum_{i=1}^{n} y_i\dfrac{\partial x_i}{\partial z_m}
    \end{bmatrix} \\
    %
    & = \begin{bmatrix}
        \dfrac{\partial y_1}{\partial z_1} & \dfrac{\partial y_2}{\partial z_1} & \cdots & \dfrac{\partial y_n}{\partial z_1} \\
        \dfrac{\partial y_1}{\partial z_2} & \dfrac{\partial y_2}{\partial z_2} & \cdots & \dfrac{\partial y_n}{\partial z_2} \\
        \vdots & \ddots & \vdots & \vdots \\
        \dfrac{\partial y_1}{\partial z_m} & \dfrac{\partial y_2}{\partial z_m} & \cdots & \dfrac{\partial y_n}{\partial z_m} \\
    \end{bmatrix} \mathbf{x} \\
    & \hspace{2.5ex} + \begin{bmatrix}
        \dfrac{\partial x_1}{\partial z_1} & \dfrac{\partial x_2}{\partial z_1} & \cdots & \dfrac{\partial x_n}{\partial z_1} \\
        \dfrac{\partial x_1}{\partial z_2} & \dfrac{\partial x_2}{\partial z_2} & \cdots & \dfrac{\partial x_n}{\partial z_2} \\
        \vdots & \ddots & \vdots & \vdots \\
        \dfrac{\partial x_1}{\partial z_m} & \dfrac{\partial x_2}{\partial z_m} & \cdots & \dfrac{\partial x_n}{\partial z_m} \\
    \end{bmatrix} \mathbf{y}
\end{align}
\begin{align}
    \boxed{\dfrac{\partial \mathbf{x}^\trans \mathbf{y}}{\partial \mathbf{z}} = \dfrac{\partial \mathbf{x}}{\partial \mathbf{z}}\mathbf{y} + \dfrac{\partial \mathbf{y}}{\partial \mathbf{z}}\mathbf{x} \in \mathbb{C}^{m}}
\end{align}
Note that, if either \(\mathbf{x}\) or \(\mathbf{y}\) does not depend on \(\mathbf{z}\), just disregard \(\dfrac{\partial \mathbf{x}}{\partial \mathbf{z}}\mathbf{y}\) or \(\dfrac{\partial \mathbf{y}}{\partial \mathbf{z}}\mathbf{x}\), respectively. When neither depends on \(\mathbf{z}\), the obvious result is the zero vector, \(\mathbf{0}\).

\subsection{\(\dfrac{\partial \mathbf{x}^\trans \mathbf{x}}{\partial \mathbf{x}} = 2\mathbf{x}\)}
Let \(\mathbf{x} \in \mathbb{C}^{n}\) and \(\mathbf{z}\in \mathbb{C}^m\), where \(\mathbf{x}\) depends on \(\mathbf{z}\). Thus,
\begin{align}
    \dfrac{\partial \mathbf{x}^\trans \mathbf{x}}{\partial \mathbf{x}} & = \dfrac{\partial}{\partial \mathbf{x}}\sum_{i=1}^{n} x_i^2 \\
    & = \sum_{i=1}^{n} \dfrac{\partial x_i^2}{\partial \mathbf{x}} \\
    %
    & = \renewcommand{\arraystretch}{2.2}\begin{bmatrix}
        \sum_{i=1}^{n} \dfrac{\partial x_i^2}{\partial x_1} \\
        \sum_{i=1}^{n} \dfrac{\partial x_i^2}{\partial x_2} \\
        \vdots \\
        \sum_{i=1}^{n} \dfrac{\partial x_i^2}{\partial x_m} \\
    \end{bmatrix} \\
    %
    & = \renewcommand{\arraystretch}{2.2}\begin{bmatrix}
        2x_1 \\
        2x_2 \\
        \vdots \\
        2x_n \\
    \end{bmatrix}
\end{align}
\begin{align}
    \boxed{\dfrac{\partial \mathbf{x}^\trans \mathbf{x}}{\partial \mathbf{x}} = 2\mathbf{x} \in \mathbb{C}^n}
\end{align}

Note that this perfectly matches with the derivate of a quadratic scalar value, i.e., \(\frac{\diff x^2}{\diff x} = 2x\).

\subsection{\(\dfrac{\partial \mathbf{x}^\trans \mathbf{x}}{\partial \mathbf{z}} = 2\dfrac{\partial \mathbf{x}}{\partial \mathbf{z}}\mathbf{x}\)}
Let \(\mathbf{x} \in \mathbb{C}^{n}\) and \(\mathbf{z}\in \mathbb{C}^m\), where \(\mathbf{x}\) depends on \(\mathbf{z}\). Thus,
\begin{align}
    \dfrac{\partial \mathbf{x}^\trans \mathbf{x}}{\partial \mathbf{z}} & = \dfrac{\partial}{\partial \mathbf{z}}\sum_{i=1}^{n} x_i^2 \\
    & = \sum_{i=1}^{n} \dfrac{\partial x_i^2}{\partial \mathbf{z}} \\
    %
    & = \renewcommand{\arraystretch}{2.2}\begin{bmatrix}
        \sum_{i=1}^{n} \dfrac{\partial x_i^2}{\partial z_1} \\
        \sum_{i=1}^{n} \dfrac{\partial x_i^2}{\partial z_2} \\
        \vdots \\
        \sum_{i=1}^{n} \dfrac{\partial x_i^2}{\partial z_m} \\
    \end{bmatrix} \\
    %
    & = \renewcommand{\arraystretch}{2.2}\begin{bmatrix}
        \sum_{i=1}^{n} 2x_i\dfrac{\partial x_i}{\partial z_1} \\
        \sum_{i=1}^{n} 2x_i\dfrac{\partial x_i}{\partial z_2} \\
        \vdots \\
        \sum_{i=1}^{n} 2x_i\dfrac{\partial x_i}{\partial z_m} \\
    \end{bmatrix} \\
    & = 2 \renewcommand{\arraystretch}{2.2}\begin{bmatrix}
        \dfrac{\partial x_1}{\partial \mathbf{z}} & \dfrac{\partial x_2}{\partial \mathbf{z}} & \cdots & \dfrac{\partial x_n}{\partial \mathbf{z}}
    \end{bmatrix} \mathbf{x}
\end{align}
\begin{align}
    \boxed{\dfrac{\partial \mathbf{x}^\trans \mathbf{x}}{\partial \mathbf{z}} = 2\dfrac{\partial \mathbf{x}}{\partial \mathbf{z}}\mathbf{x} \in \mathbb{C}^m}
\end{align}

Note that this solution could also be solved by the chain rule (c.f. Equation \eqref{eq:chain-1inter}) as follows
\begin{align}
    \boxed{\dfrac{\partial \mathbf{x}^\trans \mathbf{x}}{\partial \mathbf{z}} = \dfrac{\partial \mathbf{x}}{\partial \mathbf{z}} \dfrac{\partial \mathbf{x}^\trans \mathbf{x}}{\partial \mathbf{x}} = 2\dfrac{\partial \mathbf{x}}{\partial \mathbf{z}}\mathbf{x} \in \mathbb{C}^m}
\end{align}

\subsection{\(\dfrac{\partial \mathbf{x}^\trans \mathbf{A} \mathbf{x}}{\partial \mathbf{x}} = \left(\mathbf{A}^\trans + \mathbf{A}\right) \mathbf{x}\)}
Let \(\mathbf{A}\in \mathbb{C}^{n\times n}\) and \(\mathbf{x} \in \mathbb{C}^{n}\), in which \(\mathbf{A}\) does not depend on \(\mathbf{x}\). For the quadratic form, it follows that
\begin{align}
    \dfrac{\partial \mathbf{x}^\trans \mathbf{A} \mathbf{x}}{\partial \mathbf{x}} &= \dfrac{\partial}{\partial \mathbf{x}} \left(
    \begin{bmatrix}
        x_{1} & x_{2} & \dots & x_{n}
    \end{bmatrix}
    \begin{bmatrix}
        a_{11} & a_{12} & \dots & a_{1n} \\
        a_{21} & a_{22} & \dots & a_{2n} \\
        \vdots & \vdots & \ddots & \vdots \\
        a_{n1} & a_{n2} & \dots & a_{nn} \\
    \end{bmatrix} \begin{bmatrix}
        x_{1} \\ x_{2} \\ \vdots \\ x_{n}
    \end{bmatrix} \right) \\
    &= \dfrac{\partial}{\partial \mathbf{x}} \left(
			\begin{bmatrix}
				\displaystyle \sum_{i = 1}^{n} x_{i}a_{i1} & 
				\displaystyle \sum_{i = 1}^{n} x_{i}a_{i2} & 
				\dots & 
				\displaystyle \sum_{i = 1}^{n} x_{i}a_{in}
			\end{bmatrix} \begin{bmatrix}
				x_{1} \\ x_{2} \\ \vdots \\ x_{n}
			\end{bmatrix} \right) \\
            &= \dfrac{\partial}{\partial \mathbf{x}} \left(
				\sum_{i = 1}^{n}\sum_{j = 1}^{n} a_{ij} x_{i} x_{j}
			\right).
\end{align}

Note that the element inside the parentheses is a scalar and that a scalar-vector derivative results in a vector, that is,
\begin{align}
    \dfrac{\partial \mathbf{x}^\trans \mathbf{A} \mathbf{x}}{\partial \mathbf{x}} &= \begin{bmatrix}
        \displaystyle \dfrac{\partial}{\partial x_1} \left( \sum_{i = 1}^{n}\sum_{j = 1}^{n} x_{i} a_{ij} x_{j} \right) \\ 
        \displaystyle  \dfrac{\partial}{\partial x_2} \left( \sum_{i = 1}^{n}\sum_{j = 1}^{n} x_{i} a_{ij} x_{j} \right) \\ 
        \vdots \\ 
        \displaystyle \dfrac{\partial}{\partial x_n} \left( \sum_{i = 1}^{n}\sum_{j = 1}^{n} x_{i} a_{ij} x_{j} \right) 
    \end{bmatrix} \\
    & = \begin{bmatrix}
        \displaystyle 2x_1a_{11} + \sum_{\substack{j = 1 \\ j \neq 1}}^{n} a_{1j} x_{j} + \sum_{\substack{i = 1 \\ i \neq 1}}^{n} a_{i1} x_{i} \\
        \displaystyle 2x_2a_{22} + \sum_{\substack{j = 1 \\ j \neq 2}}^{n} a_{2j} x_{j} + \sum_{\substack{i = 1 \\ i \neq 2}}^{n} a_{i2} x_{i} \\
        \vdots \\
        \displaystyle 2x_na_{nn} + \sum_{\substack{j = 1 \\ j \neq n}}^{n} a_{nj} x_{j} + \sum_{\substack{i = 1 \\ i \neq n}}^{n} a_{in} x_{i} 
    \end{bmatrix} \\
    &= \begin{bmatrix}
        \displaystyle \sum_{j = 1}^{n} a_{1j} x_{j} \\
        \displaystyle \sum_{j = 1}^{n} a_{2j} x_{j} \\
        \vdots \\
        \displaystyle \sum_{j = 1}^{n} a_{nj} x_{j} 
    \end{bmatrix} +
    \begin{bmatrix}
        \displaystyle \sum_{i = 1}^{n} a_{i1} x_{i} \\
        \displaystyle \sum_{i = 1}^{n} a_{i2} x_{i} \\
        \vdots \\
        \displaystyle \sum_{i = 1}^{n} a_{in} x_{i} 
    \end{bmatrix} \\
    & = \mathbf{A}^\trans \mathbf{x} + \mathbf{A} \mathbf{x}
\end{align}
\begin{align}
    \label{eq:quadratic-solution}
    \boxed{\dfrac{\partial \mathbf{x}^\trans \mathbf{A} \mathbf{x}}{\partial \mathbf{x}} = \left(\mathbf{A}^\trans + \mathbf{A}\right) \mathbf{x} \in \mathbb{C}^{n}}
\end{align}
For the special case where \(\mathbf{A}\) is symmetric, we obtain
\begin{align}
    \boxed{\dfrac{\partial \mathbf{x}^\trans \mathbf{A} \mathbf{x}}{\partial \mathbf{x}} = 2\mathbf{A} \mathbf{x} \in \mathbb{C}^{n}}
\end{align}

\subsection{\(\dfrac{\partial \mathbf{x}^\trans \mathbf{A} \mathbf{x}}{\partial \mathbf{z}} = \dfrac{\partial \mathbf{x}}{\partial \mathbf{z}}\left( \mathbf{A} + \mathbf{A}^\trans \right) \mathbf{x}\)}
Let \(\mathbf{z} \in \mathbb{C}^{m}, \mathbf{x} \in \mathbb{C}^{n}\) and \(\mathbf{A}\in \mathbb{C}^{n\times n}\), where \(\mathbf{x}\) depends on \(\mathbf{z}\), but \(\mathbf{A}\) does not. For the quadratic form, it follows that
\begin{align}
    \dfrac{\partial \mathbf{x}^\trans \mathbf{A} \mathbf{x}}{\partial \mathbf{z}} &= \dfrac{\partial}{\partial \mathbf{z}} \left(
    \begin{bmatrix}
        x_{1} & x_{2} & \dots & x_{n}
    \end{bmatrix}
    \begin{bmatrix}
        a_{11} & a_{12} & \dots & a_{1n} \\
        a_{21} & a_{22} & \dots & a_{2n} \\
        \vdots & \vdots & \ddots & \vdots \\
        a_{n1} & a_{n2} & \dots & a_{nn} \\
    \end{bmatrix} \begin{bmatrix}
        x_{1} \\ x_{2} \\ \vdots \\ x_{n}
    \end{bmatrix} \right) \\
    &= \dfrac{\partial}{\partial \mathbf{z}} \left(
			\begin{bmatrix}
				\displaystyle \sum_{i = 1}^{n} x_{i}a_{i1} & 
				\displaystyle \sum_{i = 1}^{n} x_{i}a_{i2} & 
				\dots & 
				\displaystyle \sum_{i = 1}^{n} x_{i}a_{in}
			\end{bmatrix} \begin{bmatrix}
				x_{1} \\ x_{2} \\ \vdots \\ x_{n}
			\end{bmatrix} \right) \\
            &= \dfrac{\partial}{\partial \mathbf{z}} \left(
				\sum_{i = 1}^{n}\sum_{j = 1}^{n} a_{ij} x_{i} x_{j}
			\right) \\
    &= \begin{bmatrix}
        \displaystyle \dfrac{\partial}{\partial z_1} \left( \sum_{i = 1}^{n}\sum_{j = 1}^{n} x_{i} a_{ij} x_{j} \right) \\ 
        \displaystyle  \dfrac{\partial}{\partial z_2} \left( \sum_{i = 1}^{n}\sum_{j = 1}^{n} x_{i} a_{ij} x_{j} \right) \\ 
        \vdots \\ 
        \displaystyle \dfrac{\partial}{\partial z_n} \left( \sum_{i = 1}^{n}\sum_{j = 1}^{n} x_{i} a_{ij} x_{j} \right) 
    \end{bmatrix} \\
    &= \begin{bmatrix}
        \displaystyle \sum_{i = 1}^{n}\sum_{j = 1}^{n} a_{ij} \dfrac{\partial x_{i}x_{j}}{\partial z_1} \\ 
        \displaystyle \sum_{i = 1}^{n}\sum_{j = 1}^{n} a_{ij} \dfrac{\partial x_{i}x_{j}}{\partial z_2} \\ 
        \vdots \\ 
        \displaystyle \sum_{i = 1}^{n}\sum_{j = 1}^{n} a_{ij} \dfrac{\partial x_{i}x_{j}}{\partial z_n} 
    \end{bmatrix}
\end{align}

Recalling that \((fg)' = f'g + g'f\), we have that
\begin{align}
    \dfrac{\partial \mathbf{x}^\trans \mathbf{A} \mathbf{x}}{\partial \mathbf{z}} &= \begin{bmatrix}
        \displaystyle \sum_{i = 1}^{n}\sum_{j = 1}^{n} x_{j}a_{ij} \dfrac{\partial x_{i}}{\partial z_1} +
        \sum_{i = 1}^{n}\sum_{j = 1}^{n} x_{i}a_{ij} \dfrac{\partial x_{j}}{\partial z_1} \\ 
        \displaystyle \sum_{i = 1}^{n}\sum_{j = 1}^{n} x_{j}a_{ij} \dfrac{\partial x_{i}}{\partial z_2} +
        \sum_{i = 1}^{n}\sum_{j = 1}^{n} x_{i}a_{ij} \dfrac{\partial x_{j}}{\partial z_2} \\ 
        \vdots \\ 
        \displaystyle \sum_{i = 1}^{n}\sum_{j = 1}^{n} x_{j}a_{ij} \dfrac{\partial x_{i}}{\partial z_n} +
        \sum_{i = 1}^{n}\sum_{j = 1}^{n} x_{i}a_{ij} \dfrac{\partial x_{j}}{\partial z_n} 
    \end{bmatrix} \\
    &= \begin{bmatrix}
        \displaystyle \sum_{i = 1}^{n}\sum_{j = 1}^{n} x_{j}a_{ij} \dfrac{\partial x_{i}}{\partial z_1} \\
        \displaystyle \sum_{i = 1}^{n}\sum_{j = 1}^{n} x_{j}a_{ij} \dfrac{\partial x_{i}}{\partial z_2} \\
        \vdots \\ 
        \displaystyle \sum_{i = 1}^{n}\sum_{j = 1}^{n} x_{j}a_{ij} \dfrac{\partial x_{i}}{\partial z_n}
    \end{bmatrix} +
    \begin{bmatrix}
        \displaystyle \sum_{i = 1}^{n}\sum_{j = 1}^{n} x_{i}a_{ij} \dfrac{\partial x_{j}}{\partial z_1} \\ 
        \displaystyle \sum_{i = 1}^{n}\sum_{j = 1}^{n} x_{i}a_{ij} \dfrac{\partial x_{j}}{\partial z_2} \\ 
        \vdots \\ 
        \displaystyle \sum_{i = 1}^{n}\sum_{j = 1}^{n} x_{i}a_{ij} \dfrac{\partial x_{j}}{\partial z_n} 
    \end{bmatrix} \\
    &= \begin{bmatrix}
       \dfrac{\partial x_{1}}{\partial \mathbf{z}} & \dfrac{\partial x_{2}}{\partial \mathbf{z}} & \cdots  & \dfrac{\partial x_{n}}{\partial \mathbf{z}}
    \end{bmatrix}
    \begin{bmatrix}
        a_{11} & a_{12} & \dots & a_{1n} \\
        a_{21} & a_{22} & \dots & a_{2n} \\
        \vdots & \vdots & \ddots & \vdots \\
        a_{m1} & a_{m2} & \dots & a_{mn} \\
    \end{bmatrix}
    \begin{bmatrix}
        x_{1} \\ x_{2} \\ \vdots \\ x_{n}
    \end{bmatrix} + \nonumber \\
    & \hspace{2.5ex} \begin{bmatrix}
        \dfrac{\partial x_{1}}{\partial \mathbf{z}} & \dfrac{\partial x_{2}}{\partial \mathbf{z}} & \cdots  & \dfrac{\partial x_{n}}{\partial \mathbf{z}}
     \end{bmatrix}
     \begin{bmatrix}
         a_{11} & a_{21} & \dots & a_{m1} \\
         a_{12} & a_{22} & \dots & a_{m2} \\
         \vdots & \vdots & \ddots & \vdots \\
         a_{1n} & a_{2n} & \dots & a_{mn} \\
     \end{bmatrix}
     \begin{bmatrix}
         x_{1} \\ x_{2} \\ \vdots \\ x_{n}
     \end{bmatrix} \\
     & = \dfrac{\partial \mathbf{x}}{\partial \mathbf{z}}\mathbf{A} \mathbf{x} + \dfrac{\partial \mathbf{x}}{\partial \mathbf{z}}\mathbf{A}^\trans \mathbf{x}
\end{align}
\begin{align}
    \boxed{\dfrac{\partial \mathbf{x}^\trans \mathbf{A} \mathbf{x}}{\partial \mathbf{z}} = \dfrac{\partial \mathbf{x}}{\partial \mathbf{z}}\left( \mathbf{A} + \mathbf{A}^\trans \right) \mathbf{x} \in \mathbb{C}^{m}}
\end{align}

For the special case where \(\mathbf{A}\) is symmetric, we get
\begin{align}
    \boxed{\dfrac{\partial \mathbf{x}^\trans \mathbf{A} \mathbf{x}}{\partial \mathbf{z}} = 2\dfrac{\partial \mathbf{x}}{\partial \mathbf{z}}\mathbf{A} \mathbf{x} \in \mathbb{C}^{m}}
\end{align}

Note that the solution is much easier if we maintain the matrix calculus notation and apply the chain rule, that is,
\begin{align}
    \dfrac{\partial \mathbf{x}^\trans \mathbf{A} \mathbf{x}}{\partial \mathbf{z}} = \dfrac{\partial \mathbf{x}}{\partial \mathbf{z}} \dfrac{\partial \mathbf{x}^\trans \mathbf{A} \mathbf{x}}{\partial \mathbf{x}} = \dfrac{\partial \mathbf{x}}{\partial \mathbf{z}} \left(\mathbf{A}^\trans + \mathbf{A}\right) \mathbf{x},
\end{align}
where the last equality comes from Equation \eqref{eq:quadratic-solution}.

\subsection{\(\dfrac{\partial \mathbf{b}^\trans \mathbf{A} \mathbf{x}}{\partial \mathbf{x}} = \mathbf{A}^\trans \mathbf{b}\)} \label{sec:bt-A-x}
Let \(\mathbf{x} \in \mathbb{C}^{n}\), \(\mathbf{b} \in \mathbb{C}^{m}\) and \(\mathbf{A}\in \mathbb{C}^{m\times n}\), where neither \(\mathbf{b}\) nor \(\mathbf{A}\) depend on \(\mathbf{x}\). It follows that

\begin{align}
    \dfrac{\partial \mathbf{b}^\trans \mathbf{A} \mathbf{x}}{\partial \mathbf{x}} &= \dfrac{\partial}{\partial \mathbf{x}} \left(
    \begin{bmatrix}
        b_{1} & b_{2} & \dots & b_{m}
    \end{bmatrix}
    \begin{bmatrix}
        a_{11} & a_{12} & \dots & a_{1n} \\
        a_{21} & a_{22} & \dots & a_{2n} \\
        \vdots & \vdots & \ddots & \vdots \\
        a_{m1} & a_{m2} & \dots & a_{mn} \\
    \end{bmatrix} \begin{bmatrix}
        x_{1} \\ x_{2} \\ \vdots \\ x_{n}
    \end{bmatrix} \right) \\
    &= \dfrac{\partial}{\partial \mathbf{x}} \left(
			\begin{bmatrix}
				\displaystyle \sum_{i = 1}^{m} a_{i1}b_{i} & 
				\displaystyle \sum_{i = 1}^{m} a_{i2}b_{i} & 
				\dots & 
				\displaystyle \sum_{i = 1}^{m} a_{in}b_{i}
			\end{bmatrix} \begin{bmatrix}
				x_{1} \\ x_{2} \\ \vdots \\ x_{n}
			\end{bmatrix} \right) \\
    &= \dfrac{\partial}{\partial \mathbf{x}} \left(
        \sum_{i = 1}^{m}\sum_{j = 1}^{n} a_{ij} b_{i} x_{j}
    \right) \\
    &= \sum_{i = 1}^{m}\sum_{j = 1}^{n} a_{ij} b_{i} \dfrac{\partial x_{j}}{\partial \mathbf{x}} \\
    &= \begin{bmatrix}
        \sum_{i = 1}^{m} a_{i1} b_{i} \\
        \sum_{i = 1}^{m} a_{i2} b_{i} \\
        \vdots \\
        \sum_{i = 1}^{m} a_{in} b_{i} \\
    \end{bmatrix} \\
    &= \begin{bmatrix}
        a_{11} & a_{21} & \dots & a_{m1} \\
        a_{12} & a_{22} & \dots & a_{m2} \\
        \vdots & \vdots & \ddots & \vdots \\
        a_{1n} & a_{2n} & \dots & a_{mn} \\
    \end{bmatrix}
    \begin{bmatrix}
        b_1 \\
        b_2 \\
        \vdots \\
        b_m
    \end{bmatrix}
\end{align}
\begin{align}
    \boxed{\dfrac{\partial \mathbf{b}^\trans \mathbf{A} \mathbf{x}}{\partial \mathbf{x}} = \mathbf{A}^\trans \mathbf{b} \in \mathbb{C}^{n}}
\end{align}

Note that this solution could solve by simply observing that \(\mathbf{\mathbf{b}^\trans \mathbf{A}}\) is actually a linear transformation from \(\mathbb{R}^{n}\) to \(\mathbb{R}\). Thus,
\begin{align}
    \dfrac{\partial \mathbf{b}^\trans \mathbf{A} \mathbf{x}}{\partial \mathbf{x}} = \left( \mathbf{b}^\trans \mathbf{A} \right)^{\trans} = \mathbf{A}^\trans \mathbf{b},
\end{align}
where the first equality comes from the Equation \eqref{eq:lt-slution}.

\subsection{\(\dfrac{\partial \mathbf{x}^\trans \mathbf{A} \mathbf{b}}{\partial \mathbf{x}} = \mathbf{Ab}\)}
Let \(\mathbf{x} \in \mathbb{C}^{m}\), \(\mathbf{b} \in \mathbb{C}^{n}\) and \(\mathbf{A}\in \mathbb{C}^{m\times n}\), where neither \(\mathbf{b}\) nor \(\mathbf{A}\) depend on \(\mathbf{x}\). The quickest way to solve it is to note that \(\mathbf{x}^\trans \mathbf{A} \mathbf{b} =  \mathbf{b}^\trans \mathbf{A}^\trans \mathbf{x}\), which is the problem solved by the Section \ref{sec:bt-A-x}. Thus,
\begin{align}
    \dfrac{\partial \mathbf{x}^\trans \mathbf{A} \mathbf{b}}{\partial \mathbf{x}} = \dfrac{\partial \mathbf{b}^\trans \mathbf{A}^\trans \mathbf{x}}{\partial \mathbf{x}} = \left( \mathbf{b}^\trans \mathbf{A}^\trans \right)^{\trans} = \mathbf{A} \mathbf{b},
\end{align}
where the second equality comes from Equation \eqref{eq:lt-slution}. Nevertheless, here is the step-by-step
\begin{align}
    \dfrac{\partial \mathbf{x}^\trans \mathbf{A} \mathbf{b}}{\partial \mathbf{x}} &= \dfrac{\partial}{\partial \mathbf{x}} \left(
    \begin{bmatrix}
        x_{1} & x_{2} & \dots & x_{m}
    \end{bmatrix}
    \begin{bmatrix}
        a_{11} & a_{12} & \dots & a_{1n} \\
        a_{21} & a_{22} & \dots & a_{2n} \\
        \vdots & \vdots & \ddots & \vdots \\
        a_{m1} & a_{m2} & \dots & a_{mn} \\
    \end{bmatrix} \begin{bmatrix}
        b_{1} \\ b_{2} \\ \vdots \\ b_{n}
    \end{bmatrix} \right) \\
    &= \dfrac{\partial}{\partial \mathbf{x}} \left(
			\begin{bmatrix}
				\displaystyle \sum_{i = 1}^{m} a_{i1}x_{i} & 
				\displaystyle \sum_{i = 1}^{m} a_{i2}x_{i} & 
				\dots & 
				\displaystyle \sum_{i = 1}^{m} a_{in}x_{i}
			\end{bmatrix} \begin{bmatrix}
				b_{1} \\ b_{2} \\ \vdots \\ b_{n}
			\end{bmatrix} \right) \\
    &= \dfrac{\partial}{\partial \mathbf{x}} \left(
        \sum_{i = 1}^{m}\sum_{j = 1}^{n} a_{ij} b_{j} x_{i}
    \right) \\
    &= \sum_{i = 1}^{m}\sum_{j = 1}^{n} a_{ij} b_{j} \dfrac{\partial x_{i}}{\partial \mathbf{x}} \\
    &= \begin{bmatrix}
        \sum_{j = 1}^{n} a_{1j} b_{j} \\
        \sum_{j = 1}^{n} a_{2j} b_{j} \\
        \vdots \\
        \sum_{j = 1}^{n} a_{nj} b_{j} \\
    \end{bmatrix} \\
    &= \begin{bmatrix}
        a_{11} & a_{12} & \dots & a_{1n} \\
        a_{21} & a_{22} & \dots & a_{2n} \\
        \vdots & \vdots & \ddots & \vdots \\
        a_{m1} & a_{m2} & \dots & a_{mn} \\
    \end{bmatrix}
    \begin{bmatrix}
        b_1 \\
        b_2 \\
        \vdots \\
        b_n
    \end{bmatrix}
\end{align}
\begin{align}
    \boxed{\dfrac{\partial \mathbf{x}^\trans \mathbf{A} \mathbf{b}}{\partial \mathbf{x}} = \mathbf{Ab} \in \mathbb{C}^{m}}
\end{align}


\subsection{\(\dfrac{\partial \mathbf{y}^\trans \mathbf{A} \mathbf{x}}{\partial \mathbf{z}} = \dfrac{\partial \mathbf{x}}{\partial \mathbf{z}} \mathbf{A}^{\trans} \mathbf{y} + \dfrac{\partial \mathbf{y}}{\partial \mathbf{z}} \mathbf{A} \mathbf{x}\)}
Let \(\mathbf{x} \in \mathbb{C}^{n}\), \(\mathbf{y} \in \mathbb{C}^{m}\), \(\mathbf{z} \in \mathbb{C}^{p}\), and \(\mathbf{A}\in \mathbb{C}^{m\times n}\), where \(\mathbf{x}\) and \(\mathbf{y}\) depend on \(\mathbf{z}\), but \(\mathbf{A}\) does not. Therefore,
\begin{align}
    \dfrac{\partial \mathbf{y}^\trans \mathbf{A} \mathbf{x}}{\partial \mathbf{z}} &= \dfrac{\partial}{\partial \mathbf{z}} \left(
    \begin{bmatrix}
        y_{1} & y_{2} & \dots & y_{m}
    \end{bmatrix}
    \begin{bmatrix}
        a_{11} & a_{12} & \dots & a_{1n} \\
        a_{21} & a_{22} & \dots & a_{2n} \\
        \vdots & \vdots & \ddots & \vdots \\
        a_{m1} & a_{m2} & \dots & a_{mn} \\
    \end{bmatrix} \begin{bmatrix}
        x_{1} \\ x_{2} \\ \vdots \\ x_{n}
    \end{bmatrix} \right) \\
    &= \dfrac{\partial}{\partial \mathbf{z}} \left(
			\begin{bmatrix}
				\displaystyle \sum_{i = 1}^{m} a_{i1}y_{i} & 
				\displaystyle \sum_{i = 1}^{m} a_{i2}y_{i} & 
				\dots & 
				\displaystyle \sum_{i = 1}^{m} a_{in}y_{i}
			\end{bmatrix} \begin{bmatrix}
				x_{1} \\ x_{2} \\ \vdots \\ x_{n}
			\end{bmatrix} \right) \\
    &= \dfrac{\partial}{\partial \mathbf{z}} \left(
        \sum_{i = 1}^{m}\sum_{j = 1}^{n} a_{ij} x_{j} y_{i}
    \right) \\
    &= \sum_{i = 1}^{m}\sum_{j = 1}^{n} a_{ij} \dfrac{\partial y_{i}x_{j}}{\partial \mathbf{z}} %\\
    % &= \begin{bmatrix}
    %     \sum_{j = 1}^{n} a_{1j} b_{j} \\
    %     \sum_{j = 1}^{n} a_{2j} b_{j} \\
    %     \vdots \\
    %     \sum_{j = 1}^{n} a_{nj} b_{j} \\
    % \end{bmatrix} \\
    % &= \begin{bmatrix}
    %     a_{11} & a_{12} & \dots & a_{1n} \\
    %     a_{21} & a_{22} & \dots & a_{2n} \\
    %     \vdots & \vdots & \ddots & \vdots \\
    %     a_{m1} & a_{m2} & \dots & a_{mn} \\
    % \end{bmatrix}
    % \begin{bmatrix}
    %     b_1 \\
    %     b_2 \\
    %     \vdots \\
    %     b_n
    % \end{bmatrix}
\end{align}
Recalling that \((fg)' = f'g + g'f\), we have that
\begin{align}
    \dfrac{\partial \mathbf{y}^\trans \mathbf{A} \mathbf{x}}{\partial \mathbf{z}} & = \sum_{i = 1}^{m}\sum_{j = 1}^{n} a_{ij} y_{i} \dfrac{\partial x_{j}}{\partial \mathbf{z}} + \sum_{i = 1}^{m}\sum_{j = 1}^{n} a_{ij} x_{j} \dfrac{\partial y_{i}}{\partial \mathbf{z}} \\
    & = \begin{bmatrix}
        \dfrac{\partial x_1}{\partial \mathbf{z}} & \dfrac{\partial x_2}{\partial \mathbf{z}} & \cdots & \dfrac{\partial x_n}{\partial \mathbf{z}}
    \end{bmatrix}
    \begin{bmatrix}
        a_{11} & a_{21} & \dots & a_{m1} \\
        a_{12} & a_{22} & \dots & a_{m2} \\
        \vdots & \vdots & \ddots & \vdots \\
        a_{1n} & a_{2n} & \dots & a_{mn} \\
    \end{bmatrix}
    \begin{bmatrix}
        y_1 \\
        y_2 \\
        \vdots \\
        y_m
    \end{bmatrix} + \\
    & \hspace{3ex} \begin{bmatrix}
        \dfrac{\partial y_1}{\partial \mathbf{z}} & \dfrac{\partial y_2}{\partial \mathbf{z}} & \cdots & \dfrac{\partial y_m}{\partial \mathbf{z}}
    \end{bmatrix}
    \begin{bmatrix}
        a_{11} & a_{12} & \dots & a_{1n} \\
        a_{21} & a_{22} & \dots & a_{2n} \\
        \vdots & \vdots & \ddots & \vdots \\
        a_{m1} & a_{m2} & \dots & a_{mn} \\
    \end{bmatrix}
    \begin{bmatrix}
        x_1 \\
        x_2 \\
        \vdots \\
        x_n
    \end{bmatrix}
\end{align}
\begin{align}
    \boxed{\dfrac{\partial \mathbf{y}^\trans \mathbf{A} \mathbf{x}}{\partial \mathbf{z}} = \dfrac{\partial \mathbf{x}}{\partial \mathbf{z}} \mathbf{A}^{\trans} \mathbf{y} + \dfrac{\partial \mathbf{y}}{\partial \mathbf{z}} \mathbf{A} \mathbf{x}}
\end{align}

Even though this problem is trickier, we can find the same solution in a clever way by preserving the matrix calculus notation and applying the chain rule. Note that \(\mathbf{y}^\trans \mathbf{A} \mathbf{x}\) depends on both \(\mathbf{x}\) and \(\mathbf{y}\) which, in turn, depend on \(\mathbf{z}\). Therefore (c.f. Equation \eqref{eq:chain-multi-inter}),
\begin{align}
    \dfrac{\partial \mathbf{y}^\trans \mathbf{A} \mathbf{x}}{\partial \mathbf{z}} & = \dfrac{\partial \mathbf{x}}{\partial \mathbf{z}} \dfrac{\partial \mathbf{y}^\trans \mathbf{A} \mathbf{x}}{\partial \mathbf{x}} + \dfrac{\partial \mathbf{y}}{\partial \mathbf{z}} \dfrac{\partial \mathbf{y}^\trans \mathbf{A} \mathbf{x}}{\partial \mathbf{y}} \\
    & = \dfrac{\partial \mathbf{x}}{\partial \mathbf{z}} \mathbf{A}^{\trans} \mathbf{y} + \dfrac{\partial \mathbf{y}}{\partial \mathbf{z}} \mathbf{A} \mathbf{x},
\end{align}
where the second equality comes from previous solutions.

\subsection{\(\dfrac{\partial \textnormal{tr}\left(\mathbf{A} \mathbf{X}\right)}{\partial \mathbf{X}} = \mathbf{A}^\trans\)}

Let \(\mathbf{A}, \mathbf{X} \in \mathbb{R}^{n\times n}\), where \(\mathbf{A}\) does not depend on the elements in \(\mathbf{X}\).
\begin{align*}
    \dfrac{\partial \textnormal{tr}\left(\mathbf{A} \mathbf{X}\right)}{\partial \mathbf{X}} &= \dfrac{\partial}{\partial \mathbf{X}} \left( \tr{\begin{bmatrix}
        a_{11} & a_{12} & \dots & a_{1n} \\
        a_{21} & a_{22} & \dots & a_{2n} \\
        \vdots & \vdots & \ddots & \vdots \\
        a_{n1} & a_{n2} & \dots & a_{nn} \\
    \end{bmatrix}
    \begin{bmatrix}
        x_{11} & x_{12} & \dots & x_{1n} \\
        x_{21} & x_{22} & \dots & x_{2n} \\
        \vdots & \vdots & \ddots & \vdots \\
        x_{n1} & x_{n2} & \dots & x_{nn} \\
    \end{bmatrix} \right)} \\
    %
    &= \dfrac{\partial}{\partial \mathbf{X}} \left( \sum_{i = 1}^{n} \sum_{j = 1}^{n} a_{ij}x_{ji} \right) \\
    %
    &= \begin{bmatrix}
        \displaystyle \dfrac{\partial}{\partial x_{11}} \left( \sum_{i = 1}^{n} \sum_{j = 1}^{n} a_{ij}x_{ji} \right) & 
        \displaystyle \dfrac{\partial}{\partial x_{12}} \left( \sum_{i = 1}^{n} \sum_{j = 1}^{n} a_{ij}x_{ji} \right) & 
        \dots & 
        \displaystyle \dfrac{\partial}{\partial x_{1n}} \left( \sum_{i = 1}^{n} \sum_{j = 1}^{n} a_{ij}x_{ji} \right) \\
        \displaystyle \dfrac{\partial}{\partial x_{21}} \left( \sum_{i = 1}^{n} \sum_{j = 1}^{n} a_{ij}x_{ji} \right) & 
        \displaystyle \dfrac{\partial}{\partial x_{22}} \left( \sum_{i = 1}^{n} \sum_{j = 1}^{n} a_{ij}x_{ji} \right) & 
        \dots & 
        \displaystyle \dfrac{\partial}{\partial x_{2n}} \left( \sum_{i = 1}^{n} \sum_{j = 1}^{n} a_{ij}x_{ji} \right) \\
        \vdots & \vdots & \ddots & \vdots \\
        \displaystyle \dfrac{\partial}{\partial x_{n1}} \left( \sum_{i = 1}^{n} \sum_{j = 1}^{n} a_{ij}x_{ji} \right) & 
        \displaystyle \dfrac{\partial}{\partial x_{n2}} \left( \sum_{i = 1}^{n} \sum_{j = 1}^{n} a_{ij}x_{ji} \right) & 
        \dots & 
        \displaystyle \dfrac{\partial}{\partial x_{nn}} \left( \sum_{i = 1}^{n} \sum_{j = 1}^{n} a_{ij}x_{ji} \right) \\
    \end{bmatrix} \\
    &= \begin{bmatrix}
            a_{11} & a_{21} & \dots & a_{n1} \\
            a_{12} & a_{22} & \dots & a_{n2} \\
            \vdots & \vdots & \ddots & \vdots \\
            a_{1n} & a_{2n} & \dots & a_{nn} \\
        \end{bmatrix}
\end{align*}
\begin{align}
    \boxed{\dfrac{\partial \textnormal{tr}\left(\mathbf{A} \mathbf{X}\right)}{\partial \mathbf{X}} = \mathbf{A}^\trans}
\end{align}

I have no idea how to make this solution simpler.

\subsection{\(\dfrac{\partial \abs{\mathbf{X}}}{\partial \mathbf{X}} = \adj{\mathbf{X}}\)}
Let \(\mathbf{X} \in \mathbb{R}^{n\times n}\). Through Laplace expansion (cofactor expansion), we can rewrite the determinant of \(\mathbf{X}\) as the sum of the cofactors of any row or column, multiplied by its generating element, that is
\begin{align}
    \abs{\mathbf{X}} = \sum_{i = 1}^{n} x_{ki} \abs{\mathbf{C}_{ki}} = \sum_{i = 1}^{n} x_{ik} \abs{\mathbf{C}_{ik}} \,\,\,\,\,\, \forall \,\, k \in \left\{ 1, 2, ..., n \right\},
\end{align}
where \(\mathbf{C}_{ij}\) denotes the cofactor matrix of \(\mathbf{X}\) generated from element \(x_{ij}\). It is worth noting that the cofactor of \(\mathbf{C}_{ij}\) is independent of the value of any element \((i,j)\) in \(\mathbf{X}\). Therefore, it follows that
\begin{align}
    \dfrac{\partial \abs{\mathbf{X}}}{\partial \mathbf{X}} &= \dfrac{\partial}{\partial \mathbf{X}} \left( \sum_{i = 1}^{n} x_{ki} \abs{\mathbf{C}_{ki}} \right) \,\,\,\,\,\, \forall \,\, k \in \left\{ 1, 2, ..., n \right\} \\
    %
    & = \dfrac{\partial}{\partial \mathbf{X}} \left( \begin{bmatrix}
        \displaystyle \sum_{i = 1}^{n} x_{1i} \abs{\mathbf{C}_{1i}} & 
        \displaystyle \sum_{i = 1}^{n} x_{1i} \abs{\mathbf{C}_{1i}} & 
        \dots & 
        \displaystyle \sum_{i = 1}^{n} x_{1i} \abs{\mathbf{C}_{1i}} \\
        \displaystyle \sum_{i = 1}^{n} x_{2i} \abs{\mathbf{C}_{2i}} & 
        \displaystyle \sum_{i = 1}^{n} x_{2i} \abs{\mathbf{C}_{2i}} & 
        \dots & 
        \displaystyle \sum_{i = 1}^{n} x_{2i} \abs{\mathbf{C}_{2i}} \\
        \vdots & \vdots & \ddots & \vdots \\
        \displaystyle \sum_{i = 1}^{n} x_{ni} \abs{\mathbf{C}_{ni}} & 
        \displaystyle \sum_{i = 1}^{n} x_{ni} \abs{\mathbf{C}_{ni}} & 
        \dots & 
        \displaystyle \sum_{i = 1}^{n} x_{ni} \abs{\mathbf{C}_{ni}}
    \end{bmatrix} \right) \\
    %
    & = \begin{bmatrix}
        \displaystyle \dfrac{\partial}{\partial x_{11}} \left( \sum_{i = 1}^{n} x_{1i} \abs{\mathbf{C}_{1i}} \right) & 
        \displaystyle \dfrac{\partial}{\partial x_{12}} \left( \sum_{i = 1}^{n} x_{1i} \abs{\mathbf{C}_{1i}} \right) & 
        \dots & 
        \displaystyle \dfrac{\partial}{\partial x_{13}} \left( \sum_{i = 1}^{n} x_{1i} \abs{\mathbf{C}_{1i}} \right) \\
        \displaystyle \dfrac{\partial}{\partial x_{21}} \left( \sum_{i = 1}^{n} x_{2i} \abs{\mathbf{C}_{2i}} \right) & 
        \displaystyle \dfrac{\partial}{\partial x_{22}} \left( \sum_{i = 1}^{n} x_{2i} \abs{\mathbf{C}_{2i}} \right) & 
        \dots & 
        \displaystyle \dfrac{\partial}{\partial x_{33}} \left( \sum_{i = 1}^{n} x_{2i} \abs{\mathbf{C}_{2i}} \right) \\
        \vdots & \vdots & \ddots & \vdots \\
        \displaystyle \dfrac{\partial}{\partial x_{n1}} \left( \sum_{i = 1}^{n} x_{ni} \abs{\mathbf{C}_{ni}} \right) & 
        \displaystyle \dfrac{\partial}{\partial x_{n2}} \left( \sum_{i = 1}^{n} x_{ni} \abs{\mathbf{C}_{ni}} \right) & 
        \dots & 
        \displaystyle \dfrac{\partial}{\partial x_{n3}} \left( \sum_{i = 1}^{n} x_{ni} \abs{\mathbf{C}_{ni}} \right) \\
    \end{bmatrix} \\
    &= \begin{bmatrix}
        \abs{\mathbf{C}_{11}} & \abs{\mathbf{C}_{12}} & \dots & \abs{\mathbf{C}_{1n}} \\
        \abs{\mathbf{C}_{21}} & \abs{\mathbf{C}_{22}} & \dots & \abs{\mathbf{C}_{2n}} \\
        \vdots & \vdots & \ddots & \vdots \\
        \abs{\mathbf{C}_{n1}} & \abs{\mathbf{C}_{n2}} & \dots & \abs{\mathbf{C}_{nn}} \\
    \end{bmatrix}
\end{align}
\begin{align}
    \boxed{\dfrac{\partial \abs{\mathbf{X}}}{\partial \mathbf{X}} = \adj{\mathbf{X}}}
\end{align}

I have no idea how to make this solution simpler.

\subsection{\(\dfrac{\partial \mathbf{A}^{-1}}{\partial \alpha} = - {\left( \mathbf{A}^{-1} \right)}^{\trans} \dfrac{\partial \mathbf{A}}{\partial \alpha} {\left( \mathbf{A}^{-1} \right)}^{\trans}\)}
Let \(\mathbf{A}\in \mathbb{C}^{m\times n}\) and \(\alpha \in \mathbb{C}\). Remember that \(\mathbf{A}^{-1}\mathbf{A} = \mathbf{I}\). Differentiating both sides of this equation with respect to \(\alpha\), we get
\begin{align}
    \dfrac{\partial \mathbf{A}^{-1}\mathbf{A}}{\partial \alpha} & = \dfrac{\partial \mathbf{I}}{\partial \alpha} = \mathbf{0}_{m \times n},
\end{align}
where \(\mathbf{0}_{m \times n}\) is a zero matrix with dimension \(m \times n\). By applying the product rule of a matrix-matrix derivate, we get (c.f. Equation \eqref{eq:matrix-matrix-product-rule})
\begin{align}
    \dfrac{\partial \mathbf{A}^{-1}\mathbf{A}}{\partial \alpha} =  \dfrac{\partial \mathbf{A}}{\partial \alpha} \left( \mathbf{A}^{-1} \right)^{\trans} + \mathbf{A}^{\trans} \dfrac{\partial \mathbf{A}^{-1}}{\partial \alpha} & = \mathbf{0}_{m \times n}
\end{align}
Using the property \(\left( \mathbf{A}^{\trans} \right)^{-1} = \left( \mathbf{A}^{-1} \right)^{\trans}\) and rearranging this expression, we get
\begin{align}
    \boxed{\dfrac{\partial \mathbf{A}^{-1}}{\partial \alpha} = - {\left( \mathbf{A}^{-1} \right)}^{\trans} \dfrac{\partial \mathbf{A}}{\partial \alpha} {\left( \mathbf{A}^{-1} \right)}^{\trans}}
\end{align}

\nocite{*}
\printbibliography

\end{document}