\documentclass{article}

% redefine \maketitle
\makeatletter
\def\@maketitle{%
  \newpage
  \null
  \vskip 2em%
  \begin{center}%
  \let \footnote \thanks
    {\LARGE \@title \par}%
    \vskip 1.5em%
    {\large
      \lineskip .5em%
      \begin{tabular}[t]{c}%
        \@author\\
      \end{tabular}\par}%
    \vskip 1em%
    {\large {\tt Version:}\@date}%
  \end{center}%
  \par
  \vskip 1.5em}
\makeatother

% Language setting
% Replace `english' with e.g. `spanish' to change the document language
\usepackage[english]{babel}
\usepackage{float}
\usepackage{multirow}

% math
\usepackage{amsfonts}
\newcommand{\trans}{\top}
\newcommand{\hermit}{\mathsf{H}}
\newcommand{\tr}[1]{\ensuremath{\textnormal{tr}\left(#1\right)}} % trace
\newcommand{\adj}[1]{\ensuremath{\textnormal{adj}\left(#1\right)}} % adjoint
\newcommand{\obs}[1]{\textcolor{red}{(#1)}}
\newcommand*\diff{\mathop{}\!\mathrm{d}}

% |a| -> absolute value of a, which is a scalar
\newcommand\abs[1]{\left\lvert#1\right\rvert}

% Set page size and margins
% Replace `letterpaper' with`a4paper' for UK/EU standard size
\usepackage[letterpaper,top=2cm,bottom=2cm,left=3cm,right=3cm,marginparwidth=1.75cm]{geometry}

% Useful packages
\usepackage{amsmath,amssymb}
\usepackage{graphicx}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}

% title packages
\usepackage{authblk}

% bmatrix with additional gaps
\usepackage{tabstackengine}
\stackMath
\setstackgap{L}{30pt} % vertical gap
\setstacktabbedgap{10pt} % horizontal gap
\def\lrgap{\kern6pt}
\def\xbracketVectorstack#1{\left[\lrgap\Vectorstack{#1}\lrgap\right]} % use it for vectors
\def\xbracketMatrixstack#1{\left[\lrgap\tabbedCenterstack{#1}\lrgap\right]} % use it for matrices

% biblatex
\usepackage[backend=bibtex, sorting=none, style=numeric-comp, defernumbers=true]{biblatex} % using biblatex
\addbibresource{refs.bib} % add reference file
% for each cited reference create a category named "cited"

% strike out texts
\usepackage{soul}

% begin
\title{\textbf{The Guide for Matrix Calculus}  \vspace{-.3cm}}
\author{Rubem Vasconcelos Pacelli\\
  {\tt rubem.engenharia@gmail.com}}
\affil{Department of Teleinformatics Engineering, Federal University of Ceará.\\Fortaleza, Ceará, Brazil. \vspace{-.5cm}}

\begin{document}
\maketitle
\tableofcontents
\newpage
\section{Introduction}
Since my Master's degree, I've been struggling with matrix differentiation as I could not find good references that cover it nicely. The bibliographies I found at that time were books from Economics \cite{dhrymes1978mathematics}, but they use an \st{weird} unfamiliar notation.

After delving a lot, I finally found a good reference from Professor Randal's class note \cite{barnes2006matrix} (honorable mention for the Matrix Cookbook too \cite{petersen2008matrix}). However, to my surprise, when I tried to apply those matrix differentiation propositions, I got ``wrong'' answers! The truth is that \emph{matrix calculus notation is severely fragmented and there is no consensus among the researchers over which notation to follow}. Fortunately, there are two major ways to represent a derivative of a vector \cite{Singh}. If you do not select the author's representation, you will end up with the same result, but in a row vector\footnote{Although the expression ``row vector'' is quite common, I really advocate avoiding it since, once defined a vector as a column, \(\mathbf{y}^{\trans} \in \mathbb{C}^{1\times n}\) is actually a linear transformation from \(\mathbb{R}^{n}\) to \(\mathbb{R}\). That is, it has nothing to do with a vector, which is a numerical entity in a \(n\)-dimensional space. Therefore, throughout this note, I will refer to it as \(1\times n\) matrix.} instead of a column vector or vice-versa. For the cases where the resulting derivative is a matrix, you will get its transpose. The first representation is called Jacobian formulation or numerator layout, while the second one is called Hessian formulation or denominator layout. Nevertheless, even for the same layout, some conventions need to be stated as there are some disagreements.

Due to the lack of references and the need to have one, I decided to make this quick guide. The goal here is twofold: make a quick-but-comprehensive explanation of both representations, and derive the partial derivatives for the most common matrix calculus expressions you came across. In the end, I hope to have a consistent reference to guide me when writing my articles, or to help me when I need to understand an author's book that uses notations different from mine. The way I define in the Section \ref{sec:denominator} will be the way I going to adopt throughout my papers, but I will let you know when some point is not consensus among the authors. If you are only looking for a table of results, the most thorough I've seen so far is on Wikipedia \cite{Matrixca44:online}, but maybe it is not the most reliable source.

I will adopt the notation that most Engineers might be used to and only apply the Hessian formulation on the derivations since this is the notation I will adopt on my papers (it matches the derivative results I find in my books, too). If you are looking for the Jacobian formulation for the same derivations I do here, I highly recommend Professor Randal's class note, which uses this representation. The unique drawback is that he does not use complex numbers and has fewer derivations than mine.

Some of the differentiation solutions here were collected from class notes, while others I derived by myself. Obviously, this guide may have errors (I hope not). If you find it, feel free to reach me out through email or simply make a pull request on my \href{https://github.com/tapyu/courses/tree/main/matrix_diff_ref}{Github}.

\section{Notation and nomeclatures}
\label{sec:notation}

Let
\begin{align}
    \mathbf{A} = \begin{bmatrix}
        a_{11} & a_{12} & \dots & a_{1n} \\
        a_{21} & a_{22} & \dots & a_{2n} \\
        \vdots & \vdots & \ddots & \vdots \\
        a_{m1} & a_{m2} & \dots & a_{mn} \\
    \end{bmatrix} \in \mathbb{C}^{m \times n}
\end{align}
be a complex matrix with dimension equal to \(m \times n\), where \(a_{ij} \in \mathbb{C}\) is its element in the position \((i,j)\). Similarly, a complex vector is defined by
\begin{align}
    \mathbf{x} = \begin{bmatrix}
        x_1 \\
        x_2 \\
        \vdots \\
        x_n
    \end{bmatrix}  \in \mathbb{C}^{n},
\end{align}
which may also be denoted as an \(n\)-tuple, \((x_1, x_2, \dots, x_n)\), when more convenient.

Nonbold Romain and Greek alphabets represent scalars, while bold uppercase and bold lowercase represent matrices and vectors, respectively. I make no distinction between scalar- and vector-valued functions, both are denoted as nonbold letters. In Section \ref{sec:diff}, I will try to use the initial letters of the Romain alphabet (\(a, b, c, \dots\)) to represent constants (known values), and the final letters of the Romain alphabet (\(x, y, z, w, \dots\)) to represent variables (unknown values). Greek letters will be preferred to represent independent and unknown scalars that are not within a vector or matrix. Finally, the operators \(\cdot^{\trans}\), \(\cdot^{\hermit}\), \(\cdot^*\) \(\text{tr}(\cdot)\), \(\textnormal{adj}(\cdot)\), and \(\abs{\cdot}\) denote, respectively, the transpose, the hermitian, the conjugate, the trace, the adjoint, and the determinant (or absolute value when the operand is a scalar).

\subsection{Jacobian formulation (numerator layout)}

In the Jacobian formulation (also called numerator layout), the derivative matrix is written laying out the numerator in its shape, while the denominator has its shape transposed (you will understand it better as soon as you see the definitions).

\subsubsection{Vector-vector, scalar-vector, and vector-scalar derivatives}
Consider two vectors \(\mathbf{x} \in \mathbb{C}^n\) and \(\mathbf{y} \in \mathbb{C}^m\). The partial derivative of each element in \(\mathbf{y}\) by each element in \(\mathbf{x}\) is represented as
\begin{align}
    \label{eq:jacobian-formulation}
    \left[ \dfrac{\partial \mathbf{y}}{\partial \mathbf{x}} \right]_{\textnormal{Num}} = \renewcommand{\arraystretch}{2.6} \begin{bmatrix}
        \dfrac{\partial y_1}{\partial \mathbf{x}} \\
        \dfrac{\partial y_2}{\partial \mathbf{x}} \\ 
        \vdots \\ 
        \dfrac{\partial y_m}{\partial \mathbf{x}}
    \end{bmatrix} = \renewcommand{\arraystretch}{1.8}
    \begin{bmatrix}
        \dfrac{\partial y_1}{\partial x_1} & \dfrac{\partial y_1}{\partial x_2} & \dots & \dfrac{\partial y_1}{\partial x_n} \\
        \dfrac{\partial y_2}{\partial x_1} & \dfrac{\partial y_2}{\partial x_2} & \dots & \dfrac{\partial y_2}{\partial x_n} \\
        \vdots & \vdots & \ddots & \vdots \\
        \dfrac{\partial y_m}{\partial x_1} & \dfrac{\partial y_m}{\partial x_2} & \dots & \dfrac{\partial y_m}{\partial x_n} \\
    \end{bmatrix} \in \mathbb{C}^{m\times n}.
\end{align}

We can infer what is the shape of \(\dfrac{\partial y}{\partial \mathbf{x}}\) and \(\dfrac{\partial \mathbf{y}}{\partial x}\) by changing the respective vector sizes in Equation \eqref{eq:jacobian-formulation}.

\subsubsection{Matrix-scalar derivative (tangent matrix)}
The partial derivative \(\dfrac{\partial \mathbf{Y}}{\partial x}\) (usually called tangent matrix) is defined for the numerator layout as
\begin{align}
    \label{eq:tangent-num}
    \left[\dfrac{\partial \mathbf{Y}}{\partial x}\right]_{\textnormal{Num}} \renewcommand{\arraystretch}{1.8} = \begin{bmatrix}
        \dfrac{\partial y_{11}}{\partial x} & \dfrac{\partial y_{12}}{\partial x} & \dots & \dfrac{\partial y_{1n}}{\partial x} \\
        \dfrac{\partial y_{21}}{\partial x} & \dfrac{\partial y_{22}}{\partial x} & \dots & \dfrac{\partial y_{2n}}{\partial x} \\
        \vdots & \vdots & \ddots & \vdots \\
        \dfrac{\partial y_{m1}}{\partial x} & \dfrac{\partial y_{m2}}{\partial x} & \dots & \dfrac{\partial y_{mn}}{\partial x} \\
    \end{bmatrix} \in \mathbb{C}^{m \times n},
\end{align}
where \(\mathbf{Y} \in \mathbb{C}^{m \times n}\).

\subsubsection{Scalar-matrix derivative (gradient matrix)}
The partial derivative of \(\dfrac{\partial y}{\partial \mathbf{X}}\) (usually called gradient matrix) is given by
\begin{align}
    \label{eq:gradient-matrix-num}
    \renewcommand{\arraystretch}{1.8}
			\left[\dfrac{\partial y}{\partial \mathbf{X}}\right]_{\textnormal{Num}} = \begin{bmatrix}
				\dfrac{\partial y}{\partial x_{11}} & \dfrac{\partial y}{\partial x_{21}} & \dots & \dfrac{\partial y}{\partial x_{m1}} \\
				\dfrac{\partial y}{\partial x_{12}} & \dfrac{\partial y}{\partial x_{22}} & \dots & \dfrac{\partial y}{\partial x_{m2}} \\
				\vdots & \vdots & \ddots & \vdots \\
				\dfrac{\partial y}{\partial x_{1n}} & \dfrac{\partial y}{\partial x_{2n}} & \dots & \dfrac{\partial y}{\partial x_{mn}} \\
			\end{bmatrix} \in \mathbb{C}^{n \times m},
\end{align}
where \(\mathbf{X} \in \mathbb{C}^{m \times n}\).

\subsubsection{Row vector-scalar and scalar-row vector derivatives}

From these definitions, we can infer two nonobvious equalities that are rather useful when handling matrix differentiations. If we consider a special case of the gradient matrix (Eq.\eqref{eq:gradient-matrix-num}) when $m=1 \therefore \mathbf{X} = \mathbf{x}^\top \in \mathbb{R}^{1\times n}$, where $\mathbf{x} \in \mathbb{R}^n$, we have that
\begin{align}
    \left[\dfrac{\partial \alpha}{\partial \mathbf{x}^\top}\right]_{\textnormal{Num}} = \renewcommand{\arraystretch}{2.6}\begin{bmatrix}
        \dfrac{\partial \alpha}{\partial x_{1}} \\
        \dfrac{\partial \alpha}{\partial x_{2}} \\
        \vdots \\
        \dfrac{\partial \alpha}{\partial x_{n}}\vspace{1.3ex}
    \end{bmatrix} \in \mathbb{R}^{n}.
\end{align}
However, by using we definition from the Eq.\eqref{eq:jacobian-formulation}, it is also true to state that
\begin{align}
    \left[\dfrac{\partial \alpha}{\partial \mathbf{x}}\right]_{\textnormal{Num}} = \begin{bmatrix} \dfrac{\partial \alpha}{\partial x_{1}} & \dfrac{\partial \alpha}{\partial x_{2}} & \cdots & \dfrac{\partial \alpha}{\partial x_{n}} \end{bmatrix} \in \mathbb{R}^{1 \times n},
\end{align}
Therefore,
\begin{align}
    \label{eq:hidden-num}
    \left[\dfrac{\partial \alpha}{\partial \mathbf{x}^\top}\right]_{\textnormal{Num}} = \left[\dfrac{\partial \alpha}{\partial \mathbf{x}}\right]_{\textnormal{Num}}^\top
\end{align}

Similarly, from the Eq.\eqref{eq:tangent-num}, when $m=1 \therefore \mathbf{Y} = \mathbf{y}^\top \in \mathbb{R}^{1\times n}$, where $\mathbf{y} \in \mathbb{R}^n$, we have that

\begin{align}
    \left[\dfrac{\partial \mathbf{y}^\top}{\partial \alpha}\right]_{\textnormal{Num}} = \renewcommand{\arraystretch}{2.6}\begin{bmatrix}
        \dfrac{\partial y_{1}}{\partial \alpha} &
        \dfrac{\partial y_{2}}{\partial \alpha} &
        \cdots &
        \dfrac{\partial y_{n}}{\partial \alpha}\vspace{1.3ex}
    \end{bmatrix} \in \mathbb{R}^{1 \times n}.
\end{align}

However, from the Eq.\eqref{eq:jacobian-formulation}, we also have that

\begin{align}
    \left[\dfrac{\partial \mathbf{y}}{\partial \alpha}\right]_{\textnormal{Num}} = \renewcommand{\arraystretch}{2.6}\begin{bmatrix}
        \dfrac{\partial y_{1}}{\partial \alpha} \\
        \dfrac{\partial y_{2}}{\partial \alpha} \\
        \cdots \\
        \dfrac{\partial y_{n}}{\partial \alpha}\vspace{1.3ex}
    \end{bmatrix} \in \mathbb{R}^{n}.
\end{align}

Therefore,
\begin{align}
    \left[\dfrac{\partial \mathbf{y}^\top}{\partial \alpha}\right]_{\textnormal{Num}} = \left[\dfrac{\partial \mathbf{y}}{\partial \alpha}\right]_{\textnormal{Num}}^\top.
\end{align}

\subsubsection{Jacobian matrix for the numerator layout}
The matrix calculus notation for the Jacobian matrix is given by

\begin{align}
    \mathbf{J} = \frac{\partial f(\mathbf{x})}{\partial \mathbf{x}},
\end{align}
where \(f: \mathbb{R}^{n} \rightarrow \mathbb{R}^{m}\) is a vector function. As shown below, the matrix differentiation notation used to represent the Jacobian matrix is the numerator layout (c.f. Eq.\eqref{eq:jacobian-formulation})
\begin{align}
    \label{eq:jacobian-matrix}
    \mathbf{J} = \renewcommand{\arraystretch}{2.6} \begin{bmatrix}
        \dfrac{\partial f_1}{\partial \mathbf{x}} \\
        \dfrac{\partial f_2}{\partial \mathbf{x}} \\ 
        \vdots \\ 
        \dfrac{\partial f_m}{\partial \mathbf{x}}
    \end{bmatrix} = \begin{bmatrix}
        \dfrac{\partial f_1}{\partial x_1} & \dfrac{\partial f_1}{\partial x_2} & \dots & \dfrac{\partial f_1}{\partial x_n} \\
        \dfrac{\partial f_2}{\partial x_1} & \dfrac{\partial f_2}{\partial x_2} & \dots & \dfrac{\partial f_2}{\partial x_n} \\
        \vdots & \vdots & \ddots & \vdots \\
        \dfrac{\partial f_m}{\partial x_1} & \dfrac{\partial f_m}{\partial x_2} & \dots & \dfrac{\partial f_m}{\partial x_n} \\
    \end{bmatrix} \in \mathbb{R}^{m \times n},
\end{align}
where \(f = (f_1, f_2, \dots, f_m)\), being \(f_i: \mathbb{R}^n \rightarrow \mathbb{R}\), for \(1 \leq i \leq m\). Therefore, it is true to state that
\begin{align}
    \mathbf{J} = \left[ \frac{\partial f(\mathbf{x})}{\partial \mathbf{x}} \right]_{\textnormal{Num}}.
\end{align}
That is why it is also called ``Jacobian formulation''.

\subsubsection{Hessian matrix for the numerator layout}

The matrix calculus notation for the Hessian matrix in the numerator layout is given by
\begin{align}
    \label{eq:hessian-eq-numerator}
    \mathbf{H} = \dfrac{\partial^{2} f(\mathbf{x})}{\partial \mathbf{x} \partial \mathbf{x}^\top} = \renewcommand{\arraystretch}{2.2}\begin{bmatrix}
        \dfrac{\partial^{2} f}{\partial x_1^2} & \dfrac{\partial^{2} f}{\partial x_1 \partial x_2} & \cdots & \dfrac{\partial^{2} f}{\partial x_1 \partial x_n} \\
        \dfrac{\partial^{2} f}{\partial x_2 \partial x_1} & \dfrac{\partial^{2} f}{\partial x_2^2} & \cdots & \dfrac{\partial^{2} f}{\partial x_2 \partial x_n} \\
        \vdots & \vdots & \ddots & \vdots \\
        \dfrac{\partial^{2} f}{\partial x_n \partial x_1} & \dfrac{\partial^{2} f}{\partial x_n \partial x_2} & \dots & \dfrac{\partial^{2} f}{\partial x_n^2}
    \end{bmatrix} \in \mathbb{R}^{n \times n},
\end{align}
where \(f: \mathbb{R}^{n} \rightarrow \mathbb{R}\) and \(\mathbf{x} \in \mathbb{R}^{n}\). We can rewrite it by recalling the Eq.\eqref{eq:hidden-num}, that is,
\begin{align}
    \mathbf{H} = \dfrac{\partial^{2} f(\mathbf{x})}{\partial \mathbf{x} \partial \mathbf{x}^\top} = \dfrac{\partial }{\partial \mathbf{x}}\left( \dfrac{\partial f(\mathbf{x})}{\partial \mathbf{x}^\top} \right) = \dfrac{\partial }{\partial \mathbf{x}}\left( \dfrac{\partial f(\mathbf{x})}{\partial \mathbf{x}} \right)^\top.
\end{align}
Note that \(\left( \dfrac{\partial f(\mathbf{x})}{\partial \mathbf{x}} \right)^\top = \left( \dfrac{\partial f(\mathbf{x})}{\partial x_1}, \dfrac{\partial f(\mathbf{x})}{\partial x_2}, \dots, \dfrac{\partial f(\mathbf{x})}{\partial x_n} \right)\) is a vector. Therefore, we have a vector-vector derivate and, by applying the differentiation as shown in Eq.\eqref{eq:jacobian-formulation}, we get \(\mathbf{H}\). As we will see further on, the Hessian in matrix calculus notation for the denominator layout needs to be written differently.

\subsection{Hessian formulation (denominator layout)}
\label{sec:denominator}

In the Hessian formulation (also called denominator layout), the derivative matrix is written laying out the denominator in its shape, while the numerator has its shape transposed.

\subsubsection{Vector-vector, scalar-vector, and vector-scalar derivatives}
Consider two vectors \(\mathbf{x} \in \mathbb{C}^n\) and \(\mathbf{y} \in \mathbb{C}^m\). The partial derivative of each element in \(\mathbf{y}\) by each element in \(\mathbf{x}\) is represented as

\begin{align}
    \label{eq:denominator-layout}
    \left[ \dfrac{\partial \mathbf{y}}{\partial \mathbf{x}} \right]_{\textnormal{Den}} = \begin{bmatrix}
        \dfrac{\partial y_1}{\partial \mathbf{x}} & \dfrac{\partial y_2}{\partial \mathbf{x}} & \cdots & \dfrac{\partial y_3}{\partial \mathbf{x}}
    \end{bmatrix} = \begin{bmatrix}
        \dfrac{\partial y_1}{\partial x_1} & \dfrac{\partial y_2}{\partial x_1} & \dots & \dfrac{\partial y_m}{\partial x_1} \\
        \dfrac{\partial y_1}{\partial x_2} & \dfrac{\partial y_2}{\partial x_2} & \dots & \dfrac{\partial y_m}{\partial x_2} \\
        \vdots & \vdots & \ddots & \vdots \\
        \dfrac{\partial y_1}{\partial x_n} & \dfrac{\partial y_2}{\partial x_n} & \dots & \dfrac{\partial y_m}{\partial x_n} \\
    \end{bmatrix} \in \mathbb{C}^{n\times m}.
\end{align}
We can infer what is the shape of \(\dfrac{\partial y}{\partial \mathbf{x}}\) and \(\dfrac{\partial \mathbf{y}}{\partial x}\) by changing the respective vector sizes in Equation \eqref{eq:denominator-layout}.

\subsubsection{Matrix-scalar derivative (tangent matrix)}
The partial derivative \(\dfrac{\partial \mathbf{Y}}{\partial x}\) (usually called tangent matrix) is defined for the denominator layout as
\begin{align}
    \label{eq:tangent-matrix-den}
    \left[\dfrac{\partial \mathbf{Y}}{\partial x}\right]_{\textnormal{Den}} \renewcommand{\arraystretch}{1.8} = \begin{bmatrix}
        \dfrac{\partial y_{11}}{\partial x} & \dfrac{\partial y_{21}}{\partial x} & \dots & \dfrac{\partial y_{m1}}{\partial x} \\
        \dfrac{\partial y_{12}}{\partial x} & \dfrac{\partial y_{22}}{\partial x} & \dots & \dfrac{\partial y_{m2}}{\partial x} \\
        \vdots & \vdots & \ddots & \vdots \\
        \dfrac{\partial y_{1n}}{\partial x} & \dfrac{\partial y_{2n}}{\partial x} & \dots & \dfrac{\partial y_{mn}}{\partial x} \\
    \end{bmatrix} \in \mathbb{C}^{n \times m},
\end{align}
where \(\mathbf{Y} \in \mathbb{C}^{m \times n}\). However, be aware that some authors do not follow this convention for the tangent matrix for the denominator layout \cite{Matrixca44:online}. For sake of consistency (laying out the denominator and the transpose of the numerator), I will follow the convention as denoted in the Eq.\eqref{eq:tangent-matrix-den}.

\subsubsection{Scalar-matrix derivative (gradient matrix)}
The partial derivative of \(\dfrac{\partial y}{\partial \mathbf{X}}\) (usually called gradient matrix) is given by
\begin{align}
    \label{eq:gradient-matrix-den}
    \renewcommand{\arraystretch}{1.8}
			\left[\dfrac{\partial y}{\partial \mathbf{X}}\right]_{\textnormal{Den}} = \begin{bmatrix}
				\dfrac{\partial y}{\partial x_{11}} & \dfrac{\partial y}{\partial x_{12}} & \dots & \dfrac{\partial y}{\partial x_{1n}} \\
				\dfrac{\partial y}{\partial x_{21}} & \dfrac{\partial y}{\partial x_{22}} & \dots & \dfrac{\partial y}{\partial x_{2n}} \\
				\vdots & \vdots & \ddots & \vdots \\
				\dfrac{\partial y}{\partial x_{m1}} & \dfrac{\partial y}{\partial x_{m2}} & \dots & \dfrac{\partial y}{\partial x_{mn}} \\
			\end{bmatrix} \in \mathbb{C}^{m \times n},
\end{align}
where \(\mathbf{X} \in \mathbb{C}^{m \times n}\).

\subsubsection{Row vector-scalar and scalar-row vector derivatives}

From these definitions, we can infer two nonobvious equality that are rather useful when handling matrix differentiations. If we consider a special case of the gradient matrix (Eq.\eqref{eq:gradient-matrix-den}) when $m=1 \therefore \mathbf{X} = \mathbf{x}^\top \in \mathbb{R}^{1\times n}$, where $\mathbf{x} \in \mathbb{R}^n$, we have that
\begin{align}
    \left[\dfrac{\partial \alpha}{\partial \mathbf{x}^\top}\right]_{\textnormal{Den}} = \begin{bmatrix} \dfrac{\partial \alpha}{\partial x_{1}} &
        \dfrac{\partial \alpha}{\partial x_{2}} &
        \cdots &
        \dfrac{\partial \alpha}{\partial x_{n}} \end{bmatrix} \in \mathbb{R}^{1\times n},
\end{align}
however, by using we definition from the Eq.\eqref{eq:denominator-layout}, it is also true to state that
\begin{align}
    \left[\dfrac{\partial \alpha}{\partial \mathbf{x}}\right]_{\textnormal{Den}} = \begin{bmatrix} \dfrac{\partial \alpha}{\partial x_{1}} \\
        \dfrac{\partial \alpha}{\partial x_{2}} \\
        \vdots \\
        \dfrac{\partial \alpha}{\partial x_{n}} \end{bmatrix} \in \mathbb{R}^{n}.
\end{align}

Therefore,
\begin{align}
    \label{eq:hidden-den}
    \left[\dfrac{\partial \alpha}{\partial \mathbf{x}^\top}\right]_{\textnormal{Den}} = \left[\dfrac{\partial \alpha}{\partial \mathbf{x}}\right]_{\textnormal{Den}}^\top
\end{align}

Similarly, from the Eq.\eqref{eq:tangent-matrix-den}, when $m=1 \therefore \mathbf{Y} = \mathbf{y}^\top \in \mathbb{R}^{1\times n}$, where $\mathbf{y} \in \mathbb{R}^n$, we have that

\begin{align}
    \left[\dfrac{\partial \mathbf{y}^\top}{\partial \alpha}\right]_{\textnormal{Den}} = \renewcommand{\arraystretch}{2.6}\begin{bmatrix}
        \dfrac{\partial y_{1}}{\partial \alpha} \\
        \dfrac{\partial y_{2}}{\partial \alpha} \\
        \cdots \\
        \dfrac{\partial y_{n}}{\partial \alpha}\vspace{1.3ex}
    \end{bmatrix} \in \mathbb{R}^{n}.
\end{align}

However, from the Eq.\eqref{eq:denominator-layout}, we also have that

\begin{align}
    \left[\dfrac{\partial \mathbf{y}}{\partial \alpha}\right]_{\textnormal{Den}} = \renewcommand{\arraystretch}{2.6}\begin{bmatrix}
        \dfrac{\partial y_{1}}{\partial \alpha} &
        \dfrac{\partial y_{2}}{\partial \alpha} &
        \cdots &
        \dfrac{\partial y_{n}}{\partial \alpha}\vspace{1.3ex}
    \end{bmatrix} \in \mathbb{R}^{1\times n}.
\end{align}

Therefore,
\begin{align}
    \left[\dfrac{\partial \mathbf{y}^\top}{\partial \alpha}\right]_{\textnormal{Den}} = \left[\dfrac{\partial \mathbf{y}}{\partial \alpha}\right]_{\textnormal{Den}}^\top.
\end{align}

\subsubsection{The Jacobian matrix for the denominator layout}
The matrix calculus notation for the Jacobian matrix is given by
\begin{align}
    \mathbf{J} = \frac{\partial f(\mathbf{x})}{\partial \mathbf{x}},
\end{align}
where \(f: \mathbb{R}^{n} \rightarrow \mathbb{R}^{m}\) is a vector function. We have seen that this equation follows the numerator layout, not the denominator. As we will see further on, to transform from one layout to another, we need to apply the transpose operator, that is (c.f. Eq.\eqref{eq:jacobian-matrix}),
\begin{align}
    \left[ \mathbf{J} \right]_{\textnormal{Den}} & = \frac{\partial f(\mathbf{x})}{\partial \mathbf{x}}^{\top} \\
    & = \frac{\partial f(\mathbf{x})}{\partial \mathbf{x}}^\top\\
    & = \begin{bmatrix}
        \dfrac{\partial f_1(\mathbf{x})}{\partial \mathbf{x}} & \dfrac{\partial f_2(\mathbf{x})}{\partial \mathbf{x}} & \dots & \dfrac{\partial f_m(\mathbf{x})}{\partial \mathbf{x}}
    \end{bmatrix}^\top \\
    & = \renewcommand{\arraystretch}{2.2}\begin{bmatrix}
        \dfrac{\partial f_1(\mathbf{x})}{\partial \mathbf{x}}^\top \\ \dfrac{\partial f_2(\mathbf{x})}{\partial \mathbf{x}}^\top \\ \vdots \\ \dfrac{\partial f_m(\mathbf{x})}{\partial \mathbf{x}}^\top
    \end{bmatrix} \\
    & = \renewcommand{\arraystretch}{2.2}\begin{bmatrix}
        \dfrac{\partial f_1}{\partial x_1} & \dfrac{\partial f_1}{\partial x_2} & \dots & \dfrac{\partial f_1}{\partial x_n} \\
        \dfrac{\partial f_2}{\partial x_1} & \dfrac{\partial f_2}{\partial x_2} & \dots & \dfrac{\partial f_2}{\partial x_n} \\
        \vdots & \vdots & \ddots & \vdots \\
        \dfrac{\partial f_m}{\partial x_1} & \dfrac{\partial f_m}{\partial x_2} & \dots & \dfrac{\partial f_m}{\partial x_n} \\
    \end{bmatrix} \in \mathbb{R}^{m \times n}.
\end{align}

\subsubsection{The Hessian matrix for the denominator layout}
The matrix calculus notation for the Hessian matrix in the denominator layout is given by
\begin{align}
    \label{eq:hessian-eq-denomintor}
    \mathbf{H} = \dfrac{\partial^{2} f(\mathbf{x})}{\partial \mathbf{x}^2} = \renewcommand{\arraystretch}{2.2}\begin{bmatrix}
        \dfrac{\partial^{2} f}{\partial x_1^2} & \dfrac{\partial^{2} f}{\partial x_1 \partial x_2} & \cdots & \dfrac{\partial^{2} f}{\partial x_1 \partial x_n} \\
        \dfrac{\partial^{2} f}{\partial x_2 \partial x_1} & \dfrac{\partial^{2} f}{\partial x_2^2} & \cdots & \dfrac{\partial^{2} f}{\partial x_2 \partial x_n} \\
        \vdots & \vdots & \ddots & \vdots \\
        \dfrac{\partial^{2} f}{\partial x_n \partial x_1} & \dfrac{\partial^{2} f}{\partial x_n \partial x_2} & \dots & \dfrac{\partial^{2} f}{\partial x_n^2}
    \end{bmatrix} \in \mathbb{R}^{n \times n},
\end{align}
where \(f: \mathbb{R}^{n} \rightarrow \mathbb{R}\) and \(\mathbf{x} \in \mathbb{R}^{n}\). Some authors, such as Simon Haykin \cite{haykin2009neural}, adopt this notation. The element \(\partial\mathbf{x}^2\) seems to be merely a convention which means that \(f\) is being differentiated twice by \(\mathbf{x}\). It perfectly matches scalar differentiation notation and can be scaled to \(n\) consecutive derivatives without making the notation cumbersome (points to the Hessian team).

We can rewrite the equation \eqref{eq:hessian-eq-denomintor} as
\begin{align}
    \mathbf{H} = \dfrac{\partial^{2} f(\mathbf{x})}{\partial \mathbf{x}^2} = \dfrac{\partial }{\partial \mathbf{x}}\left( \dfrac{\partial f(\mathbf{x})}{\partial \mathbf{x}} \right),
\end{align}
Since \(\dfrac{\partial f(\mathbf{x})}{\partial \mathbf{x}} = \left( \dfrac{\partial f(\mathbf{x})}{\partial x_1}, \dfrac{\partial f(\mathbf{x})}{\partial x_2}, \dots, \dfrac{\partial f(\mathbf{x})}{\partial x_n} \right)\) is a vector, we have a vector-vector differentiation, which for the denominator layout yields
\begin{align}
    \mathbf{H} = \renewcommand{\arraystretch}{2.2}\begin{bmatrix}
        \dfrac{\partial^{2} f}{\partial x_1^2} & \dfrac{\partial^{2} f}{\partial x_1 \partial x_2} & \cdots & \dfrac{\partial^{2} f}{\partial x_1 \partial x_n} \\
        \dfrac{\partial^{2} f}{\partial x_2 \partial x_1} & \dfrac{\partial^{2} f}{\partial x_2^2} & \cdots & \dfrac{\partial^{2} f}{\partial x_2 \partial x_n} \\
        \vdots & \vdots & \ddots & \vdots \\
        \dfrac{\partial^{2} f}{\partial x_n \partial x_1} & \dfrac{\partial^{2} f}{\partial x_n \partial x_2} & \dots & \dfrac{\partial^{2} f}{\partial x_n^2}.
    \end{bmatrix}.
\end{align}
Therefore, it is true to state that
\begin{align}
    \mathbf{H} = \left[ \dfrac{\partial^{2} f(\mathbf{x})}{\partial \mathbf{x}^2} \right]_{\textnormal{Den}}.
\end{align}

\subsection{Comparative between Jacobian and Hessian formulations}

As you could have noticed,
\begin{align}
    \left[ \dfrac{\partial \mathbf{y}}{\partial \mathbf{x}} \right]_{\textnormal{Num}} & = \left[ \dfrac{\partial \mathbf{y}}{\partial \mathbf{x}} \right]_{\textnormal{Den}}^{\trans}, \\
    \left[ \dfrac{\partial \mathbf{Y}}{\partial x}\right]_{\textnormal{Num}} & = \left[ \dfrac{\partial \mathbf{Y}}{\partial x}\right]_{\textnormal{Den}}^{\trans}, \\
    \left[\dfrac{\partial y}{\partial \mathbf{X}}\right]_{\textnormal{Num}} & = \left[\dfrac{\partial y}{\partial \mathbf{X}}\right]_{\textnormal{Den}}^{\trans}.
\end{align}

That is the difference when you try to differentiate without paying attention to which representation the author adopted. The good news is that, as long as you differentiate it correctly, you can switch between the Jacobian and Hessian formulations by simply transposing the final result\footnote{First, you should apply \(\left[\cdot\right]_{\textnormal{Den}} = \left[\cdot\right]_{\textnormal{Num}}^{\trans}\) on partial derivatives that you get in the solution. Then, you apply the transpose to the whole solution.}. Fortunately, the denominator layout is the most adopted by authors from areas related to Electrical Engineering. That is why we will focus on the denominator layout hereafter (the notation \(\left[\cdot\right]_{\textnormal{Den}}\) will be dropped out since we do not need it anymore).

As a rule of thumb, keep in mind that:
\begin{itemize}
    \item \(\dfrac{\partial \mathbf{y}}{\partial \mathbf{x}}\) will yield a matrix.
    \item \(\dfrac{\partial \mathbf{Y}}{\partial x}\) will yield a matrix.
    \item \(\dfrac{\partial x}{\partial \mathbf{X}}\) will yield a matrix.
    \item \(\dfrac{\partial y}{\partial \mathbf{x}}\) will yield a vector.
    \item \(\dfrac{\partial \mathbf{y}}{\partial x}\) will yield a \(1\times n\) matrix (``row vector'').
    \item \(\mathbf{J} = \dfrac{\partial f(\mathbf{x})}{\partial \mathbf{x}}^\top\).
    \item \(\mathbf{H} = \dfrac{\partial^{2} f(\mathbf{x})}{\partial \mathbf{x}^2}\).
    \item \(\dfrac{\partial\alpha}{\partial \mathbf{x}^\top} = \dfrac{\partial\alpha}{\partial \mathbf{x}}^\top\) for \(\alpha\in \mathbb{R}\).
    \item \(\dfrac{\partial \mathbf{y}^\top}{\partial \alpha} = \dfrac{\partial \mathbf{y}}{\partial \alpha}^\top\) for \(\alpha\in \mathbb{R}\).
\end{itemize}
\vspace{0.2ex}

\subsection{Notations not widely agreed upon}
Expressions such as \(\dfrac{\partial \mathbf{y}}{\partial \mathbf{X}}, \dfrac{\partial \mathbf{Y}}{\partial \mathbf{X}}\), or \(\dfrac{\partial \mathbf{Y}}{\partial \mathbf{x}}\) have no agreement for Jacobian and Hessian notations. It is possible, however, to define the matrix-matrix derivative for both representations. The problem is that some authors define it in the most intuitive manner: a matrix whose element in the position \((i,j)\) is \(\partial y_{ij}/\partial x_{ij}\). However, as we saw, it is inconsistent for both formulations as the matrix for the Jacobian (Hessian) formulation must lay out its denominator (numerator) in its transposed shape. Therefore, for a consistent numerator layout, we would have
\begin{align}
    \left[\frac{\partial \mathbf{Y}}{\partial\mathbf{X}}\right]_{\textnormal{Num}} = \begin{bmatrix}
        \dfrac{\partial y_{1,1}}{\partial x_{1,1}} & \dfrac{\partial y_{1,2}}{\partial x_{2,1}} & \dots & \dfrac{\partial y_{1,m}}{\partial x_{m,1}} \\
        \dfrac{\partial y_{2,1}}{\partial x_{1,2}} & \dfrac{\partial y_{2,2}}{\partial x_{2,2}} & \dots & \dfrac{\partial y_{2,n}}{\partial x_{m,2}} \\
        \vdots & \vdots & \ddots & \vdots \\
        \dfrac{\partial y_{n,1}}{\partial x_{1,n}} & \dfrac{\partial y_{n,2}}{\partial x_{2,n}} & \dots & \dfrac{\partial y_{n,m}}{\partial x_{m,n}} \\
    \end{bmatrix} \in \mathbb{C}^{n\times m},
\end{align}
and for a consistent denominator layout, we would have

\begin{align}
    \left[\frac{\partial \mathbf{Y}}{\partial\mathbf{X}}\right]_{\textnormal{Den}} = \begin{bmatrix}
        \dfrac{\partial y_{1,1}}{\partial x_{1,1}} & \dfrac{\partial y_{2,1}}{\partial x_{1,2}} & \dots & \dfrac{\partial y_{n,1}}{\partial x_{1,n}} \\
        \dfrac{\partial y_{1,2}}{\partial x_{2,1}} & \dfrac{\partial y_{2,2}}{\partial x_{2,2}} & \dots & \dfrac{\partial y_{n,2}}{\partial x_{2,n}} \\
        \vdots & \vdots & \ddots & \vdots \\
        \dfrac{\partial y_{1,m}}{\partial x_{m,1}} & \dfrac{\partial y_{2,m}}{\partial x_{m,2}} & \dots & \dfrac{\partial y_{n,m}}{\partial x_{m,n}} \\
    \end{bmatrix} \in \mathbb{C}^{m \times n},
\end{align}
where \(\mathbf{X}\in \mathbb{C}^{m\times n}\) and \(\mathbf{Y} \in \mathbb{C}^{n\times m}\). Notwithstanding, keep in mind that both equations are not standard and their usage must be acknowledged by the reader.
\section{Identities}

We need to be cautious when applying the matrix differentiation identities since the element orders matter. For instance, for scalar elements, the product rule may be written as either \((fg)' = f'g + g'f\) or \((fg)' = g f' + f g'\). In matrix calculus, we do not have such a privilege.

\subsection{Chain rule}
\subsubsection{Univariate functions}
For scalar elements, the chain rule is given by
\begin{align}
    \dfrac{\partial w}{\partial z} = \dfrac{\partial w}{\partial x} \dfrac{\partial x}{\partial y} \dfrac{\partial y}{\partial z}.
\end{align}
Similarly, in matrix notation, we have
\begin{align}
    \label{eq:chain-1inter}
    \dfrac{\partial \mathbf{w}}{\partial \mathbf{z}} = \dfrac{\partial \mathbf{y}}{\partial \mathbf{z}} \dfrac{\partial \mathbf{x}}{\partial \mathbf{y}} \dfrac{\partial \mathbf{w}}{\partial \mathbf{x}},
\end{align}
where \(\mathbf{x} \in \mathbb{C}^{n}, \mathbf{y} \in \mathbb{C}^{m}, \mathbf{z} \in \mathbb{C}^{p}\), and \(\mathbf{w} \in \mathbb{R}^q\). In this expression, \(\mathbf{w}\) depends on \(\mathbf{x}\), \(\mathbf{x}\) depends on \(\mathbf{y}\), and \(\mathbf{y}\) depends on \(\mathbf{z}\). The number of elements in the chain rule can be increased indiscriminately. The main point here is that \emph{the chain rule in matrix calculus notation must be placed backward when compared with the standard chain rule of scalar elements}.

\subsubsection{Multivariate functions}

In the previous section, we had a case where \(\mathbf{w}\) depends on \(\mathbf{x}\), which depends on \(\mathbf{y}\), which depends on \(\mathbf{z}\). If \(\mathbf{w} = f(\mathbf{x}), \mathbf{x} = g(\mathbf{y})\), and \(\mathbf{y} = h(\mathbf{z})\), then \(f, g\), and \(h\) are functions of one variable, also called univariate functions. However, we might find a situation where \(\mathbf{w} = f(\mathbf{x}, \mathbf{y})\) is a function of two (or more) variables.

For scalar elements, we can find partial derivatives of multivariate functions by considering that \(w = f(x, y)\) is differentiable on \(x\) and \(y\). The chain rule becomes
\begin{align}
    \dfrac{\partial w}{\partial z} = \dfrac{\partial w}{\partial x} \dfrac{\partial x}{\partial z} + \dfrac{\partial w}{\partial y} \dfrac{\partial y}{\partial z}.
\end{align}

Similarly, for matrix calculus notation, we have
\begin{align}
    \label{eq:chain-multi-inter}
    \dfrac{\partial \mathbf{w}}{\partial \mathbf{z}} = \dfrac{\partial \mathbf{x}}{\partial \mathbf{z}} \dfrac{\partial \mathbf{w}}{\partial \mathbf{x}} + \dfrac{\partial \mathbf{y}}{\partial \mathbf{z}} \dfrac{\partial \mathbf{w}}{\partial \mathbf{y}}
\end{align}

Note the backward placement of each summation term. This expression can be used for an unrestricted number of variables.

\subsection{Sum (or minus) rule}
\subsubsection{Vector-vector derivative}
Let \(\mathbf{x}, \mathbf{y} \in \mathbb{C}^{m}\) and \(a, b \in \mathbb{C}\), where \(\mathbf{x}\) and \(\mathbf{y}\) depend on \(\mathbf{w} \in \mathbb{C}^{n}\), but \(a\) and \(b\) do not. Therefore,
\begin{align}
    \dfrac{\partial (a\mathbf{x} \pm b\mathbf{y})}{\partial\mathbf{w}} = a\dfrac{\partial \mathbf{x}}{\partial\mathbf{w}} \pm b\dfrac{\partial \mathbf{y}}{\partial\mathbf{w}}
\end{align}
\subsubsection{Matrix-scalar derivative}
Another is when you have
\begin{align}
    \dfrac{\partial \left( a\mathbf{X} \pm b\mathbf{Y} \right)}{\partial \alpha},
\end{align}
where \(\mathbf{X}, \mathbf{Y} \in \mathbb{C}^{m \times n}\) depend on \(\alpha \in \mathbb{C}\). The solution is
\begin{align}
    \dfrac{\partial (a\mathbf{X} \pm b\mathbf{Y})}{\partial\mathbf{\alpha}} = a\dfrac{\partial \mathbf{X}}{\partial \alpha} \pm b \dfrac{\partial \mathbf{Y}}{\partial \alpha}.
\end{align}
\subsubsection{Scalar-matrix derivative}
The scalar-matrix derivative has a similar result, i.e.,
\begin{align}
    \dfrac{\partial \left( ax \pm by \right)}{\partial \mathbf{W}} = a\dfrac{\partial x}{\partial \mathbf{W}} \pm b \dfrac{\partial y}{\partial \mathbf{W}},
\end{align}
where \(x, y \in \mathbb{C}\) depend on \(\mathbf{W} \in \mathbb{C}^{m\times n}\), but \(a,b \in \mathbb{C}\) do not.

\subsection{Product rule}
\subsubsection{Vector-vector derivative}
Let \(w \in \mathbb{C}\) and \(\mathbf{z} \in \mathbb{C}^{m}\), where both depend on \(\mathbf{x} \in \mathbb{C}^{n}\). Then,
\begin{align}
    \dfrac{\partial w \mathbf{z}}{\partial \mathbf{x}} = w \dfrac{\partial \mathbf{z}}{\partial \mathbf{x}} + \dfrac{\partial w}{\partial \mathbf{x}} \mathbf{z}^\trans.
\end{align}

Note that is not possible to apply the product rule when you have \(\mathbf{Wz}\), where \(\mathbf{W} \in \mathbb{C}^{n \times m}\) also depends on \(\mathbf{x}\). If you tried, you would get \(\partial\mathbf{W}/\partial\mathbf{x}\), which does not exist.
\subsubsection{Scalar-vector derivative}
Another possibility of applying the product rule is when you have \(\mathbf{w}^{\trans} \mathbf{z}\), where \(\mathbf{w} \in \mathbb{C}^{m}\) also depends on \(\mathbf{x} \in \mathbb{C}^{n}\). In this case, the dot product is given by
\begin{align}
    \label{eq:scalar-vector-product-rule}
    \dfrac{\partial \mathbf{w}^{\trans} \mathbf{z}}{\partial \mathbf{x}} = \dfrac{\partial \mathbf{z}}{\partial \mathbf{x}} \mathbf{w} + \dfrac{\partial \mathbf{w}}{\partial \mathbf{x}} \mathbf{z}.
\end{align}

\subsubsection{Scalar-matrix derivative}
It is still possible to apply the product rule to
\begin{align}
    \dfrac{\partial wz}{\partial \mathbf{X}},
\end{align}
where \(w,z \in \mathbb{C}\) depend on \(\mathbf{X} \in \mathbb{C}^{m \times n}\). In this case, we have
\begin{align}
    \dfrac{\partial wz}{\partial \mathbf{X}} = w \dfrac{\partial z}{\partial \mathbf{X}} + z \dfrac{\partial w}{\partial \mathbf{X}}.
\end{align}

\subsubsection{Matrix-scalar derivative}
The last case is when you have
\begin{align}
    \dfrac{\partial \mathbf{W}\mathbf{Z}}{\partial \alpha},
\end{align}
where both \(\mathbf{W} \in \mathbb{C}^{m \times p}\) and \(\mathbf{Z} \in \mathbb{C}^{p\times n}\) depend on \(\alpha \in \mathbb{C}\). In this case, we have
\begin{align}
    \label{eq:matrix-matrix-product-rule}
    \dfrac{\partial \mathbf{W}\mathbf{Z}}{\partial \alpha} = \dfrac{\partial \mathbf{Z}}{\partial \alpha}\mathbf{W}^{\trans} + \mathbf{Z}^{T} \dfrac{\mathbf{W}}{\partial \alpha}
\end{align}


\section{Solution of Matrix Differentiations}\label{sec:diff}
We usually have two ways to solve matrix differentiation:
\begin{enumerate}
    \item Performing element-by-element operations in matrices and vectors;
    \item Preserving the matrix calculus notation, performing operations on the whole matrix/vector and, eventually, using some identities.
\end{enumerate}
The latter is usually more straightforward and less toilsome than the former and is therefore preferable.

The solutions in this Section will usually show the element-by-element solution and the solution by preserving the matrix calculus notation. For the element-by-element solutions, you only need to know that a scalar-vector derivate results in a vector for the Hessian formulation. All other shapes will naturally arise. For solutions with matrix calculus notation, you need to be acquainted with some of its identities.

\subsection{\(\dfrac{\partial \mathbf{A} \mathbf{x}}{\partial \mathbf{x}} = \mathbf{A}^\trans\)}

Let \(\mathbf{A}\in \mathbb{C}^{m\times n}\) and \(\mathbf{x} \in \mathbb{C}^{n}\), in which \(\mathbf{A}\) does not depend on \(\mathbf{x}\), we have that:
\begin{align}
    \dfrac{\partial \mathbf{A} \mathbf{x}}{\partial \mathbf{x}} = & \dfrac{\partial}{\partial \mathbf{x}} \left(
        \begin{bmatrix}
            a_{11} & a_{12} & \dots & a_{1n} \\
            a_{21} & a_{22} & \dots & a_{2n} \\
            \vdots & \vdots & \ddots & \vdots \\
            a_{m1} & a_{m2} & \dots & a_{mn} \\
        \end{bmatrix} \begin{bmatrix}
            x_{1} \\ x_{2} \\ \vdots \\ x_{n}
        \end{bmatrix} \right)  \\
    %
    = & \dfrac{\partial}{\partial \mathbf{x}} \left(\begin{bmatrix} 
        \sum_{j = 1}^n a_{1j}x_j \\
        \sum_{j = 1}^n a_{2j}x_j \\
        \vdots \\
        \sum_{j = 1}^n a_{mj}x_j
    \end{bmatrix}^\trans \right)  \\
    %
    = & \begin{bmatrix}
        \dfrac{\partial}{\partial \mathbf{x}}\left(\sum_{j = 1}^n {a_{1j}x_j}\right) & \dfrac{\partial}{\partial \mathbf{x}}\left(\sum_{j = 1}^n {a_{2j}x_j}\right) & \dots & \dfrac{\partial}{\partial \mathbf{x}}\left(\sum_{j = 1}^n {a_{mj}x_j}\right)
    \end{bmatrix}
\end{align}

Since a scalar-vector derivative is represented by a vector, we have that
\begin{align}
    %
    \dfrac{\partial \mathbf{A} \mathbf{x}}{\partial \mathbf{x}} = & \begin{bmatrix}
        \dfrac{\partial}{\partial x_1} \left( \sum_{j = 1}^n a_{1j}x_j \right) & 
        \dfrac{\partial}{\partial x_1} \left( \sum_{j = 1}^n a_{21j}x_j \right) & 
        \dots & 
        \dfrac{\partial}{\partial x_1} \left( \sum_{j = 1}^n a_{mj}x_j \right) \\
        \dfrac{\partial}{\partial x_2} \left( \sum_{j = 1}^n a_{1j}x_j \right) & 
        \dfrac{\partial}{\partial x_2} \left( \sum_{j = 1}^n a_{21j}x_j \right) & 
        \dots & 
        \dfrac{\partial}{\partial x_2} \left( \sum_{j = 1}^n a_{mj}x_j \right) \\
        \vdots & \vdots & \ddots & \vdots \\
        \dfrac{\partial}{\partial x_n} \left( \sum_{j = 1}^n a_{1j}x_j \right) & 
        \dfrac{\partial}{\partial x_n} \left( \sum_{j = 1}^n a_{21j}x_j \right) & 
        \dots & 
        \dfrac{\partial}{\partial x_n} \left( \sum_{j = 1}^n a_{mj}x_j \right) \\
    \end{bmatrix}  \\
    %
    = & \begin{bmatrix}
        a_{11} & a_{21} & \dots & a_{n1} \\
        a_{12} & a_{22} & \dots & a_{n2} \\
        \vdots & \vdots & \ddots & \vdots \\
        a_{1m} & a_{2m} & \dots & a_{nm} \\
    \end{bmatrix}
\end{align}
\begin{align}
    \label{eq:lt-slution}
    \boxed{\dfrac{\partial \mathbf{A} \mathbf{x}}{\partial \mathbf{x}} = \mathbf{A}^\trans \in \mathbb{C}^{n\times m}}
\end{align}

\subsection{\(\dfrac{\partial \mathbf{A}  \mathbf{x}}{\partial \mathbf{z}} = \dfrac{\partial \mathbf{x}}{\partial \mathbf{z}} \mathbf{A}^\trans\)}

Let \(\mathbf{x} \in \mathbb{C}^{n}\), \(\mathbf{z} \in \mathbb{C}^{p}\) and \(\mathbf{A} \in \mathbb{C}^{m\times n}\), where \(\mathbf{x}\) depends on \(\mathbf{z}\), but \(\mathbf{A}\) does not. Then
\begin{align}
    \dfrac{\partial \mathbf{A}  \mathbf{x}}{\partial \mathbf{z}} & =
    %
    \begin{bmatrix}
        \dfrac{\partial}{\partial \mathbf{z}}\sum_{i=1}^{n} a_{1i}x_i & \dfrac{\partial}{\partial \mathbf{z}}\sum_{i=1}^{n} a_{2i}x_i & \cdots & \dfrac{\partial}{\partial \mathbf{z}}\sum_{i=1}^{n} a_{mi}x_i
    \end{bmatrix} \\
    %
    & = \begin{bmatrix}
        \sum_{i=1}^{n} a_{1i}\dfrac{\partial x_i}{\partial \mathbf{z}} & \sum_{i=1}^{n} a_{2i}\dfrac{\partial x_i}{\partial \mathbf{z}} & \cdots & \sum_{i=1}^{n} a_{mi}\dfrac{\partial x_i}{\partial \mathbf{z}}
    \end{bmatrix} \\
    %
    & = \underbrace{\begin{bmatrix}
        \dfrac{\partial x_1}{\partial \mathbf{z}} & \dfrac{\partial x_2}{\partial \mathbf{z}} & \cdots & \dfrac{\partial x_n}{\partial \mathbf{z}}
    \end{bmatrix}}_{p \times n}
    %
    \underbrace{\begin{bmatrix}
        a_{11} & a_{21} & \dots & a_{m1} \\
        a_{12} & a_{22} & \dots & a_{m2} \\
        \vdots & \vdots & \ddots & \vdots \\
        a_{1n} & a_{2n} & \dots & a_{mn} \\
    \end{bmatrix}}_{n \times m}
\end{align}
\begin{align}
    \boxed{\dfrac{\partial \mathbf{A}  \mathbf{x}}{\partial \mathbf{z}} = \dfrac{\partial \mathbf{x}}{\partial \mathbf{z}} \mathbf{A}^\trans \in \mathbb{C}^{p \times m}}
\end{align}

Observe that this result is equivalent to applying the chain rule (c.f. Equation \eqref{eq:chain-1inter}), that is,
\begin{align}
    \dfrac{\partial \mathbf{A}  \mathbf{x}}{\partial \mathbf{z}} = \dfrac{\partial \mathbf{x}}{\partial \mathbf{z}} \dfrac{\partial \mathbf{A}  \mathbf{x}}{\partial \mathbf{x}} = \dfrac{\partial \mathbf{x}}{\partial \mathbf{z}} \mathbf{A}^\trans.
\end{align}

\subsection{\(\dfrac{\partial \mathbf{a}^\trans \mathbf{x}}{\partial \mathbf{x}} = \mathbf{a}\)}

Let \(\mathbf{a, x} \in \mathbb{C}^{n}\), in which \(\mathbf{a}\) does not depend on \(\mathbf{x}\). You can derive the derivative for the inner product by considering that \(\mathbf{a}^\trans\) is actually a \(1\times n\) matrix that transforms \(\mathbb{R}^{n}\) into \(\mathbb{R}\), and we already know what is the derivate of a \(\mathbf{Ax}\). Thus,
\begin{align}
    \dfrac{\partial \mathbf{a}^\trans \mathbf{x}}{\partial \mathbf{x}} = {\mathbf{a}^\trans}^\trans = \mathbf{a}.
\end{align}

Even though, if you want the step-by-step, here it is:
\begin{align}
    \dfrac{\partial \mathbf{a}^\trans \mathbf{x}}{\partial \mathbf{x}} &= \dfrac{\partial}{\partial \mathbf{x}} \left(
    \begin{bmatrix}
        a_1 & a_2 & \dots & a_n
    \end{bmatrix} \begin{bmatrix}
        x_{1} \\ x_{2} \\ \vdots \\ x_{n}
    \end{bmatrix} \right) \\
    & = \dfrac{\partial}{\partial \mathbf{x}} \left( \sum_{i = 1}^n a_ix_i \right) \\
    & = \begin{bmatrix}
        \dfrac{\partial}{\partial x_1} \left( \sum_{i = 1}^n a_ix_i \right) \\ \dfrac{\partial}{\partial x_2} \left( \sum_{i = 1}^n a_ix_i \right) \\ \vdots \\ \dfrac{\partial}{\partial x_n} \left( \sum_{i = 1}^n a_ix_i \right) 
    \end{bmatrix} = \begin{bmatrix}
        a_1 \\ a_2 \\ \vdots \\ a_n
    \end{bmatrix}
\end{align}
\begin{align}
    \boxed{\dfrac{\partial \mathbf{a}^\trans \mathbf{x}}{\partial \mathbf{x}} = \mathbf{a} \in \mathbb{C}^{n}}
\end{align}

\subsection{\(\dfrac{\partial \mathbf{x}^\trans  \mathbf{a}}{\partial \mathbf{x}} = \mathbf{a}\)}

This one can be solved quickly by noticing that \(\mathbf{x}^\trans  \mathbf{a} = \mathbf{a}^\trans  \mathbf{x}\). Hence,
\begin{align}
    \dfrac{\partial \mathbf{x}^\trans  \mathbf{a}}{\partial \mathbf{x}} = \dfrac{\partial \mathbf{a}^\trans  \mathbf{x}}{\partial \mathbf{x}} = \mathbf{a}
\end{align}

Nevertheless, here is the step-by-step:
\begin{align}
    \dfrac{\partial \mathbf{x}^\trans \mathbf{a}}{\partial \mathbf{x}} &= \dfrac{\partial}{\partial \mathbf{x}} \left(
    \begin{bmatrix}
        x_1 & x_2 & \dots & x_n
    \end{bmatrix} \begin{bmatrix}
        a_{1} \\ a_{2} \\ \vdots \\ a_{n}
    \end{bmatrix} \right) \\
    & = \dfrac{\partial}{\partial \mathbf{x}} \left( \sum_{i = 1}^n x_ia_i \right) \\
    & = \begin{bmatrix}
        \dfrac{\partial}{\partial x_1} \left( \sum_{i = 1}^n x_ia_i \right) \\ \dfrac{\partial}{\partial x_2} \left( \sum_{i = 1}^n x_ia_i \right) \\ \vdots \\ \dfrac{\partial}{\partial x_n} \left( \sum_{i = 1}^n x_ia_i \right) 
    \end{bmatrix} 
    = \begin{bmatrix}
        a_1 \\ a_2 \\ \vdots \\ a_n
    \end{bmatrix}
\end{align}
\begin{align}
    \boxed{\dfrac{\partial \mathbf{x}^\trans \mathbf{a}}{\partial \mathbf{x}} = \mathbf{a} \in \mathbb{C}^{n}}
\end{align}

\subsection{\(\dfrac{\partial \mathbf{a}^\hermit  \mathbf{x}}{\partial \mathbf{x}} = \mathbf{a}^*\)}

Let \(\mathbf{a, x} \in \mathbb{C}^{n}\), in which \(\mathbf{a}\) does not depend on \(\mathbf{x}\). Once again, we could say that
\begin{align}
    \dfrac{\partial \mathbf{a}^\hermit \mathbf{x}}{\partial \mathbf{x}} = {\mathbf{a}^\hermit}^\trans = \mathbf{a}^*
\end{align}

Nevertheless, here is the step-by-step:
\begin{align}
    \dfrac{\partial \mathbf{a}^\hermit \mathbf{x}}{\partial \mathbf{x}} &= \dfrac{\partial}{\partial \mathbf{x}} \left(
    \begin{bmatrix}
        a^*_1 & a^*_2 & \dots & a^*_n
    \end{bmatrix} \begin{bmatrix}
        x_{1} \\ x_{2} \\ \vdots \\ x_{n}
    \end{bmatrix} \right) 
    = \dfrac{\partial}{\partial \mathbf{x}} \left( \sum_{i = 1}^n a^*_ix_i \right) \\
\end{align}

Since a scalar-vector derivative is represented by a vector, we have that
\begin{align}
    \dfrac{\partial \mathbf{a}^\hermit \mathbf{x}}{\partial \mathbf{x}} &= \begin{bmatrix}
        \dfrac{\partial}{\partial x_1} \left( \sum_{i = 1}^n a^*_ix_i \right) \\ \dfrac{\partial}{\partial x_2} \left( \sum_{i = 1}^n a^*_ix_i \right) \\ \vdots \\ \dfrac{\partial}{\partial x_n} \left( \sum_{i = 1}^n a^*_ix_i \right) 
    \end{bmatrix}
    = \begin{bmatrix}
        a^*_1 \\ a^*_2 \\ \vdots \\ a^*_n
    \end{bmatrix}
\end{align}
\begin{align}
    \boxed{\dfrac{\partial \mathbf{a}^\hermit \mathbf{x}}{\partial \mathbf{x}} = \mathbf{a}^* \in \mathbb{C}^{n}}
\end{align}

\subsection{\(\dfrac{\partial \mathbf{x}^\hermit \mathbf{a}}{\partial \mathbf{x}} = \mathbf{0}\)}
Notice that \(\mathbf{x}^\hermit \mathbf{a} \neq \mathbf{a}^\hermit \mathbf{x}\). Therefore, we have no choice but derive it. Let \(\mathbf{a, x} \in \mathbb{C}^{n}\), in which \(\mathbf{a}\) does not depend on \(\mathbf{x}\), we have that
\begin{align}
    \dfrac{\partial \mathbf{x}^\hermit \mathbf{a}}{\partial\mathbf{x}} & = \dfrac{\partial}{\partial\mathbf{x}} \left(
    \begin{bmatrix}
        x^*_1 & x^*_2 & \dots & x^*_n
    \end{bmatrix} \begin{bmatrix}
        a_{1} \\ a_{2} \\ \vdots \\ a_{n}
    \end{bmatrix} \right) \\
    & = \dfrac{\partial}{\partial\mathbf{x}} \left( \sum_{i = 1}^n x^*_ia_i \right) \\
    &= \begin{bmatrix}
            \dfrac{\partial}{\partial x_1} \left( \sum_{i = 1}^n x^*_ia_i \right) \\ \dfrac{\partial}{\partial x_2} \left( \sum_{i = 1}^n x^*_ia_i \right) \\ \vdots \\
            \dfrac{\partial}{\partial x_n} \left( \sum_{i = 1}^n x^*_ia_i \right)
        \end{bmatrix}.
\end{align}
By recalling that \(\dfrac{\partial x^*}{\partial x} = 0\) \obs{reference required}, we have that
\begin{align}
    \dfrac{\partial \mathbf{x}^\hermit \mathbf{a}}{\partial\mathbf{x}} = \begin{bmatrix}
        0 \\ 0 \\ \vdots \\ 0
    \end{bmatrix}
\end{align}
\begin{align}
    \boxed{\dfrac{\partial \mathbf{x}^\hermit \mathbf{a}}{\partial\mathbf{x}} = \mathbf{0} \in \mathbb{C}^{n}}
\end{align}
where \(\mathbf{0}\) is the zero vector.

\subsection{\(\dfrac{\partial \mathbf{x}^\trans \mathbf{y}}{\partial \mathbf{z}} = \dfrac{\partial \mathbf{x}}{\partial \mathbf{z}}\mathbf{y} + \dfrac{\partial \mathbf{y}}{\partial \mathbf{z}}\mathbf{x}\)}
Let \(\mathbf{x},\mathbf{y} \in \mathbb{C}^{n}\) and \(\mathbf{z} \in \mathbb{C}^{m}\). Where \(\mathbf{x}\) and \(\mathbf{y}\) depend on \(\mathbf{z}\). Thus,
\begin{align}
    \dfrac{\partial \mathbf{x}^\trans \mathbf{y}}{\partial \mathbf{z}} & = \dfrac{\partial}{\partial \mathbf{z}}\sum_{i=1}^{n} y_ix_i \\
    & = \sum_{i=1}^{n} \dfrac{\partial y_ix_i}{\partial \mathbf{z}}.
\end{align}
Recalling that \((fg)' = f'g + g'f\), we have
\begin{align}
    \dfrac{\partial \mathbf{x}^\trans \mathbf{y}}{\partial \mathbf{z}} & = \sum_{i=1}^{n} x_i\dfrac{\partial y_i}{\partial \mathbf{z}} + \sum_{i=1}^{n} y_i\dfrac{\partial x_i}{\partial \mathbf{z}} \\
    & = \begin{bmatrix}
        \sum_{i=1}^{n} x_i\dfrac{\partial y_i}{\partial z_1} \\
        \sum_{i=1}^{n} x_i\dfrac{\partial y_i}{\partial z_2} \\
        \vdots \\
        \sum_{i=1}^{n} x_i\dfrac{\partial y_i}{\partial z_m} \\
    \end{bmatrix} +
    \begin{bmatrix}
        \sum_{i=1}^{n} y_i\dfrac{\partial x_i}{\partial z_1} \\
        \sum_{i=1}^{n} y_i\dfrac{\partial x_i}{\partial z_2} \\
        \vdots \\
        \sum_{i=1}^{n} y_i\dfrac{\partial x_i}{\partial z_m}
    \end{bmatrix} \\
    %
    & = \begin{bmatrix}
        \dfrac{\partial y_1}{\partial z_1} & \dfrac{\partial y_2}{\partial z_1} & \cdots & \dfrac{\partial y_n}{\partial z_1} \\
        \dfrac{\partial y_1}{\partial z_2} & \dfrac{\partial y_2}{\partial z_2} & \cdots & \dfrac{\partial y_n}{\partial z_2} \\
        \vdots & \ddots & \vdots & \vdots \\
        \dfrac{\partial y_1}{\partial z_m} & \dfrac{\partial y_2}{\partial z_m} & \cdots & \dfrac{\partial y_n}{\partial z_m} \\
    \end{bmatrix} \mathbf{x} \\
    & \hspace{2.5ex} + \begin{bmatrix}
        \dfrac{\partial x_1}{\partial z_1} & \dfrac{\partial x_2}{\partial z_1} & \cdots & \dfrac{\partial x_n}{\partial z_1} \\
        \dfrac{\partial x_1}{\partial z_2} & \dfrac{\partial x_2}{\partial z_2} & \cdots & \dfrac{\partial x_n}{\partial z_2} \\
        \vdots & \ddots & \vdots & \vdots \\
        \dfrac{\partial x_1}{\partial z_m} & \dfrac{\partial x_2}{\partial z_m} & \cdots & \dfrac{\partial x_n}{\partial z_m} \\
    \end{bmatrix} \mathbf{y}
\end{align}
\begin{align}
    \boxed{\dfrac{\partial \mathbf{x}^\trans \mathbf{y}}{\partial \mathbf{z}} = \dfrac{\partial \mathbf{x}}{\partial \mathbf{z}}\mathbf{y} + \dfrac{\partial \mathbf{y}}{\partial \mathbf{z}}\mathbf{x} \in \mathbb{C}^{m}}
\end{align}
Note that, if either \(\mathbf{x}\) or \(\mathbf{y}\) does not depend on \(\mathbf{z}\), just disregard \(\dfrac{\partial \mathbf{x}}{\partial \mathbf{z}}\mathbf{y}\) or \(\dfrac{\partial \mathbf{y}}{\partial \mathbf{z}}\mathbf{x}\), respectively. When neither depends on \(\mathbf{z}\), the obvious result is the zero vector, \(\mathbf{0}\). A simpler way to solve it is to apply the scalar-vector product rule (see Equation \eqref{eq:scalar-vector-product-rule}), that is,
\begin{align}
    \dfrac{\partial \mathbf{x}^\trans \mathbf{y}}{\partial \mathbf{z}} = \dfrac{\partial \mathbf{x}}{\partial \mathbf{z}} \dfrac{\partial \mathbf{x}^\trans \mathbf{y}}{\partial \mathbf{x}} + \dfrac{\partial \mathbf{y}}{\partial \mathbf{z}} \dfrac{\partial \mathbf{x}^\trans \mathbf{y}}{\partial \mathbf{y}} = \dfrac{\partial \mathbf{x}}{\partial \mathbf{z}}\mathbf{y} + \dfrac{\partial \mathbf{y}}{\partial \mathbf{z}}\mathbf{x}
\end{align}

\subsection{\(\dfrac{\partial \mathbf{x}^\trans \mathbf{x}}{\partial \mathbf{x}} = 2\mathbf{x}\)}
Let \(\mathbf{x} \in \mathbb{C}^{n}\) and \(\mathbf{z}\in \mathbb{C}^m\), where \(\mathbf{x}\) depends on \(\mathbf{z}\). Thus,
\begin{align}
    \dfrac{\partial \mathbf{x}^\trans \mathbf{x}}{\partial \mathbf{x}} & = \dfrac{\partial}{\partial \mathbf{x}}\sum_{i=1}^{n} x_i^2 \\
    & = \sum_{i=1}^{n} \dfrac{\partial x_i^2}{\partial \mathbf{x}} \\
    %
    & = \renewcommand{\arraystretch}{2.2}\begin{bmatrix}
        \sum_{i=1}^{n} \dfrac{\partial x_i^2}{\partial x_1} \\
        \sum_{i=1}^{n} \dfrac{\partial x_i^2}{\partial x_2} \\
        \vdots \\
        \sum_{i=1}^{n} \dfrac{\partial x_i^2}{\partial x_m} \\
    \end{bmatrix} \\
    %
    & = \renewcommand{\arraystretch}{2.2}\begin{bmatrix}
        2x_1 \\
        2x_2 \\
        \vdots \\
        2x_n \\
    \end{bmatrix}
\end{align}
\begin{align}
    \boxed{\dfrac{\partial \mathbf{x}^\trans \mathbf{x}}{\partial \mathbf{x}} = 2\mathbf{x} \in \mathbb{C}^n}
\end{align}

Note that this perfectly matches with the derivate of a quadratic scalar value, i.e., \(\frac{\diff x^2}{\diff x} = 2x\).

\subsection{\(\dfrac{\partial \mathbf{x}^\trans \mathbf{x}}{\partial \mathbf{z}} = 2\dfrac{\partial \mathbf{x}}{\partial \mathbf{z}}\mathbf{x}\)}
Let \(\mathbf{x} \in \mathbb{C}^{n}\) and \(\mathbf{z}\in \mathbb{C}^m\), where \(\mathbf{x}\) depends on \(\mathbf{z}\). Thus,
\begin{align}
    \dfrac{\partial \mathbf{x}^\trans \mathbf{x}}{\partial \mathbf{z}} & = \dfrac{\partial}{\partial \mathbf{z}}\sum_{i=1}^{n} x_i^2 \\
    & = \sum_{i=1}^{n} \dfrac{\partial x_i^2}{\partial \mathbf{z}} \\
    %
    & = \renewcommand{\arraystretch}{2.2}\begin{bmatrix}
        \sum_{i=1}^{n} \dfrac{\partial x_i^2}{\partial z_1} \\
        \sum_{i=1}^{n} \dfrac{\partial x_i^2}{\partial z_2} \\
        \vdots \\
        \sum_{i=1}^{n} \dfrac{\partial x_i^2}{\partial z_m} \\
    \end{bmatrix} \\
    %
    & = \renewcommand{\arraystretch}{2.2}\begin{bmatrix}
        \sum_{i=1}^{n} 2x_i\dfrac{\partial x_i}{\partial z_1} \\
        \sum_{i=1}^{n} 2x_i\dfrac{\partial x_i}{\partial z_2} \\
        \vdots \\
        \sum_{i=1}^{n} 2x_i\dfrac{\partial x_i}{\partial z_m} \\
    \end{bmatrix} \\
    & = 2 \renewcommand{\arraystretch}{2.2}\begin{bmatrix}
        \dfrac{\partial x_1}{\partial \mathbf{z}} & \dfrac{\partial x_2}{\partial \mathbf{z}} & \cdots & \dfrac{\partial x_n}{\partial \mathbf{z}}
    \end{bmatrix} \mathbf{x}
\end{align}
\begin{align}
    \boxed{\dfrac{\partial \mathbf{x}^\trans \mathbf{x}}{\partial \mathbf{z}} = 2\dfrac{\partial \mathbf{x}}{\partial \mathbf{z}}\mathbf{x} \in \mathbb{C}^m}
\end{align}

Note that this solution could also be solved by the chain rule (c.f. Equation \eqref{eq:chain-1inter}) as follows
\begin{align}
    \boxed{\dfrac{\partial \mathbf{x}^\trans \mathbf{x}}{\partial \mathbf{z}} = \dfrac{\partial \mathbf{x}}{\partial \mathbf{z}} \dfrac{\partial \mathbf{x}^\trans \mathbf{x}}{\partial \mathbf{x}} = 2\dfrac{\partial \mathbf{x}}{\partial \mathbf{z}}\mathbf{x} \in \mathbb{C}^m}
\end{align}

\subsection{\(\dfrac{\partial \mathbf{x}^\trans \mathbf{A} \mathbf{x}}{\partial \mathbf{x}} = \left(\mathbf{A}^\trans + \mathbf{A}\right) \mathbf{x}\)}
Let \(\mathbf{A}\in \mathbb{C}^{n\times n}\) and \(\mathbf{x} \in \mathbb{C}^{n}\), in which \(\mathbf{A}\) does not depend on \(\mathbf{x}\). For the quadratic form, it follows that
\begin{align}
    \dfrac{\partial \mathbf{x}^\trans \mathbf{A} \mathbf{x}}{\partial \mathbf{x}} &= \dfrac{\partial}{\partial \mathbf{x}} \left(
    \begin{bmatrix}
        x_{1} & x_{2} & \dots & x_{n}
    \end{bmatrix}
    \begin{bmatrix}
        a_{11} & a_{12} & \dots & a_{1n} \\
        a_{21} & a_{22} & \dots & a_{2n} \\
        \vdots & \vdots & \ddots & \vdots \\
        a_{n1} & a_{n2} & \dots & a_{nn} \\
    \end{bmatrix} \begin{bmatrix}
        x_{1} \\ x_{2} \\ \vdots \\ x_{n}
    \end{bmatrix} \right) \\
    &= \dfrac{\partial}{\partial \mathbf{x}} \left(
			\begin{bmatrix}
				\displaystyle \sum_{i = 1}^{n} x_{i}a_{i1} & 
				\displaystyle \sum_{i = 1}^{n} x_{i}a_{i2} & 
				\dots & 
				\displaystyle \sum_{i = 1}^{n} x_{i}a_{in}
			\end{bmatrix} \begin{bmatrix}
				x_{1} \\ x_{2} \\ \vdots \\ x_{n}
			\end{bmatrix} \right) \\
            &= \dfrac{\partial}{\partial \mathbf{x}} \left(
				\sum_{i = 1}^{n}\sum_{j = 1}^{n} a_{ij} x_{i} x_{j}
			\right).
\end{align}

Note that the element inside the parentheses is a scalar and that a scalar-vector derivative results in a vector, that is,
\begin{align}
    \dfrac{\partial \mathbf{x}^\trans \mathbf{A} \mathbf{x}}{\partial \mathbf{x}} &= \begin{bmatrix}
        \displaystyle \dfrac{\partial}{\partial x_1} \left( \sum_{i = 1}^{n}\sum_{j = 1}^{n} x_{i} a_{ij} x_{j} \right) \\ 
        \displaystyle  \dfrac{\partial}{\partial x_2} \left( \sum_{i = 1}^{n}\sum_{j = 1}^{n} x_{i} a_{ij} x_{j} \right) \\ 
        \vdots \\ 
        \displaystyle \dfrac{\partial}{\partial x_n} \left( \sum_{i = 1}^{n}\sum_{j = 1}^{n} x_{i} a_{ij} x_{j} \right) 
    \end{bmatrix} \\
    & = \begin{bmatrix}
        \displaystyle 2x_1a_{11} + \sum_{\substack{j = 1 \\ j \neq 1}}^{n} a_{1j} x_{j} + \sum_{\substack{i = 1 \\ i \neq 1}}^{n} a_{i1} x_{i} \\
        \displaystyle 2x_2a_{22} + \sum_{\substack{j = 1 \\ j \neq 2}}^{n} a_{2j} x_{j} + \sum_{\substack{i = 1 \\ i \neq 2}}^{n} a_{i2} x_{i} \\
        \vdots \\
        \displaystyle 2x_na_{nn} + \sum_{\substack{j = 1 \\ j \neq n}}^{n} a_{nj} x_{j} + \sum_{\substack{i = 1 \\ i \neq n}}^{n} a_{in} x_{i} 
    \end{bmatrix} \\
    &= \begin{bmatrix}
        \displaystyle \sum_{j = 1}^{n} a_{1j} x_{j} \\
        \displaystyle \sum_{j = 1}^{n} a_{2j} x_{j} \\
        \vdots \\
        \displaystyle \sum_{j = 1}^{n} a_{nj} x_{j} 
    \end{bmatrix} +
    \begin{bmatrix}
        \displaystyle \sum_{i = 1}^{n} a_{i1} x_{i} \\
        \displaystyle \sum_{i = 1}^{n} a_{i2} x_{i} \\
        \vdots \\
        \displaystyle \sum_{i = 1}^{n} a_{in} x_{i} 
    \end{bmatrix} \\
    & = \mathbf{A}^\trans \mathbf{x} + \mathbf{A} \mathbf{x}
\end{align}
\begin{align}
    \label{eq:quadratic-solution}
    \boxed{\dfrac{\partial \mathbf{x}^\trans \mathbf{A} \mathbf{x}}{\partial \mathbf{x}} = \left(\mathbf{A}^\trans + \mathbf{A}\right) \mathbf{x} \in \mathbb{C}^{n}}
\end{align}
For the special case where \(\mathbf{A}\) is symmetric, we obtain
\begin{align}
    \boxed{\dfrac{\partial \mathbf{x}^\trans \mathbf{A} \mathbf{x}}{\partial \mathbf{x}} = 2\mathbf{A} \mathbf{x} \in \mathbb{C}^{n}}
\end{align}

\subsection{\(\dfrac{\partial \mathbf{x}^\trans \mathbf{A} \mathbf{x}}{\partial \mathbf{z}} = \dfrac{\partial \mathbf{x}}{\partial \mathbf{z}}\left( \mathbf{A} + \mathbf{A}^\trans \right) \mathbf{x}\)}
Let \(\mathbf{z} \in \mathbb{C}^{m}, \mathbf{x} \in \mathbb{C}^{n}\) and \(\mathbf{A}\in \mathbb{C}^{n\times n}\), where \(\mathbf{x}\) depends on \(\mathbf{z}\), but \(\mathbf{A}\) does not. For the quadratic form, it follows that
\begin{align}
    \dfrac{\partial \mathbf{x}^\trans \mathbf{A} \mathbf{x}}{\partial \mathbf{z}} &= \dfrac{\partial}{\partial \mathbf{z}} \left(
    \begin{bmatrix}
        x_{1} & x_{2} & \dots & x_{n}
    \end{bmatrix}
    \begin{bmatrix}
        a_{11} & a_{12} & \dots & a_{1n} \\
        a_{21} & a_{22} & \dots & a_{2n} \\
        \vdots & \vdots & \ddots & \vdots \\
        a_{n1} & a_{n2} & \dots & a_{nn} \\
    \end{bmatrix} \begin{bmatrix}
        x_{1} \\ x_{2} \\ \vdots \\ x_{n}
    \end{bmatrix} \right) \\
    &= \dfrac{\partial}{\partial \mathbf{z}} \left(
			\begin{bmatrix}
				\displaystyle \sum_{i = 1}^{n} x_{i}a_{i1} & 
				\displaystyle \sum_{i = 1}^{n} x_{i}a_{i2} & 
				\dots & 
				\displaystyle \sum_{i = 1}^{n} x_{i}a_{in}
			\end{bmatrix} \begin{bmatrix}
				x_{1} \\ x_{2} \\ \vdots \\ x_{n}
			\end{bmatrix} \right) \\
            &= \dfrac{\partial}{\partial \mathbf{z}} \left(
				\sum_{i = 1}^{n}\sum_{j = 1}^{n} a_{ij} x_{i} x_{j}
			\right) \\
    &= \begin{bmatrix}
        \displaystyle \dfrac{\partial}{\partial z_1} \left( \sum_{i = 1}^{n}\sum_{j = 1}^{n} x_{i} a_{ij} x_{j} \right) \\ 
        \displaystyle  \dfrac{\partial}{\partial z_2} \left( \sum_{i = 1}^{n}\sum_{j = 1}^{n} x_{i} a_{ij} x_{j} \right) \\ 
        \vdots \\ 
        \displaystyle \dfrac{\partial}{\partial z_n} \left( \sum_{i = 1}^{n}\sum_{j = 1}^{n} x_{i} a_{ij} x_{j} \right) 
    \end{bmatrix} \\
    &= \begin{bmatrix}
        \displaystyle \sum_{i = 1}^{n}\sum_{j = 1}^{n} a_{ij} \dfrac{\partial x_{i}x_{j}}{\partial z_1} \\ 
        \displaystyle \sum_{i = 1}^{n}\sum_{j = 1}^{n} a_{ij} \dfrac{\partial x_{i}x_{j}}{\partial z_2} \\ 
        \vdots \\ 
        \displaystyle \sum_{i = 1}^{n}\sum_{j = 1}^{n} a_{ij} \dfrac{\partial x_{i}x_{j}}{\partial z_n} 
    \end{bmatrix}
\end{align}

Recalling that \((fg)' = f'g + g'f\), we have that
\begin{align}
    \dfrac{\partial \mathbf{x}^\trans \mathbf{A} \mathbf{x}}{\partial \mathbf{z}} &= \begin{bmatrix}
        \displaystyle \sum_{i = 1}^{n}\sum_{j = 1}^{n} x_{j}a_{ij} \dfrac{\partial x_{i}}{\partial z_1} +
        \sum_{i = 1}^{n}\sum_{j = 1}^{n} x_{i}a_{ij} \dfrac{\partial x_{j}}{\partial z_1} \\ 
        \displaystyle \sum_{i = 1}^{n}\sum_{j = 1}^{n} x_{j}a_{ij} \dfrac{\partial x_{i}}{\partial z_2} +
        \sum_{i = 1}^{n}\sum_{j = 1}^{n} x_{i}a_{ij} \dfrac{\partial x_{j}}{\partial z_2} \\ 
        \vdots \\ 
        \displaystyle \sum_{i = 1}^{n}\sum_{j = 1}^{n} x_{j}a_{ij} \dfrac{\partial x_{i}}{\partial z_n} +
        \sum_{i = 1}^{n}\sum_{j = 1}^{n} x_{i}a_{ij} \dfrac{\partial x_{j}}{\partial z_n} 
    \end{bmatrix} \\
    &= \begin{bmatrix}
        \displaystyle \sum_{i = 1}^{n}\sum_{j = 1}^{n} x_{j}a_{ij} \dfrac{\partial x_{i}}{\partial z_1} \\
        \displaystyle \sum_{i = 1}^{n}\sum_{j = 1}^{n} x_{j}a_{ij} \dfrac{\partial x_{i}}{\partial z_2} \\
        \vdots \\ 
        \displaystyle \sum_{i = 1}^{n}\sum_{j = 1}^{n} x_{j}a_{ij} \dfrac{\partial x_{i}}{\partial z_n}
    \end{bmatrix} +
    \begin{bmatrix}
        \displaystyle \sum_{i = 1}^{n}\sum_{j = 1}^{n} x_{i}a_{ij} \dfrac{\partial x_{j}}{\partial z_1} \\ 
        \displaystyle \sum_{i = 1}^{n}\sum_{j = 1}^{n} x_{i}a_{ij} \dfrac{\partial x_{j}}{\partial z_2} \\ 
        \vdots \\ 
        \displaystyle \sum_{i = 1}^{n}\sum_{j = 1}^{n} x_{i}a_{ij} \dfrac{\partial x_{j}}{\partial z_n} 
    \end{bmatrix} \\
    &= \begin{bmatrix}
       \dfrac{\partial x_{1}}{\partial \mathbf{z}} & \dfrac{\partial x_{2}}{\partial \mathbf{z}} & \cdots  & \dfrac{\partial x_{n}}{\partial \mathbf{z}}
    \end{bmatrix}
    \begin{bmatrix}
        a_{11} & a_{12} & \dots & a_{1n} \\
        a_{21} & a_{22} & \dots & a_{2n} \\
        \vdots & \vdots & \ddots & \vdots \\
        a_{m1} & a_{m2} & \dots & a_{mn} \\
    \end{bmatrix}
    \begin{bmatrix}
        x_{1} \\ x_{2} \\ \vdots \\ x_{n}
    \end{bmatrix} + \nonumber \\
    & \hspace{2.5ex} \begin{bmatrix}
        \dfrac{\partial x_{1}}{\partial \mathbf{z}} & \dfrac{\partial x_{2}}{\partial \mathbf{z}} & \cdots  & \dfrac{\partial x_{n}}{\partial \mathbf{z}}
     \end{bmatrix}
     \begin{bmatrix}
         a_{11} & a_{21} & \dots & a_{m1} \\
         a_{12} & a_{22} & \dots & a_{m2} \\
         \vdots & \vdots & \ddots & \vdots \\
         a_{1n} & a_{2n} & \dots & a_{mn} \\
     \end{bmatrix}
     \begin{bmatrix}
         x_{1} \\ x_{2} \\ \vdots \\ x_{n}
     \end{bmatrix} \\
     & = \dfrac{\partial \mathbf{x}}{\partial \mathbf{z}}\mathbf{A} \mathbf{x} + \dfrac{\partial \mathbf{x}}{\partial \mathbf{z}}\mathbf{A}^\trans \mathbf{x}
\end{align}
\begin{align}
    \boxed{\dfrac{\partial \mathbf{x}^\trans \mathbf{A} \mathbf{x}}{\partial \mathbf{z}} = \dfrac{\partial \mathbf{x}}{\partial \mathbf{z}}\left( \mathbf{A} + \mathbf{A}^\trans \right) \mathbf{x} \in \mathbb{C}^{m}}
\end{align}

For the special case where \(\mathbf{A}\) is symmetric, we get
\begin{align}
    \boxed{\dfrac{\partial \mathbf{x}^\trans \mathbf{A} \mathbf{x}}{\partial \mathbf{z}} = 2\dfrac{\partial \mathbf{x}}{\partial \mathbf{z}}\mathbf{A} \mathbf{x} \in \mathbb{C}^{m}}
\end{align}

Note that the solution is much easier if we maintain the matrix calculus notation and apply the chain rule, that is,
\begin{align}
    \dfrac{\partial \mathbf{x}^\trans \mathbf{A} \mathbf{x}}{\partial \mathbf{z}} = \dfrac{\partial \mathbf{x}}{\partial \mathbf{z}} \dfrac{\partial \mathbf{x}^\trans \mathbf{A} \mathbf{x}}{\partial \mathbf{x}} = \dfrac{\partial \mathbf{x}}{\partial \mathbf{z}} \left(\mathbf{A}^\trans + \mathbf{A}\right) \mathbf{x},
\end{align}
where the last equality comes from Equation \eqref{eq:quadratic-solution}.

\subsection{\(\dfrac{\partial \mathbf{b}^\trans \mathbf{A} \mathbf{x}}{\partial \mathbf{x}} = \mathbf{A}^\trans \mathbf{b}\)} \label{sec:bt-A-x}
Let \(\mathbf{x} \in \mathbb{C}^{n}\), \(\mathbf{b} \in \mathbb{C}^{m}\) and \(\mathbf{A}\in \mathbb{C}^{m\times n}\), where neither \(\mathbf{b}\) nor \(\mathbf{A}\) depend on \(\mathbf{x}\). It follows that

\begin{align}
    \dfrac{\partial \mathbf{b}^\trans \mathbf{A} \mathbf{x}}{\partial \mathbf{x}} &= \dfrac{\partial}{\partial \mathbf{x}} \left(
    \begin{bmatrix}
        b_{1} & b_{2} & \dots & b_{m}
    \end{bmatrix}
    \begin{bmatrix}
        a_{11} & a_{12} & \dots & a_{1n} \\
        a_{21} & a_{22} & \dots & a_{2n} \\
        \vdots & \vdots & \ddots & \vdots \\
        a_{m1} & a_{m2} & \dots & a_{mn} \\
    \end{bmatrix} \begin{bmatrix}
        x_{1} \\ x_{2} \\ \vdots \\ x_{n}
    \end{bmatrix} \right) \\
    &= \dfrac{\partial}{\partial \mathbf{x}} \left(
			\begin{bmatrix}
				\displaystyle \sum_{i = 1}^{m} a_{i1}b_{i} & 
				\displaystyle \sum_{i = 1}^{m} a_{i2}b_{i} & 
				\dots & 
				\displaystyle \sum_{i = 1}^{m} a_{in}b_{i}
			\end{bmatrix} \begin{bmatrix}
				x_{1} \\ x_{2} \\ \vdots \\ x_{n}
			\end{bmatrix} \right) \\
    &= \dfrac{\partial}{\partial \mathbf{x}} \left(
        \sum_{i = 1}^{m}\sum_{j = 1}^{n} a_{ij} b_{i} x_{j}
    \right) \\
    &= \sum_{i = 1}^{m}\sum_{j = 1}^{n} a_{ij} b_{i} \dfrac{\partial x_{j}}{\partial \mathbf{x}} \\
    &= \begin{bmatrix}
        \sum_{i = 1}^{m} a_{i1} b_{i} \\
        \sum_{i = 1}^{m} a_{i2} b_{i} \\
        \vdots \\
        \sum_{i = 1}^{m} a_{in} b_{i} \\
    \end{bmatrix} \\
    &= \begin{bmatrix}
        a_{11} & a_{21} & \dots & a_{m1} \\
        a_{12} & a_{22} & \dots & a_{m2} \\
        \vdots & \vdots & \ddots & \vdots \\
        a_{1n} & a_{2n} & \dots & a_{mn} \\
    \end{bmatrix}
    \begin{bmatrix}
        b_1 \\
        b_2 \\
        \vdots \\
        b_m
    \end{bmatrix}
\end{align}
\begin{align}
    \boxed{\dfrac{\partial \mathbf{b}^\trans \mathbf{A} \mathbf{x}}{\partial \mathbf{x}} = \mathbf{A}^\trans \mathbf{b} \in \mathbb{C}^{n}}
\end{align}

Note that this solution could solve by simply observing that \(\mathbf{\mathbf{b}^\trans \mathbf{A}}\) is actually a linear transformation from \(\mathbb{R}^{n}\) to \(\mathbb{R}\). Thus,
\begin{align}
    \dfrac{\partial \mathbf{b}^\trans \mathbf{A} \mathbf{x}}{\partial \mathbf{x}} = \left( \mathbf{b}^\trans \mathbf{A} \right)^{\trans} = \mathbf{A}^\trans \mathbf{b},
\end{align}
where the first equality comes from the Equation \eqref{eq:lt-slution}.

\subsection{\(\dfrac{\partial \mathbf{x}^\trans \mathbf{A} \mathbf{b}}{\partial \mathbf{x}} = \mathbf{Ab}\)}
Let \(\mathbf{x} \in \mathbb{C}^{m}\), \(\mathbf{b} \in \mathbb{C}^{n}\) and \(\mathbf{A}\in \mathbb{C}^{m\times n}\), where neither \(\mathbf{b}\) nor \(\mathbf{A}\) depend on \(\mathbf{x}\). The quickest way to solve it is to note that \(\mathbf{x}^\trans \mathbf{A} \mathbf{b} =  \mathbf{b}^\trans \mathbf{A}^\trans \mathbf{x}\), which is the problem solved by the Section \ref{sec:bt-A-x}. Thus,
\begin{align}
    \dfrac{\partial \mathbf{x}^\trans \mathbf{A} \mathbf{b}}{\partial \mathbf{x}} = \dfrac{\partial \mathbf{b}^\trans \mathbf{A}^\trans \mathbf{x}}{\partial \mathbf{x}} = \left( \mathbf{b}^\trans \mathbf{A}^\trans \right)^{\trans} = \mathbf{A} \mathbf{b},
\end{align}
where the second equality comes from Equation \eqref{eq:lt-slution}. Nevertheless, here is the step-by-step
\begin{align}
    \dfrac{\partial \mathbf{x}^\trans \mathbf{A} \mathbf{b}}{\partial \mathbf{x}} &= \dfrac{\partial}{\partial \mathbf{x}} \left(
    \begin{bmatrix}
        x_{1} & x_{2} & \dots & x_{m}
    \end{bmatrix}
    \begin{bmatrix}
        a_{11} & a_{12} & \dots & a_{1n} \\
        a_{21} & a_{22} & \dots & a_{2n} \\
        \vdots & \vdots & \ddots & \vdots \\
        a_{m1} & a_{m2} & \dots & a_{mn} \\
    \end{bmatrix} \begin{bmatrix}
        b_{1} \\ b_{2} \\ \vdots \\ b_{n}
    \end{bmatrix} \right) \\
    &= \dfrac{\partial}{\partial \mathbf{x}} \left(
			\begin{bmatrix}
				\displaystyle \sum_{i = 1}^{m} a_{i1}x_{i} & 
				\displaystyle \sum_{i = 1}^{m} a_{i2}x_{i} & 
				\dots & 
				\displaystyle \sum_{i = 1}^{m} a_{in}x_{i}
			\end{bmatrix} \begin{bmatrix}
				b_{1} \\ b_{2} \\ \vdots \\ b_{n}
			\end{bmatrix} \right) \\
    &= \dfrac{\partial}{\partial \mathbf{x}} \left(
        \sum_{i = 1}^{m}\sum_{j = 1}^{n} a_{ij} b_{j} x_{i}
    \right) \\
    &= \sum_{i = 1}^{m}\sum_{j = 1}^{n} a_{ij} b_{j} \dfrac{\partial x_{i}}{\partial \mathbf{x}} \\
    &= \begin{bmatrix}
        \sum_{j = 1}^{n} a_{1j} b_{j} \\
        \sum_{j = 1}^{n} a_{2j} b_{j} \\
        \vdots \\
        \sum_{j = 1}^{n} a_{nj} b_{j} \\
    \end{bmatrix} \\
    &= \begin{bmatrix}
        a_{11} & a_{12} & \dots & a_{1n} \\
        a_{21} & a_{22} & \dots & a_{2n} \\
        \vdots & \vdots & \ddots & \vdots \\
        a_{m1} & a_{m2} & \dots & a_{mn} \\
    \end{bmatrix}
    \begin{bmatrix}
        b_1 \\
        b_2 \\
        \vdots \\
        b_n
    \end{bmatrix}
\end{align}
\begin{align}
    \boxed{\dfrac{\partial \mathbf{x}^\trans \mathbf{A} \mathbf{b}}{\partial \mathbf{x}} = \mathbf{Ab} \in \mathbb{C}^{m}}
\end{align}


\subsection{\(\dfrac{\partial \mathbf{y}^\trans \mathbf{A} \mathbf{x}}{\partial \mathbf{z}} = \dfrac{\partial \mathbf{x}}{\partial \mathbf{z}} \mathbf{A}^{\trans} \mathbf{y} + \dfrac{\partial \mathbf{y}}{\partial \mathbf{z}} \mathbf{A} \mathbf{x}\)}
Let \(\mathbf{x} \in \mathbb{C}^{n}\), \(\mathbf{y} \in \mathbb{C}^{m}\), \(\mathbf{z} \in \mathbb{C}^{p}\), and \(\mathbf{A}\in \mathbb{C}^{m\times n}\), where \(\mathbf{x}\) and \(\mathbf{y}\) depend on \(\mathbf{z}\), but \(\mathbf{A}\) does not. Therefore,
\begin{align}
    \dfrac{\partial \mathbf{y}^\trans \mathbf{A} \mathbf{x}}{\partial \mathbf{z}} &= \dfrac{\partial}{\partial \mathbf{z}} \left(
    \begin{bmatrix}
        y_{1} & y_{2} & \dots & y_{m}
    \end{bmatrix}
    \begin{bmatrix}
        a_{11} & a_{12} & \dots & a_{1n} \\
        a_{21} & a_{22} & \dots & a_{2n} \\
        \vdots & \vdots & \ddots & \vdots \\
        a_{m1} & a_{m2} & \dots & a_{mn} \\
    \end{bmatrix} \begin{bmatrix}
        x_{1} \\ x_{2} \\ \vdots \\ x_{n}
    \end{bmatrix} \right) \\
    &= \dfrac{\partial}{\partial \mathbf{z}} \left(
			\begin{bmatrix}
				\displaystyle \sum_{i = 1}^{m} a_{i1}y_{i} & 
				\displaystyle \sum_{i = 1}^{m} a_{i2}y_{i} & 
				\dots & 
				\displaystyle \sum_{i = 1}^{m} a_{in}y_{i}
			\end{bmatrix} \begin{bmatrix}
				x_{1} \\ x_{2} \\ \vdots \\ x_{n}
			\end{bmatrix} \right) \\
    &= \dfrac{\partial}{\partial \mathbf{z}} \left(
        \sum_{i = 1}^{m}\sum_{j = 1}^{n} a_{ij} x_{j} y_{i}
    \right) \\
    &= \sum_{i = 1}^{m}\sum_{j = 1}^{n} a_{ij} \dfrac{\partial y_{i}x_{j}}{\partial \mathbf{z}} %\\
    % &= \begin{bmatrix}
    %     \sum_{j = 1}^{n} a_{1j} b_{j} \\
    %     \sum_{j = 1}^{n} a_{2j} b_{j} \\
    %     \vdots \\
    %     \sum_{j = 1}^{n} a_{nj} b_{j} \\
    % \end{bmatrix} \\
    % &= \begin{bmatrix}
    %     a_{11} & a_{12} & \dots & a_{1n} \\
    %     a_{21} & a_{22} & \dots & a_{2n} \\
    %     \vdots & \vdots & \ddots & \vdots \\
    %     a_{m1} & a_{m2} & \dots & a_{mn} \\
    % \end{bmatrix}
    % \begin{bmatrix}
    %     b_1 \\
    %     b_2 \\
    %     \vdots \\
    %     b_n
    % \end{bmatrix}
\end{align}
Recalling that \((fg)' = f'g + g'f\), we have that
\begin{align}
    \dfrac{\partial \mathbf{y}^\trans \mathbf{A} \mathbf{x}}{\partial \mathbf{z}} & = \sum_{i = 1}^{m}\sum_{j = 1}^{n} a_{ij} y_{i} \dfrac{\partial x_{j}}{\partial \mathbf{z}} + \sum_{i = 1}^{m}\sum_{j = 1}^{n} a_{ij} x_{j} \dfrac{\partial y_{i}}{\partial \mathbf{z}} \\
    & = \begin{bmatrix}
        \dfrac{\partial x_1}{\partial \mathbf{z}} & \dfrac{\partial x_2}{\partial \mathbf{z}} & \cdots & \dfrac{\partial x_n}{\partial \mathbf{z}}
    \end{bmatrix}
    \begin{bmatrix}
        a_{11} & a_{21} & \dots & a_{m1} \\
        a_{12} & a_{22} & \dots & a_{m2} \\
        \vdots & \vdots & \ddots & \vdots \\
        a_{1n} & a_{2n} & \dots & a_{mn} \\
    \end{bmatrix}
    \begin{bmatrix}
        y_1 \\
        y_2 \\
        \vdots \\
        y_m
    \end{bmatrix} + \\
    & \hspace{3ex} \begin{bmatrix}
        \dfrac{\partial y_1}{\partial \mathbf{z}} & \dfrac{\partial y_2}{\partial \mathbf{z}} & \cdots & \dfrac{\partial y_m}{\partial \mathbf{z}}
    \end{bmatrix}
    \begin{bmatrix}
        a_{11} & a_{12} & \dots & a_{1n} \\
        a_{21} & a_{22} & \dots & a_{2n} \\
        \vdots & \vdots & \ddots & \vdots \\
        a_{m1} & a_{m2} & \dots & a_{mn} \\
    \end{bmatrix}
    \begin{bmatrix}
        x_1 \\
        x_2 \\
        \vdots \\
        x_n
    \end{bmatrix}
\end{align}
\begin{align}
    \boxed{\dfrac{\partial \mathbf{y}^\trans \mathbf{A} \mathbf{x}}{\partial \mathbf{z}} = \dfrac{\partial \mathbf{x}}{\partial \mathbf{z}} \mathbf{A}^{\trans} \mathbf{y} + \dfrac{\partial \mathbf{y}}{\partial \mathbf{z}} \mathbf{A} \mathbf{x}}
\end{align}

Even though this problem is trickier, we can find the same solution in a clever way by preserving the matrix calculus notation and applying the chain rule. Note that \(\mathbf{y}^\trans \mathbf{A} \mathbf{x}\) depends on both \(\mathbf{x}\) and \(\mathbf{y}\) which, in turn, depend on \(\mathbf{z}\). Therefore (c.f. Equation \eqref{eq:chain-multi-inter}),
\begin{align}
    \dfrac{\partial \mathbf{y}^\trans \mathbf{A} \mathbf{x}}{\partial \mathbf{z}} & = \dfrac{\partial \mathbf{x}}{\partial \mathbf{z}} \dfrac{\partial \mathbf{y}^\trans \mathbf{A} \mathbf{x}}{\partial \mathbf{x}} + \dfrac{\partial \mathbf{y}}{\partial \mathbf{z}} \dfrac{\partial \mathbf{y}^\trans \mathbf{A} \mathbf{x}}{\partial \mathbf{y}} \\
    & = \dfrac{\partial \mathbf{x}}{\partial \mathbf{z}} \mathbf{A}^{\trans} \mathbf{y} + \dfrac{\partial \mathbf{y}}{\partial \mathbf{z}} \mathbf{A} \mathbf{x},
\end{align}
where the second equality comes from previous solutions.

\subsection{\(\dfrac{\partial \textnormal{tr}\left(\mathbf{A} \mathbf{X}\right)}{\partial \mathbf{X}} = \mathbf{A}^\trans\)}

Let \(\mathbf{A}, \mathbf{X} \in \mathbb{R}^{n\times n}\), where \(\mathbf{A}\) does not depend on the elements in \(\mathbf{X}\).
\begin{align*}
    \dfrac{\partial \textnormal{tr}\left(\mathbf{A} \mathbf{X}\right)}{\partial \mathbf{X}} &= \dfrac{\partial}{\partial \mathbf{X}} \left( \tr{\begin{bmatrix}
        a_{11} & a_{12} & \dots & a_{1n} \\
        a_{21} & a_{22} & \dots & a_{2n} \\
        \vdots & \vdots & \ddots & \vdots \\
        a_{n1} & a_{n2} & \dots & a_{nn} \\
    \end{bmatrix}
    \begin{bmatrix}
        x_{11} & x_{12} & \dots & x_{1n} \\
        x_{21} & x_{22} & \dots & x_{2n} \\
        \vdots & \vdots & \ddots & \vdots \\
        x_{n1} & x_{n2} & \dots & x_{nn} \\
    \end{bmatrix} \right)} \\
    %
    &= \dfrac{\partial}{\partial \mathbf{X}} \left( \sum_{i = 1}^{n} \sum_{j = 1}^{n} a_{ij}x_{ji} \right) \\
    %
    &= \begin{bmatrix}
        \displaystyle \dfrac{\partial}{\partial x_{11}} \left( \sum_{i = 1}^{n} \sum_{j = 1}^{n} a_{ij}x_{ji} \right) & 
        \displaystyle \dfrac{\partial}{\partial x_{12}} \left( \sum_{i = 1}^{n} \sum_{j = 1}^{n} a_{ij}x_{ji} \right) & 
        \dots & 
        \displaystyle \dfrac{\partial}{\partial x_{1n}} \left( \sum_{i = 1}^{n} \sum_{j = 1}^{n} a_{ij}x_{ji} \right) \\
        \displaystyle \dfrac{\partial}{\partial x_{21}} \left( \sum_{i = 1}^{n} \sum_{j = 1}^{n} a_{ij}x_{ji} \right) & 
        \displaystyle \dfrac{\partial}{\partial x_{22}} \left( \sum_{i = 1}^{n} \sum_{j = 1}^{n} a_{ij}x_{ji} \right) & 
        \dots & 
        \displaystyle \dfrac{\partial}{\partial x_{2n}} \left( \sum_{i = 1}^{n} \sum_{j = 1}^{n} a_{ij}x_{ji} \right) \\
        \vdots & \vdots & \ddots & \vdots \\
        \displaystyle \dfrac{\partial}{\partial x_{n1}} \left( \sum_{i = 1}^{n} \sum_{j = 1}^{n} a_{ij}x_{ji} \right) & 
        \displaystyle \dfrac{\partial}{\partial x_{n2}} \left( \sum_{i = 1}^{n} \sum_{j = 1}^{n} a_{ij}x_{ji} \right) & 
        \dots & 
        \displaystyle \dfrac{\partial}{\partial x_{nn}} \left( \sum_{i = 1}^{n} \sum_{j = 1}^{n} a_{ij}x_{ji} \right) \\
    \end{bmatrix} \\
    &= \begin{bmatrix}
            a_{11} & a_{21} & \dots & a_{n1} \\
            a_{12} & a_{22} & \dots & a_{n2} \\
            \vdots & \vdots & \ddots & \vdots \\
            a_{1n} & a_{2n} & \dots & a_{nn} \\
        \end{bmatrix}
\end{align*}
\begin{align}
    \boxed{\dfrac{\partial \textnormal{tr}\left(\mathbf{A} \mathbf{X}\right)}{\partial \mathbf{X}} = \mathbf{A}^\trans}
\end{align}

I have no idea how to make this solution simpler.

\subsection{\(\dfrac{\partial \abs{\mathbf{X}}}{\partial \mathbf{X}} = \adj{\mathbf{X}}\)}
Let \(\mathbf{X} \in \mathbb{R}^{n\times n}\). Through Laplace expansion (cofactor expansion), we can rewrite the determinant of \(\mathbf{X}\) as the sum of the cofactors of any row or column, multiplied by its generating element, that is
\begin{align}
    \abs{\mathbf{X}} = \sum_{i = 1}^{n} x_{ki} \abs{\mathbf{C}_{ki}} = \sum_{i = 1}^{n} x_{ik} \abs{\mathbf{C}_{ik}} \,\,\,\,\,\, \forall \,\, k \in \left\{ 1, 2, ..., n \right\},
\end{align}
where \(\mathbf{C}_{ij}\) denotes the cofactor matrix of \(\mathbf{X}\) generated from element \(x_{ij}\). It is worth noting that the cofactor of \(\mathbf{C}_{ij}\) is independent of the value of any element \((i,j)\) in \(\mathbf{X}\). Therefore, it follows that
\begin{align}
    \dfrac{\partial \abs{\mathbf{X}}}{\partial \mathbf{X}} &= \dfrac{\partial}{\partial \mathbf{X}} \left( \sum_{i = 1}^{n} x_{ki} \abs{\mathbf{C}_{ki}} \right) \,\,\,\,\,\, \forall \,\, k \in \left\{ 1, 2, ..., n \right\} \\
    %
    & = \dfrac{\partial}{\partial \mathbf{X}} \left( \begin{bmatrix}
        \displaystyle \sum_{i = 1}^{n} x_{1i} \abs{\mathbf{C}_{1i}} & 
        \displaystyle \sum_{i = 1}^{n} x_{1i} \abs{\mathbf{C}_{1i}} & 
        \dots & 
        \displaystyle \sum_{i = 1}^{n} x_{1i} \abs{\mathbf{C}_{1i}} \\
        \displaystyle \sum_{i = 1}^{n} x_{2i} \abs{\mathbf{C}_{2i}} & 
        \displaystyle \sum_{i = 1}^{n} x_{2i} \abs{\mathbf{C}_{2i}} & 
        \dots & 
        \displaystyle \sum_{i = 1}^{n} x_{2i} \abs{\mathbf{C}_{2i}} \\
        \vdots & \vdots & \ddots & \vdots \\
        \displaystyle \sum_{i = 1}^{n} x_{ni} \abs{\mathbf{C}_{ni}} & 
        \displaystyle \sum_{i = 1}^{n} x_{ni} \abs{\mathbf{C}_{ni}} & 
        \dots & 
        \displaystyle \sum_{i = 1}^{n} x_{ni} \abs{\mathbf{C}_{ni}}
    \end{bmatrix} \right) \\
    %
    & = \begin{bmatrix}
        \displaystyle \dfrac{\partial}{\partial x_{11}} \left( \sum_{i = 1}^{n} x_{1i} \abs{\mathbf{C}_{1i}} \right) & 
        \displaystyle \dfrac{\partial}{\partial x_{12}} \left( \sum_{i = 1}^{n} x_{1i} \abs{\mathbf{C}_{1i}} \right) & 
        \dots & 
        \displaystyle \dfrac{\partial}{\partial x_{13}} \left( \sum_{i = 1}^{n} x_{1i} \abs{\mathbf{C}_{1i}} \right) \\
        \displaystyle \dfrac{\partial}{\partial x_{21}} \left( \sum_{i = 1}^{n} x_{2i} \abs{\mathbf{C}_{2i}} \right) & 
        \displaystyle \dfrac{\partial}{\partial x_{22}} \left( \sum_{i = 1}^{n} x_{2i} \abs{\mathbf{C}_{2i}} \right) & 
        \dots & 
        \displaystyle \dfrac{\partial}{\partial x_{33}} \left( \sum_{i = 1}^{n} x_{2i} \abs{\mathbf{C}_{2i}} \right) \\
        \vdots & \vdots & \ddots & \vdots \\
        \displaystyle \dfrac{\partial}{\partial x_{n1}} \left( \sum_{i = 1}^{n} x_{ni} \abs{\mathbf{C}_{ni}} \right) & 
        \displaystyle \dfrac{\partial}{\partial x_{n2}} \left( \sum_{i = 1}^{n} x_{ni} \abs{\mathbf{C}_{ni}} \right) & 
        \dots & 
        \displaystyle \dfrac{\partial}{\partial x_{n3}} \left( \sum_{i = 1}^{n} x_{ni} \abs{\mathbf{C}_{ni}} \right) \\
    \end{bmatrix} \\
    &= \begin{bmatrix}
        \abs{\mathbf{C}_{11}} & \abs{\mathbf{C}_{12}} & \dots & \abs{\mathbf{C}_{1n}} \\
        \abs{\mathbf{C}_{21}} & \abs{\mathbf{C}_{22}} & \dots & \abs{\mathbf{C}_{2n}} \\
        \vdots & \vdots & \ddots & \vdots \\
        \abs{\mathbf{C}_{n1}} & \abs{\mathbf{C}_{n2}} & \dots & \abs{\mathbf{C}_{nn}} \\
    \end{bmatrix}
\end{align}
\begin{align}
    \boxed{\dfrac{\partial \abs{\mathbf{X}}}{\partial \mathbf{X}} = \adj{\mathbf{X}}}
\end{align}

I have no idea how to make this solution simpler.

\subsection{\(\dfrac{\partial \mathbf{A}^{-1}}{\partial \alpha} = - {\left( \mathbf{A}^{-1} \right)}^{\trans} \dfrac{\partial \mathbf{A}}{\partial \alpha} {\left( \mathbf{A}^{-1} \right)}^{\trans}\)}
Let \(\mathbf{A}\in \mathbb{C}^{m\times n}\) and \(\alpha \in \mathbb{C}\). Remember that \(\mathbf{A}^{-1}\mathbf{A} = \mathbf{I}\). Differentiating both sides of this equation with respect to \(\alpha\), we get
\begin{align}
    \dfrac{\partial \mathbf{A}^{-1}\mathbf{A}}{\partial \alpha} & = \dfrac{\partial \mathbf{I}}{\partial \alpha} = \mathbf{0}_{m \times n},
\end{align}
where \(\mathbf{0}_{m \times n}\) is a zero matrix with dimension \(m \times n\). By applying the product rule of a matrix-matrix derivate, we get (c.f. Equation \eqref{eq:matrix-matrix-product-rule})
\begin{align}
    \dfrac{\partial \mathbf{A}^{-1}\mathbf{A}}{\partial \alpha} =  \dfrac{\partial \mathbf{A}}{\partial \alpha} \left( \mathbf{A}^{-1} \right)^{\trans} + \mathbf{A}^{\trans} \dfrac{\partial \mathbf{A}^{-1}}{\partial \alpha} & = \mathbf{0}_{m \times n}
\end{align}
Using the property \(\left( \mathbf{A}^{\trans} \right)^{-1} = \left( \mathbf{A}^{-1} \right)^{\trans}\) and rearranging this expression, we get
\begin{align}
    \boxed{\dfrac{\partial \mathbf{A}^{-1}}{\partial \alpha} = - {\left( \mathbf{A}^{-1} \right)}^{\trans} \dfrac{\partial \mathbf{A}}{\partial \alpha} {\left( \mathbf{A}^{-1} \right)}^{\trans}}
\end{align}

\nocite{*}
\printbibliography

\end{document}