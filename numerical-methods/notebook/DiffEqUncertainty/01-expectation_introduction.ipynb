{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## System Model\n\nFirst, lets consider the following linear model.\n\n$$u' = p u$$"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "f(u,p,t) = p.*u"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "We then wish to solve this model on the timespan `t=0.0` to `t=10.0`, with an intial condition `u0=10.0` and parameter `p=-0.3`. We can then setup the differential equations, solve, and plot as follows"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "using DifferentialEquations, Plots\nu0 = [10.0]\np = [-0.3]\ntspan = (0.0,10.0)\nprob = ODEProblem(f,u0,tspan,p)\nsol = solve(prob)\nplot(sol)\nylims!(0.0,10.0)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "However, what if we wish to consider a random initial condition? Assume `u0` is distributed uniformly from `-10.0` to `10.0`, i.e.,"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "using Distributions\nu0_dist = [Uniform(-10.0,10.0)]"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can then run a Monte Carlo simulation of 100,000 trajectories by solving an `EnsembleProblem`."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "prob_func(prob,i,repeat) = remake(prob, u0 = rand.(u0_dist))\nensemble_prob = EnsembleProblem(prob,prob_func=prob_func)\n\nensemblesol = solve(ensemble_prob,Tsit5(),EnsembleThreads(),trajectories=100000)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plotting the first 250 trajectories produces"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "plot(ensemblesol, vars = (0,1), lw=1,alpha=0.1, label=nothing, idxs = 1:250)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Given the ensemble solution, we can then compute the expectation of a function $g\\left(\\cdot\\right)$ of the system state `u` at any time in the timespan, e.g. the state itself at `t=4.0` as"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "g(sol) = sol(4.0)\nmean([g(sol) for sol in ensemblesol])"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Alternatively, DiffEqUncertainty.jl offers a convenient interface for this type of calculation, `expectation()`."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "using DiffEqUncertainty\nexpectation(g, prob, u0_dist, p, MonteCarlo(), Tsit5(); trajectories=100000)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "`expectation()` takes the function of interest $g$, an `ODEProblem`, the initial conditions and parameters, and an `AbstractExpectationAlgorithm`. Here we use `MonteCarlo()` to use the Monte Carlo algorithm. Note that the initial conditions and parameters can be arrays that freely mix numeric and continuous distribution types from Distributions.jl. Recall, that `u0_dist = [Uniform(-10.0,10.0)]`, while `p = [-0.3]`. From this specification, the expectation is solved as\n\n$$\\mathbb{E}\\left[g\\left(X\\right)\\vert X\\sim Pf\\right]$$\n\nwhere $Pf$ is the \"push-forward\" density of the initial joint pdf $f$ on initial conditions and parameters. \n\nAlternatively, we could solve the same problem using the `Koopman()` algorithm."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "expectation(g, prob, u0_dist, p, Koopman(), Tsit5())"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Being that this system is linear, we can analytically compute the solution as a deterministic ODE with its initial condition set to the expectation of the initial condition, i.e.,\n\n$$e^{pt}\\mathbb{E}\\left[u_0\\right]$$"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "exp(p[1]*4.0)*mean(u0_dist[1])"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "We see that for this case the `Koopman()` algorithm produces a more accurate solution than `MonteCarlo()`. Not only is it more accurate, it is also much faster"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "@time expectation(g, prob, u0_dist, p, MonteCarlo(), Tsit5(); trajectories=100000)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "@time expectation(g, prob, u0_dist, p, Koopman(), Tsit5())"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Changing the distribution, we arrive at"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "u0_dist = [Uniform(0.0,10.0)]\n@time expectation(g, prob, u0_dist, p, MonteCarlo(), Tsit5(); trajectories=100_000)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "and"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "@time expectation(g, prob, u0_dist, p, Koopman(), Tsit5())[1]"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "where the analytical solution is"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "exp(p[1]*4.0)*mean(u0_dist[1])"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that the `Koopman()` algorithm doesn't currently support infinite or semi-infinite integration domains, where the integration domain is determined by the extrema of the given distributions. So, trying to using a `Normal` distribution will produce `NaN`"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "u0_dist = [Normal(3.0,2.0)]\nexpectation(g, prob, u0_dist, p, Koopman(), Tsit5())"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, the analytical solution is"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "exp(p[1]*4.0)*mean(u0_dist[1])"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using a truncated distribution will alleviate this problem. However, there is another gotcha. If a large majority of the probability mass of the distribution exists in a small region in the support, then the adaptive methods used to solve the expectation can \"miss\" the non-zero portions of the distribution and errantly return 0.0."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "u0_dist = [truncated(Normal(3.0,2.0),-1000,1000)]\nexpectation(g, prob, u0_dist, p, Koopman(), Tsit5())"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "whereas truncating at $\\pm 4\\sigma$ produces the correct result"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "u0_dist = [truncated(Normal(3.0,2.0),-5,11)]\nexpectation(g, prob, u0_dist, p, Koopman(), Tsit5())"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "If a large truncation is required, it is best practice to center the distribution on the truncated interval. This is because many of the underlying quadrature algorithms use the center of the interval as an evaluation point."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "u0_dist = [truncated(Normal(3.0,2.0),3-1000,3+1000)]\nexpectation(g, prob, u0_dist, p, Koopman(), Tsit5())"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vector-Valued Functions\n`expectation()` can also handle vector-valued functions. Simply pass the vector-valued function and set the `nout` kwarg to the length of the vector the function returns.\n\nHere, we demonstrate this by computing the expectation of `u` at `t=4.0s` and `t=6.0s`"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "g(sol) = [sol(4.0)[1], sol(6.0)[1]]\nexpectation(g, prob, u0_dist, p, Koopman(), Tsit5(); nout = 2)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "with analytical solution"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "exp.(p.*[4.0,6.0])*mean(u0_dist[1])"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "this can be used to compute the expectation at a range of times simultaneously"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "saveat = tspan[1]:.5:tspan[2]\ng(sol) = Matrix(sol)\nmean_koop = expectation(g, prob, u0_dist, p, Koopman(), Tsit5(); nout = length(saveat), saveat=saveat)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can then plot these values along with the analytical solution"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "plot(t->exp(p[1]*t)*mean(u0_dist[1]),tspan..., xlabel=\"t\", label=\"analytical\")\nscatter!(collect(saveat),mean_koop.u[:],marker=:o, label=nothing)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Benefits of Using Vector-Valued Functions\nIn the above examples we used vector-valued expectation calculations to compute the various expectations required. Alternatively, one could simply compute multiple scalar-valued expectations. However, in most cases it is more efficient to use the vector-valued form. This is especially true when the ODE to be solved is computationally expensive.\n\nTo demonstrate this, lets compute the expectation of $x$, $x^2$, and $x^3$ using both approaches while counting the number of times `g()` is evaluated. This is the same as the number of simulation runs required to arrive at the solution. First, consider the scalar-valued approach. Here, we follow the same method as before, but we add a counter to our function evaluation that stores the number of function calls for each expectation calculation to an array."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "function g(sol, power, counter)\n    counter[power] = counter[power] + 1\n    sol(4.0)[1]^power\nend\n\ncounters = [0,0,0]\nx_koop = expectation(s->g(s,1,counters), prob, u0_dist, p, Koopman(), Tsit5())\nx2_koop = expectation(s->g(s,2,counters), prob, u0_dist, p, Koopman(), Tsit5())\nx3_koop = expectation(s->g(s,3,counters), prob, u0_dist, p, Koopman(), Tsit5())\ncounters"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Leading to a total of `j sum(counters)` function evaluations.\n\nNow, lets compare this to the vector-valued approach"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "function g(sol, counter) \n    counter[1] = counter[1] + 1\n    v = sol(4.0)[1]\n    [v, v^2, v^3]\nend\n\ncounter = [0]\nexpectation(s->g(s,counter), prob, u0_dist, p, Koopman(), Tsit5(); nout = 3)\ncounter"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is `j round(counter[1]/sum(counters)*100,digits=2)`% the number of simulations required when using scalar-valued expectations. Note how the number of evaluations used in the vector-valued form is equivelent to the maximum number of evaluations for the 3 scalar-valued expectation calls.\n\n## Higher-Order Moments\nLeveraging this vector-valued capability, we can also efficiently compute higher-order central moments.\n\n### Variance\nThe variance, or 2nd central moment, of a random variable $X$ is defined as\n\n$$\\mathrm{Var}\\left(X\\right)=\\mathbb{E}\\left[\\left(X-\\mu\\right)^2\\right]$$\n\nwhere\n\n$$\\mu = \\mathbb{E}\\left[X\\right]$$\n\nThe expression for the variance can be expanded to\n\n$$\\mathrm{Var}\\left(X\\right)=\\mathbb{E}\\left[X^2\\right]-\\mathbb{E}\\left[X\\right]^2$$\n\nUsing this, we define a function that returns the expectations of $X$ and $X^2$ as a vector-valued function and then compute the variance from these"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "function g(sol) \n    x = sol(4.0)[1]\n    [x, x^2]\nend\n\nkoop = expectation(g, prob, u0_dist, p, Koopman(), Tsit5(); nout = 2)\nmean_koop = koop[1]\nvar_koop = koop[2] - mean_koop^2"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "For a linear system, we can propagate the variance analytically as\n\n$e^{2pt}\\mathrm{Var}\\left(u_0\\right)$"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "exp(2*p[1]*4.0)*var(u0_dist[1])"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "This can be computed at multiple time instances as well"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "saveat = tspan[1]:.5:tspan[2]\ng(sol) = [Matrix(sol)'; (Matrix(sol).^2)']\n\nkoop = expectation(g, prob, u0_dist, p, Koopman(), Tsit5(); nout = length(saveat)*2, saveat=saveat)\nμ = koop.u[1:length(saveat)]\nσ = sqrt.(koop.u[length(saveat)+1:end] - μ.^2)\n\nplot(t->exp(p[1]*t)*mean(u0_dist[1]),tspan..., ribbon = t->-sqrt(exp(2*p[1]*t)*var(u0_dist[1])), label=\"Analytical Mean, 1 std bounds\")\nscatter!(collect(saveat),μ,marker=:x, yerror = σ, c=:black, label = \"Koopman Mean, 1 std bounds\")"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Skewness\nA similar approach can be used to compute skewness"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "function g(sol) \n    v = sol(4.0)[1]\n    [v, v^2, v^3]\nend\n\nkoop = expectation(g, prob, u0_dist, p, Koopman(), Tsit5(); nout = 3)\nmean_koop = koop[1]\nvar_koop = koop[2] - mean_koop^2\n(koop[3] - 3.0*mean_koop*var_koop - mean_koop^3) / var_koop^(3/2)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "As the system is linear, we expect the skewness to be unchanged from the inital distribution. Becasue the distribution is a truncated Normal distribution centered on the mean, the true skewness is `0.0`.\n\n### nth Central Moment\nDiffEqUncertainty provides a convenience function `centralmoment` around this approach for higher-order central moments. It takes an integer for the number of central moments you wish to compute. While the rest of the arguments are the same as for  `expectation()`. The following will return central moments 1-5."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "g(sol) = sol(4.0)[1]\ncentralmoment(5, g, prob, u0_dist, p, Koopman(), Tsit5(),\n                ireltol = 1e-9, iabstol = 1e-9)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Batch-Mode\nIt is also possible to solve the various simulations in parallel by using the `batch` kwarg and a batch-mode supported quadrature algorithm via the `quadalg` kwarg. To view the list of batch compatible quadrature algorithms, refer to [Quadrature.jl](https://github.com/SciML/Quadrature.jl). Note: Batch-mode operation is built on top of DifferentialEquation.jl's `EnsembleProblem`. See the [EnsembleProblem documentation](https://diffeq.sciml.ai/stable/features/ensemble/) for additional options.\n\nThe default quadtrature algorithm used by `expectation()` does not support batch-mode evaluation. So, we first load dependencies for additional quadrature algorithms"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "using Quadrature, Cuba"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "We then solve our expectation as before using a `batch=10` multi-thread parallelization via `EnsembleThreads()` of Cuba's SUAVE algorithm. However, in this case we introduce additional uncertainty in the model parameter."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "u0_dist = [truncated(Normal(3.0,2.0),-5,11)]\np_dist = [truncated(Normal(-.7, .1), -1,0)]\n\ng(sol) = sol(6.0)[1]\n\nexpectation(g, prob, u0_dist, p_dist, Koopman(), Tsit5(), EnsembleThreads(); \n                quadalg = CubaSUAVE(), batch=10)[1]"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, lets compare the performance of the batch and non-batch modes"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "using BenchmarkTools\n\n@btime expectation(g, prob, u0_dist, p_dist, Koopman(), Tsit5(); \n                quadalg = CubaSUAVE())[1]"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "@btime expectation(g, prob, u0_dist, p_dist, Koopman(), Tsit5(), EnsembleThreads(); \n                quadalg = CubaSUAVE(), batch=10)[1]"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is also possible to parallelize across the GPU. However, one must be careful of the limitations of ensemble solutions with the GPU. Please refer to [DiffEqGPU.jl](https://github.com/SciML/DiffEqGPU.jl) for details.\n\nHere we load `DiffEqGPU` and modify our problem to use Float32 and to put the ODE in the required GPU form"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "using DiffEqGPU\n\nfunction f(du, u,p,t) \n    @inbounds begin\n        du[1] = p[1]*u[1];\n    end\n    nothing\nend\n\nu0 = Float32[10.0]\np = Float32[-0.3]\ntspan = (0.0f0,10.0f0)\nprob = ODEProblem(f,u0,tspan,p)\n\ng(sol) = sol(6.0)[1]\n\nu0_dist = [truncated(Normal(3.0f0,2.0f0),-5f0,11f0)]\np_dist = [truncated(Normal(-.7f0, .1f0), -1f0,0f0)]\n\n@btime expectation(g, prob, u0_dist, p_dist, Koopman(), Tsit5(), EnsembleGPUArray(); \n                   quadalg = CubaSUAVE(), batch=1000)[1]"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "The performance gains realized by leveraging batch GPU processing is problem dependent. In this case, the number of batch evaluations required to overcome the overhead of using the GPU exceeds the number of simulations required to converge to the quadrature solution."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "using SciMLTutorials\nSciMLTutorials.tutorial_footer(WEAVE_ARGS[:folder],WEAVE_ARGS[:file])"
      ],
      "metadata": {},
      "execution_count": null
    }
  ],
  "nbformat_minor": 2,
  "metadata": {
    "language_info": {
      "file_extension": ".jl",
      "mimetype": "application/julia",
      "name": "julia",
      "version": "1.8.1"
    },
    "kernelspec": {
      "name": "julia-1.8",
      "display_name": "Julia 1.8.1",
      "language": "julia"
    }
  },
  "nbformat": 4
}
