{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "What if you have data and a general model and would like to evaluate the\nprobability that the fitted model outcomes would have had a given behavior?\nThe purpose of this tutorial is to demonstrate a fast workflow for doing exactly\nthis. It composes together a few different pieces of the SciML ecosystem:\n\n1. Parameter estimation with uncertainty with Bayesian differential equations by\n   integrating the differentiable differential equation solvers with the\n   [Turing.jl library](https://turing.ml/dev/).\n2. Fast calculation of probabilistic estimates of differential equation solutions\n   with parametric uncertainty using the Koopman expectation.\n3. GPU-acceleration of batched differential equation solves.\n\nLet's dive right in.\n\n## Bayesian Parameter Estimation with Uncertainty\n\nLet's start by importing all of the necessary libraries:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "using Turing, Distributions, DifferentialEquations\nusing MCMCChains, Plots, StatsPlots\nusing Random\nusing DiffEqUncertainty\nusing KernelDensity, DiffEqUncertainty\nusing Cuba, DiffEqGPU\n\nRandom.seed!(1);"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "For this tutorial we will use the Lotka-Volterra equation:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "function lotka_volterra(du,u,p,t)\n  @inbounds begin\n      x = u[1]\n      y = u[2]\n      α = p[1]\n      β = p[2]\n      γ = p[3]\n      δ = p[4]\n      du[1] = (α - β*y)*x\n      du[2] = (δ*x - γ)*y\n  end\nend\np = [1.5, 1.0, 3.0, 1.0]\nu0 = [1.0,1.0]\nprob1 = ODEProblem(lotka_volterra,u0,(0.0,10.0),p)\nsol = solve(prob1,Tsit5())\nplot(sol)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the Lotka-Volterra equation we will generate a dataset with known parameters:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "sol1 = solve(prob1,Tsit5(),saveat=0.1)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's assume our dataset should have noise. We can add this noise in and\nplot the noisy data against the generating set:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "odedata = Array(sol1) + 0.8 * randn(size(Array(sol1)))\nplot(sol1, alpha = 0.3, legend = false); scatter!(sol1.t, odedata')"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's assume that all we know is the data `odedata` and the model form.\nWhat we want to do is use the data to inform us of the parameters, but also\nget a probabilistic sense of the uncertainty around our parameter estimate. This\nis done via Bayesian estimation. For a full look at Bayesian estimation of\ndifferential equations, look at the [Bayesian differential equation](https://turing.ml/dev/tutorials/10-bayesiandiffeq/)\ntutorial from Turing.jl.\n\nFollowing that tutorial, we choose a set of priors and perform `NUTS` sampling\nto arrive at the MCMC chain:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "Turing.setadbackend(:forwarddiff)\n\n@model function fitlv(data, prob1)\n    σ ~ InverseGamma(2, 3) # ~ is the tilde character\n    α ~ truncated(Normal(1.5,0.5),1.0,2.0)\n    β ~ truncated(Normal(1.2,0.5),0.5,1.5)\n    γ ~ truncated(Normal(3.0,0.5),2,4)\n    δ ~ truncated(Normal(1.0,0.5),0.5,1.5)\n\n    p = [α,β,γ,δ]\n    prob = remake(prob1, p=p)\n    predicted = solve(prob,Tsit5(),saveat=0.1)\n\n    for i = 1:length(predicted)\n        data[:,i] ~ MvNormal(predicted[i], σ)\n    end\nend\n\nmodel = fitlv(odedata, prob1)\n\n# This next command runs 3 independent chains without using multithreading.\nchain = mapreduce(c -> sample(model, NUTS(.45),1000), chainscat, 1:3)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "This chain gives a discrete approximation to the probability distribution of our\ndesired quantites. We can plot the chains to see this distributions in action:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "plot(chain)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Great! From our data we have arrived at a probability distribution for the\nour parameter values.\n\n## Evaluating Model Hypotheses with the Koopman Expectation\n\nNow let's try and ask a question: what is the expected value of `x` (the first\nterm in the differential equation) at time `t=10` given the known uncertainties\nin our parameters? This is a good tutorial question because all other probabilistic\nstatements can be phrased similarly. Asking a question like, \"what is the probability\nthat `x(T) > 1` at the final time `T`?\", can similarly be phrased as an expected\nvalue (probability statements are expected values of characteristic functions\nwhich are 1 if true 0 if false). So in general, the kinds of questions we want\nto ask and answer are expectations about the solutions of the differential equation.\n\nThe trivial to solve this problem is to sample 100,000 sets of parameters from\nour parameter distribution given by the Bayesian estimation, solve the ODE\n100,000 times, and then take the average. But is 100,000 ODE solves enough?\nWell it's hard to tell, and even then, the convergence of this approach is slow.\nThis is the Monte Carlo approach and it converges to the correct answer by\n`sqrt(N)`. Slow.\n\nHowever, the [Koopman expectation](https://arxiv.org/abs/2008.08737) can converge\nwith much fewer points, allowing the use of higher order quadrature methods to\nconverge exponentially faster in many cases. To use the Koopman expectation\nfunctionality provided by [DiffEqUncertainty.jl](https://github.com/SciML/DiffEqUncertainty.jl),\nwe first need to define our observable function `g`. This function designates the\nthing about the solution we wish to calculate the expectation of. Thus for our\nquestion \"what is the expected value of `x`at time `t=10`?\", we would simply use:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "function g(sol)\n    sol[1,end]\nend"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we need to use the `expectation` call, where we need to provide our initial\ncondition and parameters as probability distirbutions. For this case, we will use\nthe same constant `u0` as before. But, let's turn our Bayesian MCMC chains into\ndistributions through [kernel density estimation](https://github.com/JuliaStats/KernelDensity.jl)\n(the plots of the distribution above are just KDE plots!)."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "p_kde = [kde(vec(Array(chain[:α]))),kde(vec(Array(chain[:β]))),\n         kde(vec(Array(chain[:γ]))),kde(vec(Array(chain[:δ])))]"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have our observable and our uncertainty distributions, let's calculate\nthe expected value:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "expect = expectation(g, prob1, u0, p_kde, Koopman(), Tsit5(), quadalg = CubaCuhre())"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note how that gives the expectation and a residual for the error bound!"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "expect.resid"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GPU-Accelerated Expectations\n\nAre we done? No, we need to add some GPUs! As mentioned earlier, probability\ncalculations can take quite a bit of ODE solves, so let's parallelize across\nthe parameters. [DiffEqGPU.jl](https://github.com/SciML/DiffEqGPU.jl) allows you\nto GPU-parallelize across parameters by using the\n[Ensemble interface](https://diffeq.sciml.ai/stable/features/ensemble/). Note that\nyou do not have to do any of the heavy lifting: all of the conversion to GPU\nkernels is done automaticaly by simply specifying `EnsembleGPUArray` as the\nensembling method. For example:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "function lotka_volterra(du,u,p,t)\n  @inbounds begin\n      x = u[1]\n      y = u[2]\n      α = p[1]\n      β = p[2]\n      γ = p[3]\n      δ = p[4]\n      du[1] = (α - β*y)*x\n      du[2] = (δ*x - γ)*y\n  end\nend\np = [1.5, 1.0, 3.0, 1.0]\nu0 = [1.0,1.0]\nprob = ODEProblem(lotka_volterra,u0,(0.0,10.0),p)\nprob_func = (prob,i,repeat) -> remake(prob,p=rand(Float64,4).*p)\nmonteprob = EnsembleProblem(prob, prob_func = prob_func, safetycopy=false)\n@time sol = solve(monteprob,Tsit5(),EnsembleGPUArray(),trajectories=10_000,saveat=1.0f0)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's now use this in the ensembling method. We need to specify a `batch` for the\nnumber of ODEs solved at the same time, and pass in our enembling method. The\nfollowing is a GPU-accelerated uncertainty quanitified estimate of the expectation\nof the solution:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "expectation(g, prob1, u0, p_kde, Koopman(), Tsit5(), EnsembleGPUArray(), batch=100, quadalg = CubaCuhre())"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "using SciMLTutorials\nSciMLTutorials.tutorial_footer(WEAVE_ARGS[:folder],WEAVE_ARGS[:file])"
      ],
      "metadata": {},
      "execution_count": null
    }
  ],
  "nbformat_minor": 2,
  "metadata": {
    "language_info": {
      "file_extension": ".jl",
      "mimetype": "application/julia",
      "name": "julia",
      "version": "1.8.1"
    },
    "kernelspec": {
      "name": "julia-1.8",
      "display_name": "Julia 1.8.1",
      "language": "julia"
    }
  },
  "nbformat": 4
}
