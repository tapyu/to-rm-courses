{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "These exercises teach common workflows which involve SciML's tools like\nDifferentialEquations.jl, DiffEqFlux.jl, and the connections to parts like\nstochastic differential equations and Bayesian estimation.\nThe designation (B) is for \"Beginner\", meaning that a user new to the package\nshould feel comfortable trying this exercise. An exercise designated (I) is\nfor \"Intermediate\", meaning the user may want to have some previous background\nin DifferentialEquations.jl or try some (B) exercises first. The additional\n(E) designation is for \"Experienced\", which are portions of exercises which may\ntake some work.\n\nThe exercises are described as follows:\n\n- Exercise 1 takes the user through solving a stiff ordinary differential equation\n  and using the ModelingToolkit.jl to automatically convert the function to a\n  symbolic form to derive the analytical Jacobian to speed up the solver. The\n  same biological system is then solved with stochasticity, utilizing\n  EnsembleProblems to understand 95% bounds on the solution. Finally,\n  probabilistic programming is employed to perform Bayesian parameter estimation\n  of the parameters against data.\n- Exercise 2 takes the user through defining hybrid delay differential equation,\n  that is a differential equation with events, and using differentiable programming\n  techniques (automatic differentiation) to to perform gradient-based parameter\n  estimation.\n- Exercise 3 takes the user through differential-algebraic equation (DAE)\n  modeling, the concept of index, and using both mass-matrix and implicit\n  ODE representations. This will require doing a bit of math, but the student\n  will understand how to change their equations to make their DAE numerically\n  easier for the integrators.\n- Exercise 4 takes the user through optimizing a PDE solver, utilizing\n  automatic sparsity pattern recognition, automatic conversion of numerical\n  codes to symbolic codes for analytical construction of the Jacobian,\n  preconditioned GMRES, and setting up a solver for IMEX and GPUs, and compute\n  adjoints of PDEs.\n- Exercise 5 focuses on a chaotic orbit, utilizing parallel ensembles across\n  supercomputers and GPUs to quickly describe phase space.\n- Exercise 6 takes the user through training a neural stochastic differential\n  equation, using GPU-accleration and adjoints through Flux.jl's neural\n  network framework to build efficient training codes.\n\nThis exercise worksheet is meant to be a living document leading new users through\na deep dive of the DifferentialEquations.jl feature set. If you further suggestions\nor want to contribute new problems, please open an issue or PR at the\nSciMLTutorials.jl repository.\n\n# Problem 1: Investigating Sources of Randomness and Uncertainty in a Stiff Biological System (B)\n\nIn this problem we will walk through the basics of simulating models with\nDifferentialEquations.jl. Let's take the\n[Oregonator model of the Belousov-Zhabotinskii chemical reaction system](https://www.radford.edu/~thompson/vodef90web/problems/demosnodislin/Demos_Pitagora/DemoOrego/demoorego.pdf).\nThis system describes a classical example in non-equilibrium thermodynmics\nand is a well-known natural chemical oscillator.\n\n## Part 1: Simulating the Oregonator ODE model\n\nWhen modeling, usually one starts off by investigating the deterministic model.\nThe deterministic ODE formulation of the Oregonator is\ngiven by the equations\n\n$$\\begin{align}\n\\frac{dx}{dt} &= s(y-xy + x - qx^2)\\\\\n\\frac{dy}{dt} &= (-y - xy + z)/s\\\\\n\\frac{dz}{dt} &= w(x - z)\\end{align}$$\n\nwith parameter values $s=77.27$, $w=0.161$, and $q=8.375 \\times 10^{-6}$, and\ninitial conditions $x(0)=1$, $y(0)=2$, and $z(0)=3$. Use\n[the tutorial on solving ODEs](https://docs.sciml.ai/dev/tutorials/ode_example)\nto solve this differential equation on the\ntimespan of $t\\in[0,360]$ with the default ODE solver. To investigate the result,\nplot the solution of all components over time, and plot the phase space plot of\nthe solution (hint: use `vars=(1,2,3)`). What shape is being drawn in phase space?\n\n## Part 2: Investigating Stiffness\n\nBecause the reaction rates of `q` vs `s` is very large, this model has a \"fast\"\nsystem and a \"slow\" system. This is typical of ODEs which exhibit a property\nknown as stiffness. Stiffness changes the ODE solvers which can handle the\nequation well. [Take a look at the ODE solver page](https://docs.sciml.ai/dev/solvers/ode_solve)\nand investigate solving the equation using methods for non-stiff equations\n(ex: `Tsit5`) and stiff equations (ex: `Rodas5`).\n\nBenchmark using $t\\in[0,50]$ using `@btime` from BenchmarkTools.jl. What\nhappens when you increase the timespan?\n\n## (Optional) Part 3: Specifying Analytical Jacobians (I)\n\nStiff ODE solvers internally utilize the Jacobian of the ODE system in order\nto improve the stepsizes in the solution. However, computing and factorizing\nthe Jacobian is costly, and thus it can be beneficial to provide the analytical\nsolution.\n\nUse the\n[ODEFunction definition page](https://docs.sciml.ai/dev/features/performance_overloads)\nto define an `ODEFunction` which holds both the OREGO ODE and its Jacobian, and solve using `Rodas5`.\n\n## (Optional) Part 4: Automatic Symbolicification and Analytical Jacobian Calculations\n\nDeriving Jacobians by hand is tedious. Thankfully symbolic mathematical systems\ncan do the work for you. And thankfully, DifferentialEquations.jl has tools\nto automatically convert numerical problems into symbolic problems to perform\nthe analysis on!\n\nfollow the [ModelingToolkit.jl README](https://github.com/JuliaDiffEq/ModelingToolkit.jl)\nto automatically convert your ODE definition\nto its symbolic form using `modelingtoolkitize` and calculate the analytical\nJacobian. Use the compilation functions to build the `ODEFunction` with the\nembedded analytical solution.\n\n## Part 5: Adding stochasticity with stochastic differential equations\n\nHow does this system react in the presense of stochasticity? We can investigate\nthis question by using stochastic differential equations. A stochastic\ndifferential equation formulation of this model is known as the multiplicative\nnoise model, is created with:\n\n$$\\begin{align}\ndx &= s(y-xy + x - qx^2)dt + \\sigma_1 x dW_1\\\\\ndy &= \\frac{-y - xy + z}{s}dt + \\sigma_2 y dW_2\\\\\ndz &= w(x - z)dt + \\sigma_3 z dW_3\\end{align}$$\n\nwith $\\sigma_i = 0.1$ where the `dW` terms describe a Brownian motion, a\ncontinuous random process with normally distributed increments. Use the\n[tutorial on solving SDEs](https://docs.sciml.ai/dev/tutorials/sde_example)\nto solve simulate this model. Then,\n[use the `EnsembleProblem`](https://docs.sciml.ai/dev/features/ensemble)\nto generate and plot 100 trajectories of the stochastic model, and use\n`EnsembleSummary` to plot the mean and 5%-95% region over time.\n\nTry solving with the `ImplicitRKMil` and `SOSRI` methods. Notice that it isn't\nstiff every single time!\n\n(For fun, see if you can make the Euler-Maruyama `EM()` method solve this equation.\nThis requires a choice of `dt` small enough to be stable. This is the \"standard\"\nmethod!)\n\n## Part 6: Gillespie jump models of discrete stochasticity\n\nWhen biological models have very few particles, continuous models no longer\nmake sense, and instead using the full discrete formulation can be required\nto accuracy describe the dynamics. A discrete differential equation, or\nGillespie model, is a continuous-time Markov chain with Poisson-distributed\njumps. A discrete description of the Oregonator model is given by a chemical\nreaction systems:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "A+Y -> X+P\nX+Y -> 2P\nA+X -> 2X + 2Z\n2X  -> A + P (note: this has rate kX^2!)\nB + Z -> Y"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "where reactions take place at a rate which is propoertional to its components,\ni.e. the first reaction has a rate `k*A*Y` for some `k`.\nUse the [tutorial on Gillespie SSA models](https://docs.sciml.ai/dev/tutorials/discrete_stochastic_example)\nto implement the `JumpProblem` for this model, and use the `EnsembleProblem`\nand `EnsembleSummary` to characterize the stochastic trajectories.\n\nFor what rate constants does the model give the oscillatory dynamics for the\nODE approximation? For information on the true reaction rates, consult\n[the original paper](https://pubs.acs.org/doi/abs/10.1021/ja00780a001).\n\n## Part 7: Probabilistic Programming / Bayesian Parameter Estimation with DiffEqBayes.jl + Turing.jl (I)\n\nIn many casees, one comes to understand the proper values for their model's\nparameters by utilizing data fitting techniques. In this case, we will use\nthe DiffEqBayes.jl library to perform a Bayesian estimation of the parameters.\nFor our data we will the following potential output:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "t = 0.0:1.0:30.0\ndata = [1.0 2.05224 2.11422 2.1857 2.26827 2.3641 2.47618 2.60869 2.7677 2.96232 3.20711 3.52709 3.97005 4.64319 5.86202 9.29322 536.068 82388.9 57868.4 1.00399 1.00169 1.00117 1.00094 1.00082 1.00075 1.0007 1.00068 1.00066 1.00065 1.00065 1.00065\n        2.0 1.9494 1.89645 1.84227 1.78727 1.73178 1.67601 1.62008 1.56402 1.50772 1.45094 1.39322 1.33366 1.2705 1.19958 1.10651 0.57194 0.180316 0.431409 251.774 591.754 857.464 1062.78 1219.05 1335.56 1419.88 1478.22 1515.63 1536.25 1543.45 1539.98\n        3.0 2.82065 2.68703 2.58974 2.52405 2.48644 2.47449 2.48686 2.52337 2.58526 2.67563 2.80053 2.9713 3.21051 3.5712 4.23706 12.0266 14868.8 24987.8 23453.4 19202.2 15721.6 12872.0 10538.8 8628.66 7064.73 5784.29 4735.96 3877.66 3174.94 2599.6]"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Follow the exmaples on the parameter estimation page](https://docs.sciml.ai/latest/analysis/parameter_estimation/#Bayesian-Inference-Examples-1)\nto perform a Bayesian parameter estimation. What are the most likely parameters\nfor the model given the posterior parameter distributions?\n\nUse the `ODEProblem` to perform the fit. If you have time, use the `EnsembleProblem`\nof `SDEProblem`s to perform a fit over averages of the SDE solutions. Note that\nthe SDE fit will take significantly more computational resources! See the GPU\nparallelism section for details on how to accelerate this.\n\n## (Optional) Part 8: Using DiffEqBiological's Reaction Network DSL\n\nDiffEqBiological.jl is a helper library for the DifferentialEquations.jl\necosystem for defining chemical reaction systems at a high leevel for easy\nsimulation in these various forms. Use the descrption\n[from the Chemical Reaction Networks documentation page](https://docs.sciml.ai/dev/models/biological)\nto build a reaction network and generate the ODE/SDE/jump equations, and\ncompare the result to your handcoded versions.\n\n# Problem 2: Fitting Hybrid Delay Pharmacokinetic Models with Automated Responses (B)\n\nHybrid differential equations are differential equations with events, where\nevents are some interaction that occurs according to a prespecified condition.\nFor example, the bouncing ball is a classic hybrid differential equation given\nby an ODE (Newton's Law of Gravity) mixed with the fact that, whenever the\nball hits the floor (`x=0`), then the velocity of the ball flips (`v=-v`).\n\nIn addition, many models incorporate delays, that is the driving force of the\nequation is dependent not on the current values, but values from the past.\nThese delay differential equations model how individuals in the economy act\non old information, or that biological processes take time to adapt to a new\nenvironment.\n\nIn this equation we will build a hybrid delayed pharmacokinetic model and\nuse the parameter estimation techniques to fit this it to a data.\n\n## Part 1: Defining an ODE with Predetermined Doses\n\nFirst, let's define the simplest hybrid ordinary differential equation: an ODE\nwhere the events take place at fixed times. The ODE we will use is known as\nthe one-compartment model:\n\n$$\\begin{align}\n\\frac{d[Depot]}{dt} &= -K_a [Depot] + R\\\\\n\\frac{d[Central]}{dt} &= K_a [Depot] - K_e [Central]\\end{align}$$\n\nwith $t \\in [0,90]$, $u_0 = [100.0,0]$, and $p=[K_a,K_e]=[2.268,0.07398]$.\n\nWith this model, use [the event handling documentation page](https://docs.sciml.ai/dev/features/callback_functions)\nto define a `DiscreteCallback` which fires at `t ∈ [24,48,72]` and adds a\ndose of 100 into `[Depot]`. (Hint: you'll want to set `tstops=[24,48,72]` to\nforce the ODE solver to step at these times).\n\n## Part 2: Adding Delays\n\nNow let's assume that instead of there being one compartment, there are many\ntransit compartment that the drug must move through in order to reach the\ncentral compartment. This effectively delays the effect of the transition from\n`[Depot]` to `[Central]`. To model this effect, we will use the delay\ndifferential equation which utilizes a fixed time delay $\\tau$:\n\n$$\\begin{align}\n\\frac{d[Depot]}{dt} &= -K_a [Depot](t)\\\\\n\\frac{d[Central]}{dt} &= K_a [Depot](t-\\tau) - K_e [Central]\\end{align}$$\n\nwhere the parameter $τ = 6.0$.\n[Use the DDE tutorial](https://docs.sciml.ai/dev/tutorials/dde_example)\nto define and solve this delayed version of the hybrid model.\n\n## Part 3: Automatic Differentiation (AD) for Optimization (I)\n\nIn order to fit parameters $(K_a,K_e,\\tau)$ we will want to be able to calculate\nthe gradient of the solution with respect to the initial conditions. One way to\ndo this is via Automatic Differentition (AD). For small numbers of parameters\n(<100), it is fastest to use Forward-Mode Automatic Differentition\n(even faster than using adjoint sensitivity analysis!). Thus for this problem\nwe will make use of ForwardDiff.jl to use Dual number arithmetic to retrive\nboth the solution and its derivative w.r.t. parameters in a single solve.\n\n[Use the information from the page on local sensitvity analysis](https://docs.sciml.ai/dev/analysis/sensitivity)\nto define the input dual numbers, solve the equation, and plot both the solution\nover time and the derivative of the solution w.r.t. the parameters.\n\n## Part 4: Fitting Known Quantities with DiffEqParamEstim.jl + Optim.jl\n\nNow let's fit the delayed model to a dataset. For the data, use the array"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "t = 0.0:12.0:90.0\ndata = [100.0 0.246196 0.000597933 0.24547 0.000596251 0.245275 0.000595453 0.245511\n        0.0 53.7939 16.8784 58.7789 18.3777 59.1879 18.5003 59.2611]"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use [the parameter estimation page](https://docs.sciml.ai/dev/analysis/parameter_estimation)\nto define a loss function with `build_loss_objective` and optimize the parameters\nagainst the data. What parameters were used to generate the data?\n\n## Part 5: Implementing Control-Based Logic with ContinuousCallbacks (I)\n\nNow that we have fit our delay differential equation model to the dataset, we\nwant to start testing out automated treatment strategies. Let's assume that\ninstead of giving doses at fixed time points, we invent a wearable which\nmonitors the patient and administers a dose whenever the internal drug\nconcentration falls below 25. To model this effect, we will need to use\n`ContinuousCallbacks` to define a callback that triggers when `[Central]` falls\nbelow the threshold value.\n\n[Use the documentation on the event handling page](https://docs.sciml.ai/dev/features/callback_functions) to define such a callback,\nand plot the solution over time. How many times does the auto-doser administer\na dose? How much does this change as you change the delay time $\\tau$?\n\n## Part 6: Global Sensitivity Analysis with the Morris and Sobol Methods\n\nTo understand how the parameters effect the solution in a global sense, one\nwants to use Global Sensitivity Analysis. Use the\n[GSA documentation page](https://docs.sciml.ai/dev/analysis/global_sensitivity)\nperform global sensitivity analysis and quantify the effect of the various\nparameters on the solution.\n\n# Problem 3: Differential-Algebraic Equation Modeling of a Double Pendulum (B)\n\nDifferential-Algebraic Equaton (DAE) systems are like ODEs but allow for adding\nconstraints into the models. This problem will look at solving the double\npenulum problem with enforcement of the rigid body constraints, requiring that\nthe total distance `L` is constant throughout the simulation. While these\nequations can be rewritten in an ODE form, in many cases it can be simpler\nto solve the equation directly with the constraints. This tutorial will\ncover both the idea of index, how to manually perform index reduction,\nand how to make use of mass matrix and implicit ODE solvers to handle these\nproblems.\n\n## Part 1: Simple Introduction to DAEs: Mass-Matrix Robertson Equations\n\nA mass-matrix ordinary differential equation (ODE) is an ODE where the\nleft-hand side, the derivative side, is multiplied by a matrix known as the\nmass matrix. This is described as:\n\n$$Mu' = f(u,p,t)$$\n\nwhere $M$ is the mass matrix. When $M$ is invertible, there is an ODE which is\nequivalent to this formulation. When $M$ is not invertible, this can have a\ndistinctly different behavior and is as Differential-Algebraic Equation (DAE).\n\nSolve the Robertson DAE:\n\n$$\\begin{align}\n\\frac{dy_1}{dt} &= -0.04y_1 + 10^4 y_2y_3\\\\\n\\frac{dy_2}{dt} &=  0.04y_1 - 10^4 y_2y_3 - 3\\times 10^7 y_2^2\\\\\n1 &= y_1 + y_2 + y_3\\end{align}$$\n\nwith $y(0) = [1,0,0]$ and $dy(0) = [-0.04,0.04,0.0]$ using the mass-matrix\nformulation and `Rodas5()`. Use the\n[ODEProblem page](https://docs.sciml.ai/dev/types/ode_types)\nto find out how to declare a mass matrix.\n\n(Hint: what if the last row has all zeros?)\n\n## Part 2: Solving the Implicit Robertson Equations with IDA\n\nUse the [DAE Tutorial](https://docs.sciml.ai/dev/tutorials/dae_example)\nto define a DAE in its implicit form and solve the Robertson equation with IDA.\nWhy is `differential_vars = [true,true,false]`?\n\n## Part 3: Manual Index Reduction of the Single Pendulum\n\nThe index of a DAE is a notion used to measure distance from\nits related ODE. There are many different definitions of index,\nbut we're going to stick to the idea of differential index:\nthe number of differentiations required to convert a system\nof DAEs into explicit ODE form. DAEs of high index are\nusually transformed via a procedure called index reduction.\nThe following example will demonstrate this.\n\nConsider the index 3 DAE system of the cartesian pendulum.\nAfter writing down the force equations in both directions,\nwe arrive at the following DAE:\n\n$$\n\\begin{align}\nm\\ddot{x} &= \\frac{x}{L}T \\\\\nm\\ddot{y} &= \\frac{y}{L}T - mg \\\\\nx^2 + y^2 &= L\n\\end{align}\n$$\n\nNotice that we don't have an equation describing the\nbehaviour of `T`. Let us now perform index reduction to\nextract an equation for `T`\n\nDifferentiate this third equation twice with respect to time\nto reduce it from index 3 to index 1.\n\n## Part 4: Single Pendulum Solution with IDA\nWrite these equations in implicit form and solve the system using\nIDA.\n\n## Part 5: Solving the Double Penulum DAE System\n\nThe following equations describe a double\npendulum system:\n$$\n\\begin{align}\nm_2\\ddot{x_2} &= \\frac{x_2}{L_2}T_2 \\\\\nm_2\\ddot{y_2} &= \\frac{y_2}{L_2}T_2 - m_2g \\\\\n{x_2}^2 + {y_2}^2 &= L_2 \\\\\nm_1\\ddot{x_1} &= \\frac{x_1}{L_1}T_1 - \\frac{x_2}{L_2}T_2 \\\\\nm_2\\ddot{y_1} &= \\frac{y_1}{L_1}T_2 - m_1g - \\frac{y_2}{L_2}T_2 \\\\\n{x_1}^2 + {y_1}^2 &= L_1 \\\\\n\\end{align}\n$$\n\nPerform index reduction and solve it like in the previous example.\n\n# Problem 4: Performance Optimizing and Parallelizing Semilinear PDE Solvers (I)\n\nThis problem will focus on implementing and optimizing the solution of the\n2-dimensional Brusselator equations. The BRUSS equations are a well-known\nhighly stiff oscillatory system of partial differential equations which are\nused in stiff ODE solver benchmarks. In this tutorial we will walk first\nthrough a simple implementation, then do allocation-free implementations and\nlooking deep into solver options and benchmarking.\n\n## Part 1: Implementing the BRUSS PDE System as ODEs\n\nThe Brusselator PDE is defined as follows:\n\n$$\\begin{align}\n\\frac{\\partial u}{\\partial t} &= 1 + u^2v - 4.4u + \\alpha(\\frac{\\partial^2 u}{\\partial x^2} + \\frac{\\partial^2 u}{\\partial y^2}) + f(x, y, t)\\\\\n\\frac{\\partial v}{\\partial t} &= 3.4u - u^2v + \\alpha(\\frac{\\partial^2 u}{\\partial x^2} + \\frac{\\partial^2 u}{\\partial y^2})\\end{align}$$\n\nwhere\n\n$$f(x, y, t) = \\begin{cases}\n5 & \\quad \\text{if } (x-0.3)^2+(y-0.6)^2 ≤ 0.1^2 \\text{ and } t ≥ 1.1 \\\\\n0 & \\quad \\text{else}\\end{cases}$$\n\nand the initial conditions are\n\n$$\\begin{align}\nu(x, y, 0) &= 22\\cdot y(1-y)^{3/2} \\\\\nv(x, y, 0) &= 27\\cdot x(1-x)^{3/2}\\end{align}$$\n\nwith the periodic boundary condition\n\n$$\\begin{align}\nu(x+1,y,t) &= u(x,y,t) \\\\\nu(x,y+1,t) &= u(x,y,t)\\end{align}$$\n\non a timespan of $t \\in [0,22]$.\n\nTo solve this PDE, we will discretize it into a system of ODEs with the finite\ndifference method. We discretize `u` and `v` into arrays of the values at each\ntime point: `u[i,j] = u(i*dx,j*dy)` for some choice of `dx`/`dy`, and same for\n`v`. Then our ODE is defined with `U[i,j,k] = [u v]`. The second derivative\noperator, the Laplacian, discretizes to become a tridiagonal matrix with\n`[1 -2 1]` and a `1` in the top right and bottom left corners. The nonlinear functions\nare then applied at each point in space (they are broadcast). Use `dx=dy=1/32`.\n\nYou will know when you have the correct solution when you plot the solution\nat `x=y=0` and see a periodic orbit, e.g., `ts=0:0.05:22; plot(ts, sol1.(ts,\nidxs=1))`.\n\nIf you are not familiar with this process, see\n[the Gierer-Meinhardt example from the SciMLTutorials.](http://tutorials.sciml.ai/html/introduction/03-optimizing_diffeq_code.html)\n\nNote: Start by doing the simplest implementation!\n\n## Part 2: Optimizing the BRUSS Code\n\nPDEs are expensive to solve, and so we will go nowhere without some code\noptimizing! Follow the steps described in the\n[the Gierer-Meinhardt example from the SciMLTutorials](http://tutorials.sciml.ai/html/introduction/03-optimizing_diffeq_code.html)\nto optimize your Brusselator code. Try other formulations and see what ends\nup the fastest! Find a trade-off between performance and simplicity that suits\nyour needs.\n\n## Part 3: Exploiting Jacobian Sparsity with Color Differentiation\n\nUse the `sparsity!` function from [SparseDiffTools](https://github.com/JuliaDiffEq/SparseDiffTools.jl)\nto generate the sparsity pattern for the Jacobian of this problem. Follow\nthe documentations [on the DiffEqFunction page](https://docs.sciml.ai/dev/features/performance_overloads)\nto specify the sparsity pattern of the Jacobian. Generate an add the color\nvector to speed up the computation of the Jacobian.\n\n## (Optional) Part 4: Structured Jacobians\n\nSpecify the sparsity pattern using a BlockBandedMatrix from\n[BlockBandedMatrices.jl](https://github.com/JuliaMatrices/BlockBandedMatrices.jl)\nto accelerate the previous sparsity handling tricks.\n\n## (Optional) Part 5: Automatic Symbolicification and Analytical Jacobian\n\nUse the `modelingtoolkitize` function from ModelingToolkit.jl to convert your\nnumerical ODE function into a symbolic ODE function and use that to compute and\nsolve with an analytical sparse Jacobian.\n\n## Part 6: Utilizing Preconditioned-GMRES Linear Solvers\n\nUse the [linear solver specification page](https://docs.sciml.ai/dev/features/linear_nonlinear)\nto solve the equation with `TRBDF2` with GMRES. Use the Sundials documentation\nto solve the equation with `CVODE_BDF` with Sundials' special internal GMRES.\nTo both of these, use the [AlgebraicMultigrid.jl](https://github.com/JuliaLinearAlgebra/AlgebraicMultigrid.jl)\nto add a preconditioner to the GMRES solver.\n\n## Part 7: Exploring IMEX and Exponential Integrator Techniques (E)\n\nInstead of using the standard `ODEProblem`, define a [`SplitODEProblem`](https://docs.sciml.ai/dev/types/split_ode_types)\nto move some of the equation to the \"non-stiff part\". Try different splits\nand solve with `KenCarp4` to see if the solution can be accelerated.\n\nNext, use `MatrixFreeOperator` and `DiffEqArrayOperator` to define part of the equation as linear, and\nuse the `ETDRK4` exponential integrator to solve the equation. Note that this\ntechnique is not appropriate for this equation since it relies on the\nnonlinear term being non-stiff for best results.\n\n## Part 8: Work-Precision Diagrams for Benchmarking Solver Choices\n\nUse the `WorkPrecisionSet` method from\n[DiffEqDevTools.jl](https://github.com/JuliaDiffEq/DiffEqDevTools.jl) to\nbenchmark multiple different solver methods and find out what combination is\nmost efficient.\n[Take a look at DiffEqBenchmarks.jl](https://github.com/JuliaDiffEq/DiffEqBenchmarks.jl)\nfor usage examples.\n\n## Part 9: GPU-Parallelism for PDEs (E)\n\nFully vectorize your implementation of the ODE and use a `CuArray` from\n[CuArrays.jl](https://github.com/JuliaGPU/CuArrays.jl) as the initial condition\nto cause the whole solution to be GPU accelerated.\n\n## Part 10: Adjoint Sensitivity Analysis for Gradients of PDEs\n\nIn order to optimize the parameters of a PDE, you need to be able to compute\nthe gradient of the solution with respect to the parameters. This is done\nthrough sensitivity analysis. For PDEs, generally the system is at a scale\nwhere forward sensitivity analysis (forward-mode automatic differentiation)\nis no longer suitable, and for these cases one uses adjoint sensitivity analysis.\n\nRewrite the PDE so the constant terms are parameters, and use the\n[adjoint sensitivity analysis](https://docs.sciml.ai/latest/analysis/sensitivity/#Adjoint-Sensitivity-Analysis-1)\ndocumentation to solve for the solution gradient with a cost function being the\nL2 distance of the solution from the value 1. Solve with interpolated and\ncheckpointed adjoints. Play with using reverse-mode automatic differentiation\nvs direct computation of vector-Jacobian products using the `autojacvec` option\nof the `SensitivityAlg`. Find the set of options most suitable for this PDE.\n\nIf you have compute time, use this adjoint to optimize the parameters of the\nPDE with respect to this cost function.\n\n# Problem 5: Global Parameter Sensitivity and Optimality with GPU and Distributed Ensembles (B)\n\nIn this example we will investigate how the parameters \"generally\" effect the\nsolution in the chaotic Henon-Heiles system. By \"generally\" we will use global\nsensitivity analysis methods to get an average global characterization of the\nparameters on the solution. In addition to a global sensitivity approach, we\nwill generate large ensembles of solutions with different parameters using\na GPU-based parallelism approach.\n\n## Part 1: Implementing the Henon-Heiles System (B)\n\nThe Henon-Heiles Hamiltonian system is described by the ODEs:\n\n$$\\begin{align}\n\\frac{dp_1}{dt} &= -q_1 (1 + 2q_2)\\\\\n\\frac{dp_2}{dt} &= -q_2 - (q_1^2 - q_2^2)\\\\\n\\frac{dq_1}{dt} &= p_1\\\\\n\\frac{dq_2}{dt} &= p_2\\end{align}$$\n\nwith initial conditions $u_0 = [0.1,0.0,0.0,0.5]$.\nSolve this system over the timespan $t\\in[0,1000]$\n\n## (Optional) Part 2: Alternative Dynamical Implmentations of Henon-Heiles (B)\n\nThe Henon-Heiles defines a Hamiltonian system with certain structures which\ncan be utilized for a more efficient solution. Use [the Dynamical problems page](https://docs.sciml.ai/dev/types/dynamical_types)\nto define a `SecondOrderODEProblem` corresponding to the acceleration terms:\n\n$$\\begin{align}\n\\frac{d^2q_1}{dt^2} &= -q_1 (1 + 2q_2)\\\\\n\\frac{d^2q_2}{dt^2} &= -q_2 - (q_1^2 - q_2^2)\\end{align}$$\n\nSolve this with a method that is specific to dynamical problems, like `DPRKN6`.\n\nThe Hamiltonian can also be directly described:\n\n$$H(p,q) = \\frac{1}{2}(p_1^2 + p_2^2) + \\frac{1}{2}(q_1^2+q_2^2+2q_1^2 q_2 - \\frac{2}{3}q_2^3)$$\n\nSolve this problem using the `HamiltonianProblem` constructor from DiffEqPhysics.jl.\n\n## Part 3: Parallelized Ensemble Solving\n\nTo understand the orbits of the Henon-Heiles system, it can be useful to solve\nthe system with many different initial conditions. Use the\n[ensemble interface](https://docs.sciml.ai/dev/features/ensemble)\nto solve with randomized initial conditions in parallel using threads with\n`EnsembleThreads()`. Then, use `addprocs()` to add more cores and solve using\n`EnsembleDistributed()`. The former will solve using all of the cores on a\nsingle computer, while the latter will use all of the cores on which there\nare processors, which can include thousands across a supercomputer! See\n[Julia's parallel computing setup page](https://docs.julialang.org/en/v1/manual/parallel-computing/index.html)\nfor more details on the setup.\n\n## Part 4: Parallelized GPU Ensemble Solving\n\nSetup the CUDAnative.jl library and use the `EnsembleGPUArray()` method to\nparallelize the solution across the thousands of cores of a GPU. Note that\nthis will efficiency solve for hundreds of thousands of trajectores.\n\n# Problem 6: Training Neural Stochastic Differential Equations with GPU acceleration (I)\n\nIn the previous models we had to define a model. Now let's shift the burden of\nmodel-proofing onto data by utilizing neural differential equations. A neural\ndifferential equation is a differential equation where the model equations\nare replaced, either in full or in part, by a neural network. For example, a\nneural ordinary differential equation is an equation $u^\\prime = f(u,p,t)$\nwhere $f$ is a neural network. We can learn this neural network from data using\nvarious methods, the easiest of which is known as the single shooting method,\nwhere one chooses neural network parameters, solves the equation, and checks\nthe ODE's solution against data as a loss.\n\nIn this example we will define and train various forms of neural differential\nequations. Note that all of the differential equation types are compatible with\nneural differential equations, so this is only going to scratch the surface of\nthe possibilites!\n\n## Part 1: Constructing and Training a Basic Neural ODE\n\nUse the [DiffEqFlux.jl README](https://github.com/JuliaDiffEq/DiffEqFlux.jl) to\nconstruct a neural ODE to train against the training data:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "u0 = Float32[2.; 0.]\ndatasize = 30\ntspan = (0.0f0,1.5f0)\n\nfunction trueODEfunc(du,u,p,t)\n    true_A = [-0.1 2.0; -2.0 -0.1]\n    du .= ((u.^3)'true_A)'\nend\nt = range(tspan[1],tspan[2],length=datasize)\nprob = ODEProblem(trueODEfunc,u0,tspan)\node_data = Array(solve(prob,Tsit5(),saveat=t))"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 2: GPU-accelerating the Neural ODE Process\n\nUse the `gpu` function from Flux.jl to transform all of the calculations onto\nthe GPU and train the neural ODE using GPU-accelerated `Tsit5` with adjoints.\n\n## Part 3: Defining and Training a Mixed Neural ODE\n\nGather data from the Lotka-Volterra equation:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "function lotka_volterra(du,u,p,t)\n  x, y = u\n  α, β, δ, γ = p\n  du[1] = dx = α*x - β*x*y\n  du[2] = dy = -δ*y + γ*x*y\nend\nu0 = [1.0,1.0]\ntspan = (0.0,10.0)\np = [1.5,1.0,3.0,1.0]\nprob = ODEProblem(lotka_volterra,u0,tspan,p)\nsol = Array(solve(prob,Tsit5())(0.0:1.0:10.0))"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now use the\n[mixed neural section of the documentation](https://github.com/JuliaDiffEq/DiffEqFlux.jl#mixed-neural-des)\nto define the mixed neural ODE where the functional form of $\\frac{dx}{dt}$ is\nknown, and try to derive a neural formulation for $\\frac{dy}{dt}$ directly from\nthe data.\n\n## Part 4: Constructing a Basic Neural SDE\n\nGenerate data from the Lotka-Volterra equation with multiplicative noise"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "function lotka_volterra(du,u,p,t)\n  x, y = u\n  α, β, δ, γ = p\n  du[1] = dx = α*x - β*x*y\n  du[2] = dy = -δ*y + γ*x*y\nend\nfunction lv_noise(du,u,p,t)\n  du[1] = p[5]*u[1]\n  du[2] = p[6]*u[2]\nend\nu0 = [1.0,1.0]\ntspan = (0.0,10.0)\np = [1.5,1.0,3.0,1.0,0.1,0.1]\nprob = SDEProblem(lotka_volterra,lv_noise,u0,tspan,p)\nsol = [Array(solve(prob,SOSRI())(0.0:1.0:10.0)) for i in 1:20] # 20 solution samples"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train a neural stochastic differential equation $dX = f(X)dt + g(X)dW_t$ where\nboth the drift ($f$) and the diffusion ($g$) functions are neural networks.\nSee if constraining $g$ can make the problem easier to fit.\n\n## Part 5: Optimizing the training behavior with minibatching (E)\n\nUse minibatching on the data to improve the training procedure. An example\n[can be found at this PR](https://github.com/FluxML/model-zoo/pull/88)."
      ],
      "metadata": {}
    }
  ],
  "nbformat_minor": 2,
  "metadata": {
    "language_info": {
      "file_extension": ".jl",
      "mimetype": "application/julia",
      "name": "julia",
      "version": "1.8.1"
    },
    "kernelspec": {
      "name": "julia-1.8",
      "display_name": "Julia 1.8.1",
      "language": "julia"
    }
  },
  "nbformat": 4
}
