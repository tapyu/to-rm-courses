using LinearAlgebra, DSP, Plots, LaTeXStrings
Î£ = sum

N = 200 # number of samples
ğ‘ = I(2) # correlation matrix
ğ© = [1, 1.6] # cross-correlation vector between the desired and input signals
ğ± = randn(N) # input vector
ğ¡ = [1, 1.6] # filter coefficients
Î¼ = .1

ğ = rand(N)
for n âˆˆ 2:N
    ğ±â‚â‚™â‚ = [ğ±[n], ğ±[n-1]] # input vector at the instant n
    ğ[n] = ğ¡ â‹… ğ±â‚â‚™â‚ # d(n)
end

# steepest descent
ğ°â‚â‚™â‚ = rand(2) # initial guess of the coefficient vector
ğ– = rand(2, N) # save the coefficient vector evolution
ğ–[:,1] = ğ°â‚â‚™â‚ # save initial position
ğ² = rand(N) # output signal
ğ”¼eÂ² = zeros(N) # error signal
for n âˆˆ 2:N
    ğ±â‚â‚™â‚ = [ğ±[n], ğ±[n-1]] # input vector at the instant n
    ğ²[n] = ğ°â‚â‚™â‚ â‹… ğ±â‚â‚™â‚ # y(n)
    ğ”¼eÂ²[n] = ((n-2)*ğ”¼eÂ²[n-1] + (ğ[n] - ğ²[n])^2)/(n-1)
    ğ â‚â‚™â‚ = -2ğ© + 2ğ‘*ğ°â‚â‚™â‚ # deterministic gradient
    global ğ°â‚â‚™â‚ -= Î¼*ğ â‚â‚™â‚
    ğ–[:,n] = ğ°â‚â‚™â‚
end
p1 = plot([ğ² ğ], title="Steepest descent", label=[L"\mathbf{w}(n) = \mathbf{w}(n) - \mu \mathbf{g}(n)" L"d(n)"])
e1 = plot(ğ”¼eÂ², title="MSE of the Steepest descent", label=L"\mathbb{E}[e^2(n)]")

# Newton's algorithm
ğ°â‚â‚™â‚ = rand(2) # initial guess of the coefficient vector
ğ² = rand(N) # output signal
ğ”¼eÂ² = zeros(N) # error signal
ğ‡ = 2ğ‘ # the Hessian
for n âˆˆ 2:N
    ğ±â‚â‚™â‚ = [ğ±[n], ğ±[n-1]] # input vector at the instant n
    ğ²[n] = ğ°â‚â‚™â‚ â‹… ğ±â‚â‚™â‚ # y(n)
    ğ”¼eÂ²[n] = ((n-2)*ğ”¼eÂ²[n-1] + (ğ[n] - ğ²[n])^2)/(n-1)
    ğ â‚â‚™â‚ = -2ğ© + 2ğ‘*ğ°â‚â‚™â‚ # deterministic gradient
    Î”ğ°â‚â‚™â‚Šâ‚â‚ = -inv(ğ‡)*ğ â‚â‚™â‚
    global ğ°â‚â‚™â‚ += Î”ğ°â‚â‚™â‚Šâ‚â‚
end
p2 = plot([ğ² ğ], title="Newton's algorithm" , label=[L"\mathbf{w}(n) = \mathbf{w}(n) + \mu \mathbf{H}^{-1}\mathbf{g}(n)" L"d(n)"])
e2 = plot(ğ”¼eÂ², title="MSE of the Newton's algorithm", label=L"\mathbb{E}[e^2(n)]")

# Least-Mean Squares (LMS) algorithm
ğ°â‚â‚™â‚ = rand(2) # initial guess of the coefficient vector
ğ² = rand(N) # output signal
ğ”¼eÂ² = zeros(N) # error signal
for n âˆˆ 2:N
    ğ±â‚â‚™â‚ = [ğ±[n], ğ±[n-1]] # input vector at the instant n
    ğ²[n] = ğ°â‚â‚™â‚ â‹… ğ±â‚â‚™â‚ # y(n)
    eâ‚â‚™â‚ = ğ[n] - ğ²[n]
    ğ”¼eÂ²[n] = ((n-2)*ğ”¼eÂ²[n-1] + eâ‚â‚™â‚^2)/(n-1)
    ğ Ì‚â‚â‚™â‚ = -2eâ‚â‚™â‚*ğ±â‚â‚™â‚ # stochastic gradient
    global ğ°â‚â‚™â‚ -= Î¼*ğ Ì‚â‚â‚™â‚
end
p3 = plot([ğ² ğ], title="LMS algorithm", label=[L"\mathbf{w}(n) = \mathbf{w}(n) + 2\mu e(n)\mathbf{x}(n)" L"d(n)"])
e3 = plot(ğ”¼eÂ², title="MSE of the LMS algorithm", label=L"\mathbb{E}[e^2(n)]")

# normalized LMS algorithm
ğ°â‚â‚™â‚ = rand(2) # initial guess of the coefficient vector
Î³ = 1
ğ² = rand(N) # output signal
ğ”¼eÂ² = zeros(N) # error signal
for n âˆˆ 2:N
    ğ±â‚â‚™â‚ = [ğ±[n], ğ±[n-1]] # input vector at the instant n
    ğ²[n] = ğ°â‚â‚™â‚ â‹… ğ±â‚â‚™â‚ # y(n)
    eâ‚â‚™â‚ = ğ[n] - ğ²[n]
    ğ”¼eÂ²[n] = ((n-2)*ğ”¼eÂ²[n-1] + eâ‚â‚™â‚^2)/(n-1)
    ğ Ì‚â‚â‚™â‚ = -2eâ‚â‚™â‚*ğ±â‚â‚™â‚ # stochastic gradient
    global ğ°â‚â‚™â‚ -= Î¼*ğ Ì‚â‚â‚™â‚/(ğ±â‚â‚™â‚â‹…ğ±â‚â‚™â‚ + Î³)
end
p4 = plot([ğ² ğ], title="Normalized LMS algorithm", label=[L"\mathbf{w}(n) = \mathbf{w}(n) + \frac{2\mu e(n)\mathbf{x}(n)}{\mathbf{x}^\mathrm{T}(n)\mathbf{x}(n) + \gamma}" L"d(n)"])
e4 = plot(ğ”¼eÂ², title="MSE of the normalized LMS algorithm", label=L"\mathbb{E}[e^2(n)]")

fig = plot(p1, p2, p3, p4, layout=(4,1), size=(1200,800))
savefig(fig, "figs/q4.png")

fig = plot(e1, e2, e3, e4, layout=(4,1), size=(1200,800))
savefig(fig, "figs/q4-error-evolution.png")