using LinearAlgebra, Plots, LaTeXStrings

Nâ‚› = 200 # number of samples
ğ± = randn(Nâ‚›)
# ğ± = 2rand(Nâ‚›) .- 1 # ~ U(-1, 1)
Î¼â‚˜â‚â‚“ = 1/12
N = 13 # 12 tapped delay plus the bias

# system output
ğÊ¼ = rand(Nâ‚›)
for n âˆˆ N:Nâ‚›
    ğÊ¼[n] = ğÊ¼[n-1] + ğ±[n] - ğ±[n-12]
end

# reference signal (system output + noise)
ğ§ = âˆš(1e-3)randn(Nâ‚›) # ~ N(0, 1e-3)
ğ = ğÊ¼ + ğ§

plots = Array{Plots.Plot{Plots.GRBackend}, 1}(undef,4) # or Vector{Plots.GRBackend}(undef,4)
for (j, i) âˆˆ enumerate((1, 2, 10, 50))
    Î¼ = Î¼â‚˜â‚â‚“/i # step learning
    # the LMS algorithm
    ğ°â‚â‚™â‚ = rand(N) # initial coefficient vector
    ğ² = [fill(NaN, N); rand(Nâ‚›-N)] # adaptive filter output
    for n âˆˆ N:Nâ‚›
        ğ±â‚â‚™â‚ = ğ±[n:-1:n-12] # input signal
        ğ²[n] = ğ±â‚â‚™â‚ â‹… ğ°â‚â‚™â‚ # adaptive filter output
        eâ‚â‚™â‚ = ğ[n] - ğ²[n] # signal error
        ğ°â‚â‚™â‚ += 2Î¼*eâ‚â‚™â‚*ğ±â‚â‚™â‚
    end
    plots[j] = plot([ğ ğ²], label=[L"d(n)" L"y(n)"*" for "*L"\mu=\mu_{max}"*(i!=1 ? "/$(i)" : "")], xlabel="n")
end

fig = plot(plots..., layout=(4,1), size=(1200,800))

savefig(fig, "figs/q5_lms_algorithm.png")