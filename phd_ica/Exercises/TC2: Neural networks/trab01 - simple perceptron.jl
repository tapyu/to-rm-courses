using FileIO, JLD2, Random, LinearAlgebra, Plots, LaTeXStrings
Î£=sum

## load the data
ğ—, labels = FileIO.load("Dataset/Iris [uci]/iris.jld2", "ğ—", "ğ") # ğ— â¡ [attributes X instances]
# PS choose only one!!!
# uncomment â†“ if you want to train for all attributes
ğ— = [fill(-1, size(ğ—,2))'; ğ—] # add the -1 input (bias)
# uncomment â†“ if you want to train for petal length and width (to plot the decision surface)
# ğ— = [fill(-1, size(ğ—,2))'; ğ—[3:4,:]] # add the -1 input (bias)

## useful functions
function shuffle_dataset(ğ—, ğ)
    shuffle_indices = Random.shuffle(1:size(ğ—,2))
    return ğ—[:, shuffle_indices], ğ[shuffle_indices]
end

function train(ğ—â‚œáµ£â‚™, ğâ‚œáµ£â‚™, ğ°)
    ğâ‚œáµ£â‚™ = rand(length(ğâ‚œáµ£â‚™)) # vector of errors
    Ï† = uâ‚™ -> uâ‚™â‰¥0 ? 1 : 0 # activation function of the simple Perceptron
    for (n, (ğ±â‚™, dâ‚™)) âˆˆ enumerate(zip(eachcol(ğ—â‚œáµ£â‚™), ğâ‚œáµ£â‚™))
        Î¼â‚™ = dot(ğ±â‚™,ğ°) # inner product
        yâ‚™ = Ï†(Î¼â‚™) # for the training phase, you do not pass yâ‚™ to a harder decisor (the McCulloch and Pitts's activation function) since you are in intended to classify yâ‚™. Rather, you are interested in updating ğ° (???)
        eâ‚™ = dâ‚™ - yâ‚™
        ğ° += Î±*eâ‚™*ğ±â‚™
        ğâ‚œáµ£â‚™[n] = eâ‚™
    end
    Ïµâ‚œáµ£â‚™ = sum(ğâ‚œáµ£â‚™)/length(ğâ‚œáµ£â‚™) # the accuracy for this epoch
    return ğ°, Ïµâ‚œáµ£â‚™
end

function test(ğ—â‚œâ‚›â‚œ, ğâ‚œâ‚›â‚œ, ğ°, is_confusion_matrix=false)
    Ï† = uâ‚™ -> uâ‚™>0 ? 1 : 0 # activation function of the simple Perceptron
    ğâ‚œâ‚›â‚œ = rand(length(ğâ‚œâ‚›â‚œ)) # vector of errors
    for (n, (ğ±â‚™, dâ‚™)) âˆˆ enumerate(zip(eachcol(ğ—â‚œâ‚›â‚œ), ğâ‚œâ‚›â‚œ))
        Î¼â‚™ = ğ±â‚™â‹…ğ° # inner product
        yâ‚™ = Ï†(Î¼â‚™) # for the simple Perceptron, yâ‚™ âˆˆ {0,1}. Therefore, it is not necessary to pass yâ‚™ to a harder decisor since Ï†(â‹…) already does this job
        ğâ‚œâ‚›â‚œ[n] = dâ‚™ - yâ‚™
    end
    if !is_confusion_matrix # return the accuracy for this realization
        Ïµâ‚œâ‚›â‚œ = sum(ğâ‚œâ‚›â‚œ)/length(ğâ‚œâ‚›â‚œ)
        return Ïµâ‚œâ‚›â‚œ
    else
        return Int.(ğâ‚œâ‚›â‚œ) # return the errors over the instances to plot the confusion matrix
    end
end

## algorithm hyperparameters
Náµ£ = 20 # number of realizations
Nâ‚ = size(ğ—, 1) # =5 (including bias) number of Attributes, that is, input vector size at each intance. They mean: sepal length, sepal width, petal length, petal width
N = size(ğ—, 2) # =150 number of instances(samples)
Nâ‚œáµ£â‚™ = 80 # % percentage of instances for the train dataset
Nâ‚œâ‚›â‚œ = 20 # % percentage of instances for the test dataset
Nâ‚‘ = 100 # number of epochs
Î± = 0.01 # learning step

## init
all_ğ›œÌ„â‚œâ‚›â‚œ, all_ÏƒÂ²â‚‘, all_ğ°â‚’â‚šâ‚œ = rand(3), rand(3), rand(Nâ‚,3)
for (i, desired_label) âˆˆ enumerate(("setosa", "virginica", "versicolor"))
    local ğ = labels.==desired_label # dâ‚™ âˆˆ {0,1}
    ğ›œâ‚œâ‚›â‚œ = rand(Náµ£) # vector that stores the error test dataset for each realization (to compute the final statistics)
    for náµ£ âˆˆ 1:Náµ£ # for each realization
        # initializing!
        ğ›œâ‚œáµ£â‚™ = rand(Nâ‚‘) # vector that stores the error train dataset for each epoch (to see its evolution)
        global ğ° = ones(Nâ‚) # initialize a new McCulloch-Pitts neuron (a new set of parameters)
        global ğ— # ?

        # prepare the data!
        ğ—, ğ = shuffle_dataset(ğ—, ğ)
        # hould-out
        global ğ—â‚œáµ£â‚™ = ğ—[:,1:(N*Nâ‚œáµ£â‚™)Ã·100]
        global ğâ‚œáµ£â‚™ = ğ[1:(N*Nâ‚œáµ£â‚™)Ã·100]
        global ğ—â‚œâ‚›â‚œ = ğ—[:,length(ğâ‚œáµ£â‚™)+1:end]
        global ğâ‚œâ‚›â‚œ = ğ[length(ğâ‚œáµ£â‚™)+1:end]

        # train and test!
        for nâ‚‘ âˆˆ 1:Nâ‚‘ # for each epoch
            ğ°, ğ›œâ‚œáµ£â‚™[nâ‚‘] = train(ğ—â‚œáµ£â‚™, ğâ‚œáµ£â‚™, ğ°)
            ğ—â‚œáµ£â‚™, ğâ‚œáµ£â‚™ = shuffle_dataset(ğ—â‚œáµ£â‚™, ğâ‚œáµ£â‚™)
        end
        ğ›œâ‚œâ‚›â‚œ[náµ£] = test(ğ—â‚œâ‚›â‚œ, ğâ‚œâ‚›â‚œ, ğ°)
        
        # make plots!
        if náµ£ == 1
            all_ğ°â‚’â‚šâ‚œ[:,i] = ğ° # save the optimum value reached during the 1th realization for setosa, versicolor, and virginica
            # if all attributes was taken into account, compute the accuracyxepochs for all classes
            if length(ğ°) != 3
                local p = plot(ğ›œâ‚œáµ£â‚™, label="", xlabel=L"Epochs", ylabel=L"\epsilon_n", linewidth=2, title="Training accuracy for $(desired_label) class by epochs")
                display(p)
                savefig(p, "figs/trab1 (simple perceptron)/epsilon_n-by-epochs-for$(desired_label).png")
                # for the setosa class, compute the confusion matrix
                if desired_label == "setosa"
                    ğ‚ = zeros(2,2) # confusion matrix
                    ğâ‚œâ‚›â‚œ = test(ğ—â‚œâ‚›â‚œ, ğâ‚œâ‚›â‚œ, ğ°, true)
                    for n âˆˆ 1:length(ğâ‚œâ‚›â‚œ)
                        # predicted x true label
                        ğ‚[ğâ‚œâ‚›â‚œ[n]-ğâ‚œâ‚›â‚œ[n]+1, ğâ‚œâ‚›â‚œ[n]+1] += 1
                    end
                    h = heatmap(ğ‚, xlabel="Predicted labels", ylabel="True labels", xticks=(1:2, ("setosa", "not setosa")), yticks=(1:2, ("setosa", "not setosa")), title="Confusion matrix for the setosa class")
                    savefig(h, "figs/trab1 (simple perceptron)/setosa-heatmap.png")
                    display(h) # TODO: put the number onto each heatmap square
                end
            end
            # decision surface
            if length(ğ°) == 3 # plot the surface only if the learning procedure was taken with only two attributes, the petal length and petal width (equals to 3 because the bias)
                Ï† = uâ‚™ -> uâ‚™â‰¥0 ? 1 : 0 # activation function of the simple Perceptron
                xâ‚ƒ_range = floor(minimum(ğ—[2,:])):.1:ceil(maximum(ğ—[2,:]))
                xâ‚„_range = floor(minimum(ğ—[3,:])):.1:ceil(maximum(ğ—[3,:]))
                y(xâ‚ƒ, xâ‚„) = Ï†(dot([-1, xâ‚ƒ, xâ‚„], ğ°))
                p = surface(xâ‚ƒ_range, xâ‚„_range, y, camera=(60,40,0), xlabel = "petal length", ylabel = "petal width", zlabel="decision surface")


                # scatter plot for the petal length and petal length width for the setosa class
                # train and desired label 
                scatter!(ğ—â‚œáµ£â‚™[2,ğâ‚œáµ£â‚™.==1], ğ—â‚œáµ£â‚™[3,ğâ‚œáµ£â‚™.==1], ones(length(filter(x->x==1, ğâ‚œáµ£â‚™))),
                        markershape = :hexagon,
                        markersize = 4,
                        markeralpha = 0.6,
                        markercolor = :green,
                        markerstrokewidth = 3,
                        markerstrokealpha = 0.2,
                        markerstrokecolor = :black,
                        xlabel = "petal\nlength",
                        ylabel = "petal width",
                        camera = (60,40,0),
                        label = "$(desired_label) train set")
                
                # test and desired label 
                scatter!(ğ—â‚œâ‚›â‚œ[2,ğâ‚œâ‚›â‚œ.==1], ğ—â‚œâ‚›â‚œ[3,ğâ‚œâ‚›â‚œ.==1], ones(length(filter(x->x==1, ğâ‚œâ‚›â‚œ))),
                        markershape = :cross,
                        markersize = 4,
                        markeralpha = 0.6,
                        markercolor = :green,
                        markerstrokewidth = 3,
                        markerstrokealpha = 0.2,
                        markerstrokecolor = :black,
                        xlabel = "petal\nlength",
                        ylabel = "petal width",
                        camera = (60,40,0),
                        label = "$(desired_label) test set")

                # train and not desired label 
                scatter!(ğ—â‚œáµ£â‚™[2,ğâ‚œáµ£â‚™.==0], ğ—â‚œáµ£â‚™[3,ğâ‚œáµ£â‚™.==0], zeros(length(filter(x->x==0, ğâ‚œáµ£â‚™))),
                        markershape = :hexagon,
                        markersize = 4,
                        markeralpha = 0.6,
                        markercolor = :red,
                        markerstrokewidth = 3,
                        markerstrokealpha = 0.2,
                        markerstrokecolor = :black,
                        xlabel = "petal\nlength",
                        ylabel = "petal width",
                        camera = (60,40,0),
                        label = "not $(desired_label) train set")

                # test and not desired label 
                scatter!(ğ—â‚œâ‚›â‚œ[2,ğâ‚œâ‚›â‚œ.==0], ğ—â‚œâ‚›â‚œ[3,ğâ‚œâ‚›â‚œ.==0], zeros(length(filter(x->x==0, ğâ‚œâ‚›â‚œ))),
                        markershape = :cross,
                        markersize = 4,
                        markeralpha = 0.6,
                        markercolor = :red,
                        markerstrokewidth = 3,
                        markerstrokealpha = 0.2,
                        markerstrokecolor = :black,
                        xlabel = "petal\nlength",
                        ylabel = "petal width",
                        camera = (60,40,0),
                        label = "not $(desired_label) test set")
                
                title!("Decision surface for the class $(desired_label)")
                display(p)
                savefig(p,"figs/trab1 (simple perceptron)/decision-surface-for-$(desired_label).png")
            end
        end
    end
    ğ›œÌ„â‚œâ‚›â‚œ = sum(ğ›œâ‚œâ‚›â‚œ)/length(ğ›œâ‚œâ‚›â‚œ) # mean of the accuracy of all realizations
    ğ”¼ğ›œÂ² = Î£(ğ›œâ‚œâ‚›â‚œ.^2)/length(ğ›œâ‚œâ‚›â‚œ) # MSE (Mean squared erro), that is, the second moment of realization accuracies
    ÏƒÂ²â‚‘ = ğ”¼ğ›œÂ² - ğ›œÌ„â‚œâ‚›â‚œ^2 # variance of all realization accuracies
    
    # save the performance
    all_ğ›œÌ„â‚œâ‚›â‚œ[i] = ğ›œÌ„â‚œâ‚›â‚œ
    all_ÏƒÂ²â‚‘[i] = ÏƒÂ²â‚‘

end