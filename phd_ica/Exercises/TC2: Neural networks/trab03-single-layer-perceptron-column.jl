using FileIO, JLD2, Random, LinearAlgebra, Plots, LaTeXStrings
Î£=sum

### Vertebral column ###

## parameters and hyperparameters
N = 310 # number of instances
Nâ‚ = 6 # number of number of attributes, that is, input vector size at each intance n
c = 3 # number of perceptrons (neurons) of the single layer
Náµ£ = 20 # number of realizations
Nâ‚‘ = 100 # number of epochs
Nâ‚œáµ£â‚™ = 80 # % percentage of instances for the train dataset
Nâ‚œâ‚›â‚œ = 20 # % percentage of instances for the test dataset
Î± = 0.001 # learning step

## load the data
ğ—, labels = FileIO.load("Dataset/Vertebral column [uci]/dataset_3classes.jld2", "ğ—", "ğ")

## useful functions
function shuffle_dataset(ğ—, ğƒ)
    shuffle_indices = Random.shuffle(1:size(ğ—,2))
    return ğ—[:, shuffle_indices], ğƒ[:, shuffle_indices]
end

function train(ğ—â‚œáµ£â‚™, ğƒâ‚œáµ£â‚™, ğ–)
    Ï† = uâ‚™ -> uâ‚™>0 ? 1 : 0 # McCulloch and Pitts's activation function (step function)
    Nâ‚‘ = 0 # number of errors - misclassification
    for (ğ±â‚™, ğâ‚™) âˆˆ zip(eachcol(ğ—â‚œâ‚›â‚œ), eachcol(ğƒâ‚œâ‚›â‚œ))
        ğ›â‚™ = ğ–*ğ±â‚™
        ğ²â‚™ = map(Ï†, ğ›â‚™) # for the training phase, you do not pass yâ‚™ to a harder decisor (the McCulloch and Pitts's activation function) (??? TODO)
        ğâ‚™ = ğâ‚™ - ğ²â‚™
        ğ– += Î±*ğâ‚™*ğ±â‚™'

        # this part is optional: only if it is interested in seeing the dataset evolution
        i = findfirst(x->x==maximum(ğ›â‚™), ğ›â‚™)
        Nâ‚‘ = ğâ‚™[i]==1 ? Nâ‚‘ : Nâ‚‘+1
    end
    accuracy = (size(ğƒ,2)-Nâ‚‘)/size(ğƒ,2)
    return ğ–, accuracy
end

function test(ğ—â‚œâ‚›â‚œ, ğƒâ‚œâ‚›â‚œ, ğ–)
    Ï† = uâ‚™ -> uâ‚™>0 ? 1 : 0 # McCulloch and Pitts's activation function (step function)
    Nâ‚‘ = 0 # number of errors - misclassification
    for (ğ±â‚™, ğâ‚™) âˆˆ zip(eachcol(ğ—â‚œâ‚›â‚œ), eachcol(ğƒâ‚œâ‚›â‚œ))
        ğ›â‚™ = ğ–*ğ±â‚™
        # yâ‚™ = map(Ï†, ğ›â‚™) # theoretically, you need to pass ğ›â‚™ through the activation function, but, in order to solve ambiguous instances (see Ajalmar's handwritings), we pick the class with the highest activation function input
        i = findfirst(x->x==maximum(ğ›â‚™), ğ›â‚™) # predicted value â†’ choose the highest activation function input as the selected class
        Nâ‚‘ = ğâ‚™[i]==1 ? Nâ‚‘ : Nâ‚‘+1
    end
    accuracy = (size(ğƒ,2)-Nâ‚‘)/size(ğƒ,2)
    return accuracy
end

function one_hot_encoding(label)
    return ["DH", "SL", "NO"].==label
end

ğ— = [fill(-1, size(ğ—,2))'; ğ—] # add the -1 input (bias)
ğƒ = rand(c,0)
for label âˆˆ labels
    global ğƒ = [ğƒ one_hot_encoding(label)]
end

## init
accâ‚œâ‚›â‚œ = rand(Náµ£) # vector of accuracies for test dataset
for náµ£ âˆˆ 1:Náµ£
    # initialize!
    ğ– = rand(c, Nâ‚+1) # [ğ°â‚áµ€; ğ°â‚‚áµ€; ...; ğ°áµ€_c]
    accâ‚œáµ£â‚™ = rand(Nâ‚‘) # vector of accuracies for train dataset (to see its evolution during training phase)

    # prepare the data!
    global ğ—, ğƒ = shuffle_dataset(ğ—, ğƒ)
    # hould-out
    global ğ—â‚œáµ£â‚™ = ğ—[:,1:(N*Nâ‚œáµ£â‚™)Ã·100]
    global ğƒâ‚œáµ£â‚™ = ğƒ[:,1:(N*Nâ‚œáµ£â‚™)Ã·100]
    global ğ—â‚œâ‚›â‚œ = ğ—[:,size(ğƒâ‚œáµ£â‚™, 2)+1:end]
    global ğƒâ‚œâ‚›â‚œ = ğƒ[:,size(ğƒâ‚œáµ£â‚™, 2)+1:end]

    # train!
    for nâ‚‘ âˆˆ 1:Nâ‚‘ # for each epoch
        ğ–, accâ‚œáµ£â‚™[nâ‚‘] = train(ğ—â‚œáµ£â‚™, ğƒâ‚œáµ£â‚™, ğ–)
        ğ—â‚œáµ£â‚™, ğƒâ‚œáµ£â‚™ = shuffle_dataset(ğ—â‚œáµ£â‚™, ğƒâ‚œáµ£â‚™)
    end
    # test!
    global accâ‚œâ‚›â‚œ[náµ£] = test(ğ—â‚œâ‚›â‚œ, ğƒâ‚œâ‚›â‚œ, ğ–) # taxa de acerto TODO: ver como Ã© isso em inglÃªs

    # plot training dataset accuracy evolution
    local fig = plot(accâ‚œáµ£â‚™, ylims=(0,2), label=["Disk Hernia" "Spondylolisthesis" "Normal"], xlabel="Epochs", ylabel="Hit rate", linewidth=2)
    println("\n\n$(accâ‚œáµ£â‚™)\n\n")
    display(fig)
    savefig(fig, "figs/trab3 (single layer perceptron)/column - training dataset evolution for realization $(náµ£).png")
end

display(plot(accâ‚œâ‚›â‚œ))

aÌ„cÌ„cÌ„ = Î£(accâ‚œâ‚›â‚œ)/Náµ£ # accuracy
ğ”¼accÂ² = Î£(accâ‚œâ‚›â‚œ.^2)/Náµ£
Ïƒacc = sqrt.(ğ”¼accÂ² .- aÌ„cÌ„cÌ„.^2) # standard deviation

println("Accuracy: $(aÌ„cÌ„cÌ„)")
println("Standard deviation: $(Ïƒacc)")