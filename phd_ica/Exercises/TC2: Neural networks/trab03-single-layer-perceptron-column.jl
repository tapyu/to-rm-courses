using FileIO, JLD2, Random, LinearAlgebra, Plots, LaTeXStrings
Î£=sum

### Vertebral column ###

## parameters and hyperparameters
N = 310 # number of instances
Nâ‚ = 6 # number of number of attributes, that is, input vector size at each intance n
c = 3 # number of perceptrons (neurons) of the single layer
Náµ£ = 20 # number of realizations
Nâ‚‘ = 100 # number of epochs
Nâ‚œáµ£â‚™ = 80 # % percentage of instances for the train dataset
Nâ‚œâ‚›â‚œ = 20 # % percentage of instances for the test dataset
Î± = 0.001 # learning step

## load the data
ğ—, labels = FileIO.load("Dataset/Vertebral column [uci]/dataset_3classes.jld2", "ğ—", "ğ")

## useful functions
function shuffle_dataset(ğ—, ğƒ)
    shuffle_indices = Random.shuffle(1:size(ğ—,2))
    return ğ—[:, shuffle_indices], ğƒ[:, shuffle_indices]
end

function train(ğ—â‚œáµ£â‚™, ğƒâ‚œáµ£â‚™, ğ–)
    Ï† = uâ‚™ -> uâ‚™>0 ? 1 : 0 # McCulloch and Pitts's activation function (step function)
    ğ„â‚œáµ£â‚™ = rand(size(ğƒâ‚œáµ£â‚™,1),size(ğƒâ‚œáµ£â‚™,2)) # matrix with all errors (TODO, do it better)
    for (n, (ğ±â‚™, ğâ‚™)) âˆˆ enumerate(zip(eachcol(ğ—â‚œáµ£â‚™), eachcol(ğƒâ‚œáµ£â‚™)))
        ğ›â‚™ = ğ–*ğ±â‚™
        ğ²â‚™ = map(Ï†, ğ›â‚™) # for the training phase, you do not pass yâ‚™ to a harder decisor (the McCulloch and Pitts's activation function) (??? TODO)
        ğ²â‚™ = ğ›â‚™
        ğâ‚™ = ğâ‚™ - ğ²â‚™
        ğ– += Î±*ğâ‚™*ğ±â‚™'
        ğ„â‚œáµ£â‚™[:,n] = ğâ‚™
    end
    ğÂ²â‚œáµ£â‚™ = Î£(eachcol(ğ„â‚œáµ£â‚™.^2))/size(ğ„â‚œáµ£â‚™,2)  # MSE (Mean squared error), that is, the the estimative of second moment of the error signal for this epoch
    return ğ–, ğÂ²â‚œáµ£â‚™
end

function test(ğ—â‚œâ‚›â‚œ, ğƒâ‚œâ‚›â‚œ, ğ–)
    Ï† = uâ‚™ -> uâ‚™>0 ? 1 : 0 # McCulloch and Pitts's activation function (step function)
    ğ„â‚œâ‚›â‚œ = rand(size(ğƒâ‚œáµ£â‚™,1),size(ğƒâ‚œáµ£â‚™,2)) # vector of errors
    for (n, (ğ±â‚™, ğâ‚™)) âˆˆ enumerate(zip(eachcol(ğ—â‚œâ‚›â‚œ), eachcol(ğƒâ‚œâ‚›â‚œ)))
        ğ›â‚™ = ğ–*ğ±â‚™
        # yâ‚™ = map(Ï†, ğ›â‚™) # theoretically, you need to pass ğ›â‚™ through the activation function, but, in order to solve ambiguous instances (see Ajalmar's handwritings), we pick the class with the highest activation function input
        ğ²â‚™ = ğ›â‚™.==maximum(ğ›â‚™) # choose the highest activation function input as the selected class
        ğâ‚™ = ğâ‚™ - ğ²â‚™
        ğ– += Î±*ğâ‚™*ğ±â‚™'
        ğ„â‚œâ‚›â‚œ[:,n] = ğâ‚™
    end
    ğÂ²â‚œâ‚›â‚œ = Î£(eachcol(ğ„â‚œâ‚›â‚œ.^2))/size(ğ„â‚œâ‚›â‚œ,2) # MSE (Mean squared error), that is, the the estimative of second moment of the error signal for this epoch
    return ğÂ²â‚œâ‚›â‚œ # MSE
end

function one_hot_encoding(label)
    return ["DH", "SL", "NO"].==label
end

ğ— = [fill(-1, size(ğ—,2))'; ğ—] # add the -1 input (bias)
ğƒ = rand(c,0)
for label âˆˆ labels
    global ğƒ = [ğƒ one_hot_encoding(label)]
end

## init
MSEâ‚œâ‚›â‚œ = rand(c,0)
for náµ£ âˆˆ 1:Náµ£
    # initialize!
    ğ– = rand(c, Nâ‚+1) # [ğ°â‚áµ€; ğ°â‚‚áµ€; ...; ğ°áµ€_c]
    MSEâ‚œáµ£â‚™ = rand(c,0)

    # prepare the data!
    global ğ—, ğƒ = shuffle_dataset(ğ—, ğƒ)
    # hould-out
    global ğ—â‚œáµ£â‚™ = ğ—[:,1:(N*Nâ‚œáµ£â‚™)Ã·100]
    global ğƒâ‚œáµ£â‚™ = ğƒ[:,1:(N*Nâ‚œáµ£â‚™)Ã·100]
    global ğ—â‚œâ‚›â‚œ = ğ—[:,size(ğƒâ‚œáµ£â‚™, 2)+1:end]
    global ğƒâ‚œâ‚›â‚œ = ğƒ[:,size(ğƒâ‚œáµ£â‚™, 2)+1:end]

    # train and test!
    for nâ‚‘ âˆˆ 1:Nâ‚‘ # for each epoch
        ğ–, MSEâ‚œáµ£â‚™ne = train(ğ—â‚œáµ£â‚™, ğƒâ‚œáµ£â‚™, ğ–)
        MSEâ‚œáµ£â‚™ = [MSEâ‚œáµ£â‚™ MSEâ‚œáµ£â‚™ne]
        ğ—â‚œáµ£â‚™, ğƒâ‚œáµ£â‚™ = shuffle_dataset(ğ—â‚œáµ£â‚™, ğƒâ‚œáµ£â‚™)
    end
    local fig = plot(MSEâ‚œáµ£â‚™', ylims=(0,2), label=["Disk Hernia" "Spondylolisthesis" "Normal"], xlabel="Epochs", ylabel="MSE", linewidth=2)
    savefig(fig, "figs/trab3 (single layer perceptron)/column - training dataset evolution for realization $(náµ£).png")
    global MSEâ‚œâ‚›â‚œ = [MSEâ‚œâ‚›â‚œ test(ğ—â‚œâ‚›â‚œ, ğƒâ‚œâ‚›â‚œ, ğ–)]
end

MÌ„SÌ„EÌ„ = Î£(eachcol(MSEâ‚œâ‚›â‚œ))/size(MSEâ‚œâ‚›â‚œ,2) # accuracy
ğ”¼MSEÂ² = Î£(eachcol(MSEâ‚œâ‚›â‚œ.^2))/size(MSEâ‚œâ‚›â‚œ,2)
Ïƒâ‚˜â‚›â‚‘ = sqrt.(ğ”¼MSEÂ² .- MÌ„SÌ„EÌ„.^2) # standard deviation

println("\n\nAccuracy for Disk Hernia = $(MÌ„SÌ„EÌ„[1])\nAccuracy for Spondylolisthesis =$(MÌ„SÌ„EÌ„[2])\nAccuracy for Normal = $(MÌ„SÌ„EÌ„[3])")
println("Standard deviation for Disk Hernia = $(Ïƒâ‚˜â‚›â‚‘[1])\nStandard deviation for Spondylolisthesis =$(Ïƒâ‚˜â‚›â‚‘[2])\nStandard deviation for Normal = $(Ïƒâ‚˜â‚›â‚‘[3])")