using FileIO, JLD2, Random, LinearAlgebra, Plots, LaTeXStrings
Î£=sum

## algorithm parameters and hyperparameters
Náµ£ = 20 # number of realizations
Nâ‚‘ = 100 # number of epochs
c = 3 # number of perceptrons (neurons) of the single layer
Î± = 0.001 # learning step
Ïƒâ‚“ = .1 # signal standard deviation
N = 150 # number of instances
Nâ‚ = 2 # number of attributes
Nâ‚œáµ£â‚™ = 80 # % percentage of instances for the train dataset
Nâ‚œâ‚›â‚œ = 20 # % percentage of instances for the test dataset

## generate dummy data
ğ—âš« = [Ïƒâ‚“*randn(50)'.+1.5; Ïƒâ‚“*randn(50)'.+1]
ğ—â–³ = [Ïƒâ‚“*randn(50)'.+1; Ïƒâ‚“*randn(50)'.+2]
ğ—â­ = [Ïƒâ‚“*randn(50)'.+2; Ïƒâ‚“*randn(50)'.+2]

ğ— = [fill(-1,N)'; [ğ—âš« ğ—â–³ ğ—â­]]
ğƒ = [repeat([1,0,0],1,50) repeat([0,1,0],1,50) repeat([0,0,1],1,50)]

## useful functions
function shuffle_dataset(ğ—, ğƒ)
    shuffle_indices = Random.shuffle(1:size(ğ—,2))
    return ğ—[:, shuffle_indices], ğƒ[:, shuffle_indices]
end

function train(ğ—â‚œáµ£â‚™, ğƒâ‚œáµ£â‚™, ğ–)
    Ï† = uâ‚™ -> uâ‚™>0 ? 1 : 0 # McCulloch and Pitts's activation function (step function)
    ğ„â‚œáµ£â‚™ = rand(size(ğƒâ‚œáµ£â‚™,1),size(ğƒâ‚œáµ£â‚™,2)) # matrix with all errors (TODO, do it better)
    for (n, (ğ±â‚™, ğâ‚™)) âˆˆ enumerate(zip(eachcol(ğ—â‚œáµ£â‚™), eachcol(ğƒâ‚œáµ£â‚™)))
        ğ›â‚™ = ğ–*ğ±â‚™
        ğ²â‚™ = map(Ï†, ğ›â‚™) # for the training phase, you do not pass yâ‚™ to a harder decisor (the McCulloch and Pitts's activation function) (??? TODO)
        ğ²â‚™ = ğ›â‚™
        ğâ‚™ = ğâ‚™ - ğ²â‚™
        ğ– += Î±*ğâ‚™*ğ±â‚™'
        ğ„â‚œáµ£â‚™[:,n] = ğâ‚™
    end
    ğÂ²â‚œáµ£â‚™ = Î£(eachcol(ğ„â‚œáµ£â‚™.^2))/size(ğ„â‚œáµ£â‚™,2)  # MSE (Mean squared error), that is, the the estimative of second moment of the error signal for this epoch
    return ğ–, ğÂ²â‚œáµ£â‚™
end

function test(ğ—â‚œâ‚›â‚œ, ğƒâ‚œâ‚›â‚œ, ğ–)
    Ï† = uâ‚™ -> uâ‚™>0 ? 1 : 0 # McCulloch and Pitts's activation function (step function)
    ğ„â‚œâ‚›â‚œ = rand(size(ğƒâ‚œáµ£â‚™,1),size(ğƒâ‚œáµ£â‚™,2)) # vector of errors
    for (n, (ğ±â‚™, ğâ‚™)) âˆˆ enumerate(zip(eachcol(ğ—â‚œâ‚›â‚œ), eachcol(ğƒâ‚œâ‚›â‚œ)))
        ğ›â‚™ = ğ–*ğ±â‚™
        # yâ‚™ = map(Ï†, ğ›â‚™) # theoretically, you need to pass ğ›â‚™ through the activation function, but, in order to solve ambiguous instances (see Ajalmar's handwritings), we pick the class with the highest activation function input
        ğ²â‚™ = ğ›â‚™.==maximum(ğ›â‚™) # choose the highest activation function input as the selected class
        ğâ‚™ = ğâ‚™ - ğ²â‚™
        ğ– += Î±*ğâ‚™*ğ±â‚™'
        ğ„â‚œâ‚›â‚œ[:,n] = ğâ‚™
    end
    ğÂ²â‚œâ‚›â‚œ = Î£(eachcol(ğ„â‚œâ‚›â‚œ.^2))/size(ğ„â‚œâ‚›â‚œ,2) # MSE (Mean squared error), that is, the the estimative of second moment of the error signal for this epoch
    return ğÂ²â‚œâ‚›â‚œ # MSE
end

## init
MSEâ‚œâ‚›â‚œ = rand(c,0)
for náµ£ âˆˆ 1:Náµ£
    # initialize!
    ğ– = rand(c, Nâ‚+1) # [ğ°â‚áµ€; ğ°â‚‚áµ€; ...; ğ°áµ€_c]
    MSEâ‚œáµ£â‚™ = rand(c,0)

    # prepare the data!
    global ğ—, ğƒ = shuffle_dataset(ğ—, ğƒ)
    # hould-out
    global ğ—â‚œáµ£â‚™ = ğ—[:,1:(N*Nâ‚œáµ£â‚™)Ã·100]
    global ğƒâ‚œáµ£â‚™ = ğƒ[:,1:(N*Nâ‚œáµ£â‚™)Ã·100]
    global ğ—â‚œâ‚›â‚œ = ğ—[:,size(ğƒâ‚œáµ£â‚™,2)+1:end]
    global ğƒâ‚œâ‚›â‚œ = ğƒ[:,size(ğƒâ‚œáµ£â‚™,2)+1:end]

    # train and test!
    for nâ‚‘ âˆˆ 1:Nâ‚‘ # for each epoch
        ğ–, MSEâ‚œáµ£â‚™ne = train(ğ—â‚œáµ£â‚™, ğƒâ‚œáµ£â‚™, ğ–)
        MSEâ‚œáµ£â‚™ = [MSEâ‚œáµ£â‚™ MSEâ‚œáµ£â‚™ne]
    end
    global MSEâ‚œâ‚›â‚œ = [MSEâ‚œâ‚›â‚œ test(ğ—â‚œâ‚›â‚œ, ğƒâ‚œâ‚›â‚œ, ğ–)]
    
    # make plots!
    if náµ£==1
        # plot training MSE x epochs
        local fig = plot(MSEâ‚œáµ£â‚™', ylims=(0,2), label=["circle" "triangle" "star"], xlabel="Epochs", ylabel="MSE", linewidth=2)
        savefig(fig, "figs/trab3 (single layer perceptron)/dummy data - training dataset evolution.png")

        # plot decision surface for the 1th realization
        local Ï† = uâ‚™ -> uâ‚™â‰¥0 ? 1 : 0 # activation function of the simple Perceptron
        local xâ‚_range = floor(minimum(ğ—[2,:])):.1:ceil(maximum(ğ—[2,:]))
        local xâ‚‚_range = floor(minimum(ğ—[3,:])):.1:ceil(maximum(ğ—[3,:]))
        global y(xâ‚, xâ‚‚) = findfirst(a -> a==maximum(ğ–*[-1, xâ‚, xâ‚‚]), ğ–*[-1, xâ‚, xâ‚‚]) # predict

        fig = contour(xâ‚_range, xâ‚‚_range, y, xlabel = L"x_1", ylabel = L"x_2", fill=true, levels=2)
        # train circe label
        scatter!(ğ—â‚œáµ£â‚™[2,ğƒâ‚œáµ£â‚™[1,:].==1], ğ—â‚œáµ£â‚™[3,ğƒâ‚œáµ£â‚™[1,:].==1], markershape = :hexagon, markersize = 8, markeralpha = 0.6, markercolor = :white, markerstrokewidth = 3, markerstrokealpha = 0.2, markerstrokecolor = :black, label = "circe class [train]")
            
        # test circle label
        scatter!(ğ—â‚œâ‚›â‚œ[2,ğƒâ‚œâ‚›â‚œ[1,:].==1], ğ—â‚œâ‚›â‚œ[3,ğƒâ‚œâ‚›â‚œ[1,:].==1], markershape = :dtriangle, markersize = 8, markeralpha = 0.6, markercolor = :white, markerstrokewidth = 3, markerstrokealpha = 0.2, markerstrokecolor = :black, label = "circle class [test]")

        # train triangle label
        scatter!(ğ—â‚œáµ£â‚™[2,ğƒâ‚œáµ£â‚™[2,:].==1], ğ—â‚œáµ£â‚™[3,ğƒâ‚œáµ£â‚™[2,:].==1], markershape = :hexagon, markersize = 8, markeralpha = 0.6, markercolor = :cyan, markerstrokewidth = 3, markerstrokealpha = 0.2, markerstrokecolor = :black, label = "triangle class [train]")

        # test triangle label
        scatter!(ğ—â‚œâ‚›â‚œ[2,ğƒâ‚œâ‚›â‚œ[2,:].==1], ğ—â‚œâ‚›â‚œ[3,ğƒâ‚œâ‚›â‚œ[2,:].==1], markershape = :dtriangle, markersize = 8, markeralpha = 0.6, markercolor = :cyan, markerstrokewidth = 3, markerstrokealpha = 0.2, markerstrokecolor = :black, label = "triangle class [test]")

        # train star label
        scatter!(ğ—â‚œáµ£â‚™[2,ğƒâ‚œáµ£â‚™[3,:].==1], ğ—â‚œáµ£â‚™[3,ğƒâ‚œáµ£â‚™[3,:].==1], markershape = :hexagon, markersize = 8, markeralpha = 0.6, markercolor = :green, markerstrokewidth = 3, markerstrokealpha = 0.2, markerstrokecolor = :black, label = "star class [train]")

        # test star label
        scatter!(ğ—â‚œâ‚›â‚œ[2,ğƒâ‚œâ‚›â‚œ[3,:].==1], ğ—â‚œâ‚›â‚œ[3,ğƒâ‚œâ‚›â‚œ[3,:].==1], markershape = :dtriangle, markersize = 8, markeralpha = 0.6, markercolor = :green, markerstrokewidth = 3, markerstrokealpha = 0.2, markerstrokecolor = :black, label = "star class [test]")

        title!("Decision surface")
        savefig(fig, "figs/trab3 (single layer perceptron)/dummy data - decision surface.png")
    end
end

MÌ„SÌ„EÌ„ = Î£(eachcol(MSEâ‚œâ‚›â‚œ))/size(MSEâ‚œâ‚›â‚œ,2) # accuracy
ğ”¼MSEÂ² = Î£(eachcol(MSEâ‚œâ‚›â‚œ.^2))/size(MSEâ‚œâ‚›â‚œ,2)
Ïƒâ‚˜â‚›â‚‘ = sqrt.(ğ”¼MSEÂ² .- MÌ„SÌ„EÌ„.^2) # standard deviation

println("\n\nAccuracy for circle = $(MÌ„SÌ„EÌ„[1])\nAccuracy for star =$(MÌ„SÌ„EÌ„[2])\nAccuracy for triangle = $(MÌ„SÌ„EÌ„[3])")
println("Standard deviation for circle = $(Ïƒâ‚˜â‚›â‚‘[1])\nStandard deviation for star =$(Ïƒâ‚˜â‚›â‚‘[2])\nStandard deviation for triangle = $(Ïƒâ‚˜â‚›â‚‘[3])")