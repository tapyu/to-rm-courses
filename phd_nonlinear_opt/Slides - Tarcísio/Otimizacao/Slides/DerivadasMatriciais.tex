% !TeX root = Otimizacao.tex
% !TeX encoding = UTF-8
% !TeX spellcheck = pt_BR
% !TeX program = pdflatex

\section{Conceitos de Derivadas Multivariável}

%\renewcommand*{\arraystretch}{1.8}

\subsection{Fundamentos Teóricos}

\begin{frame}
	\frametitle{\normalsize Derivadas Multivariável}
	\begin{itemize}
		\item Na resolução de problemas de otimização é bastante importante que se tenha uma base sólida sobre derivadas multivariável.
		\item Nesta seção, serão apresentados alguns casos, que são comumente encontrados na literatura.
	\end{itemize}
\end{frame}

\begin{frame}
	\frametitle{\normalsize Derivada Vetor-Escalar}
	\begin{itemize}
		\item Seja um vetor $\vtY = \Transp{\begin{bmatrix} y_1 & y_2 & \dots & y_N \end{bmatrix}} \in \mathbb{C}^{N \times 1}$ e um escalar $x$, temos que:
		\[\renewcommand{\arraystretch}{1.8}
			\dfrac{\partial \vtY}{\partial x} = \begin{bmatrix} 
				\dfrac{\partial y_1}{\partial x} \\
				\dfrac{\partial y_2}{\partial x} \\ 
				\vdots \\ 
				\dfrac{\partial y_N}{\partial x} 
			\end{bmatrix}
		\]
		\item Como se pode perceber, para se derivar um vetor em relação a um escalar, basta que derivar cada elemento do vetor $\vtY$ pelo escalar $x$.
		\item Neste caso, a dimensão de $\dfrac{\partial \vtY}{\partial x}$ é a mesma de $\vtY$.
	\end{itemize}
\end{frame}

\begin{frame}
	\frametitle{\normalsize Derivada Escalar-Vetor}
	\begin{itemize}
		\item Sejam agora o vetor $\vtX = \Transp{\begin{bmatrix} x_1 & x_2 & \dots & x_N \end{bmatrix}} \in \mathbb{C}^{N \times 1}$ e um escalar $y$, temos que:
		\[\renewcommand{\arraystretch}{1.8}
			\dfrac{\partial y}{\partial \vtX} = \begin{bmatrix} 
				\dfrac{\partial y}{\partial x_1} \\
				\dfrac{\partial y}{\partial x_2} \\ 
				\vdots \\ 
				\dfrac{\partial y}{\partial x_N} 
			\end{bmatrix}
		\]
		\item De modo análogo ao caso anterior, para se derivar um escalar em relação a um vetor, basta que derivar o escalar $y$ em relação a cada elemento do vetor $\vtX$.
		\item A dimensão de $\dfrac{\partial y}{\partial \vtX}$ é a mesma de $\vtX$.
	\end{itemize}
\end{frame}

\begin{frame}
	\frametitle{\normalsize Derivada Vetor-Vetor}
	\begin{itemize}
		\item Sejam os vetores $\vtX = \Transp{\begin{bmatrix} x_1 & x_2 & \dots & x_N \end{bmatrix}}$ e $\vtY = \Transp{\begin{bmatrix} y_1 & y_2 & \dots & y_M \end{bmatrix}}$, em que $\vtX \in \mathbb{C}^{N \times 1}$ e $\vtY \in \mathbb{C}^{M \times 1}$.
		\item Existem na literatura duas formas distintas de se representar a derivada de vetor em relação a outro vetor.
		\vspace*{-3ex}
		\begin{columns}
			\begin{column}[t]{0.4\paperwidth}
				\begin{itemize}
					\item ``\textit{Denominator Layout}'' ou formulação Jacobiana;
					\[\renewcommand{\arraystretch}{1.8}
						\left[ \dfrac{\partial \vtY}{\partial \vtX} \right]_{\textrm{Den}} = \begin{bmatrix} 
							\dfrac{\partial \vtY}{\partial x_1} \\
							\dfrac{\partial \vtY}{\partial x_2} \\ 
							\vdots \\ 
							\dfrac{\partial \vtY}{\partial x_N} 
						\end{bmatrix}
					\]
				\end{itemize}
			\end{column}
			\begin{column}[t]{0.4\paperwidth}
				\begin{itemize}
					\item ``\textit{Numerator Layout}'' ou formulação Hessiana;
					\[\renewcommand{\arraystretch}{1.8}
						\left[ \dfrac{\partial \vtY}{\partial \vtX} \right]_{\textrm{Num}} = \begin{bmatrix} 
							\dfrac{\partial \vtY}{\partial x_1} &
							\dfrac{\partial \vtY}{\partial x_2} & 
							\dots &
							\dfrac{\partial \vtY}{\partial x_N} 
						\end{bmatrix}
					\]
				\end{itemize}
			\end{column}
		\end{columns}
	\end{itemize}
\end{frame}

\begin{frame}
	\frametitle{\normalsize Derivada Vetor-Vetor}
	\begin{itemize}
		\item Como se pode perceber
		\[
			\left[ \dfrac{\partial \vtY}{\partial \vtX} \right]_{\textrm{Den}} = \Transp{\left[ \dfrac{\partial \vtY}{\partial \vtX} \right]_{\textrm{Num}}}
		\]
		\item Para nosso estudo, iremos utilizar a formulação Hessiana, por ser a mais comumente utilizada na literatura.
		\item Assim,
		\[\renewcommand{\arraystretch}{1.8}
			\dfrac{\partial \vtY}{\partial \vtX} = \begin{bmatrix}
				\dfrac{\partial y_1}{\partial x_1} & \dfrac{\partial y_2}{\partial x_1} & \dots & \dfrac{\partial y_M}{\partial x_1} \\
				\dfrac{\partial y_1}{\partial x_2} & \dfrac{\partial y_2}{\partial x_2} & \dots & \dfrac{\partial y_M}{\partial x_2} \\
				\vdots & \vdots & \ddots & \vdots \\
				\dfrac{\partial y_1}{\partial x_N} & \dfrac{\partial y_2}{\partial x_N} & \dots & \dfrac{\partial y_M}{\partial x_N} \\
			\end{bmatrix}
		\]
	\end{itemize}
\end{frame}

\begin{frame}
	\frametitle{\normalsize Derivada Matriz-Escalar}
	\begin{itemize}
		\item Segue a mesma lógica que a derivada vetor-escalar.
		\item Desta forma, seja uma matriz $\mtY \in \mathbb{C}^{N \times M}$ e um escalar $x$
		\item Assim,
		\[\renewcommand{\arraystretch}{1.8}
			\dfrac{\partial \mtY}{\partial x} = \begin{bmatrix}
				\dfrac{\partial y_{11}}{\partial x} & \dfrac{\partial y_{12}}{\partial x} & \dots & \dfrac{\partial y_{1M}}{\partial x} \\
				\dfrac{\partial y_{21}}{\partial x} & \dfrac{\partial y_{22}}{\partial x} & \dots & \dfrac{\partial y_{2M}}{\partial x} \\
				\vdots & \vdots & \ddots & \vdots \\
				\dfrac{\partial y_{N1}}{\partial x} & \dfrac{\partial y_{N2}}{\partial x} & \dots & \dfrac{\partial y_{NM}}{\partial x} \\
			\end{bmatrix}
		\]
	\end{itemize}
\end{frame}

\begin{frame}
	\frametitle{\normalsize Derivada Escalar-Matriz}
	\begin{itemize}
		\item Segue a mesma lógica que a derivada escalar-vetor.
		\item Desta forma, seja uma matriz $\mtX \in \mathbb{C}^{N \times M}$ e um escalar $y$
		\item Assim,
		\[\renewcommand{\arraystretch}{1.8}
			\dfrac{\partial y}{\partial \mtX} = \begin{bmatrix}
				\dfrac{\partial y}{\partial x_{11}} & \dfrac{\partial y}{\partial x_{12}} & \dots & \dfrac{\partial y}{\partial x_{1M}} \\
				\dfrac{\partial y}{\partial x_{21}} & \dfrac{\partial y}{\partial x_{22}} & \dots & \dfrac{\partial y}{\partial x_{2M}} \\
				\vdots & \vdots & \ddots & \vdots \\
				\dfrac{\partial y}{\partial x_{N1}} & \dfrac{\partial y}{\partial x_{N2}} & \dots & \dfrac{\partial y}{\partial x_{NM}} \\
			\end{bmatrix}
		\]
	\end{itemize}
\end{frame}

\subsection{Demonstrações de Derivadas Multivariável}
\subsubsection{Derivada de $\Herm{\vtA} \vtX$}

\begin{frame}
	\frametitle{\normalsize Derivada de $\Herm{\vtA} \vtX$}
	\begin{itemize}
		\item Sejam $\vtA, \vtX \in \mathbb{C}^{N \times 1}$, temos que:
		{\tiny
		\begin{align*}
			\dfrac{\partial \Herm{\vtA} \vtX}{\partial \vtX} &= \dfrac{\partial}{\partial \vtX} \left(
			\begin{bmatrix}
				\Conj{a}_1 & \Conj{a}_2 & \dots & \Conj{a}_N
			\end{bmatrix} \begin{bmatrix}
				x_{1} \\ x_{2} \\ \vdots \\ x_{N}
			\end{bmatrix} \right) 
			= \frac{\partial}{\partial \vtX} \left( \sum_{i = 1}^N \Conj{a}_ix_i \right) \\
		\end{align*}}
		\item Recaindo na formulação da derivada escalar-vetor, logo:
		{\tiny
		\begin{align*}
			\frac{\partial \Herm{\vtA} \vtX}{\partial \vtX} &= \begin{bmatrix}
				\dfrac{\partial}{\partial x_1} \left( \sum_{i = 1}^N \Conj{a}_ix_i \right) \\ \dfrac{\partial}{\partial x_2} \left( \sum_{i = 1}^N \Conj{a}_ix_i \right) \\ \vdots \\ \dfrac{\partial}{\partial x_N} \left( \sum_{i = 1}^N \Conj{a}_ix_i \right) 
			\end{bmatrix}
			= \begin{bmatrix}
				\Conj{a}_1 \\ \Conj{a}_2 \\ \vdots \\ \Conj{a}_N
			\end{bmatrix}
		\end{align*}}
		\[
			\boxed{\frac{\partial \Herm{\vtA} \vtX}{\partial \vtX} = \Conj{\vtA}}
		\]
	\end{itemize}
\end{frame}

\subsubsection{Derivada de $\Transp{\vtA} \vtX$}
\begin{frame}
	\frametitle{\normalsize Derivada de $\Transp{\vtA} \vtX$}
	\begin{itemize}
		\item Considerando os mesmo vetores $\vtA, \vtX \in \mathbb{C}^{N \times 1}$, temos que:
		{\tiny
		\begin{align*}
			\frac{\partial \Transp{\vtA} \vtX}{\partial \vtX} &= \frac{\partial}{\partial \vtX} \left(
			\begin{bmatrix}
				a_1 & a_2 & \dots & a_N
			\end{bmatrix} \begin{bmatrix}
				x_{1} \\ x_{2} \\ \vdots \\ x_{N}
			\end{bmatrix} \right)
			= \frac{\partial}{\partial \vtX} \left( \sum_{i = 1}^N a_ix_i \right)
		\end{align*}}
		\item De modo análogo ao caso anterior,
		{\tiny
		\begin{align*}
			\frac{\partial \Transp{\vtA} \vtX}{\partial \vtX} &= \begin{bmatrix}
				\dfrac{\partial}{\partial x_1} \left( \sum_{i = 1}^N a_ix_i \right) \\ \dfrac{\partial}{\partial x_2} \left( \sum_{i = 1}^N a_ix_i \right) \\ \vdots \\ \dfrac{\partial}{\partial x_N} \left( \sum_{i = 1}^N a_ix_i \right) 
			\end{bmatrix} 
			= \begin{bmatrix}
				a_1 \\ a_2 \\ \vdots \\ a_N
			\end{bmatrix}
		\end{align*}}
		\[
			\boxed{\frac{\partial \Transp{\vtA} \vtX}{\partial \vtX} = \vtA}
		\]
	\end{itemize}
\end{frame}

\subsubsection{Derivada de $\Herm{\vtX} \vtA$}
\begin{frame}
	\frametitle{\normalsize Derivada de $\Herm{\vtX} \vtA$}
	\begin{itemize}
		\item Considerando os mesmo vetores $\vtA, \vtX \in \mathbb{C}^{N \times 1}$, temos que:
		{\tiny
		\begin{align*}
			\frac{\partial \Herm{\vtX} \vtA}{\partial \vtX} &= \frac{\partial}{\partial \vtX} \left(
			\begin{bmatrix}
				x^*_1 & x^*_2 & \dots & x^*_N
			\end{bmatrix} \begin{bmatrix}
				a_{1} \\ a_{2} \\ \vdots \\ a_{N}
			\end{bmatrix} \right) 
			= \frac{\partial}{\partial \vtX} \left( \sum_{i = 1}^N x^*_ia_i \right)
		\end{align*}}
		\item De modo análogo aos casos anteriores e sabendo que $\color{red} \dfrac{dx^*}{dx} = 0$, temos
		{\tiny
		\begin{align*}
			\dfrac{\partial \Herm{\vtX} \vtA}{\partial \vtX} &= \begin{bmatrix}
					\dfrac{\partial}{\partial x_1} \left( \sum_{i = 1}^N x^*_ia_i \right) \\ \dfrac{\partial}{\partial x_2} \left( \sum_{i = 1}^N x^*_ia_i \right) \\ \vdots \\
					\dfrac{\partial}{\partial x_N} \left( \sum_{i = 1}^N x^*_ia_i \right) 
				\end{bmatrix} 
				= \begin{bmatrix}
					0 \\ 0 \\ \vdots \\ 0
				\end{bmatrix}
		\end{align*}}
		\[
			\boxed{\dfrac{\partial \Herm{\vtX} \vtA}{\partial \vtX} = \vtZero}
		\]
	\end{itemize}
\end{frame}

\subsubsection{Derivada de $\Transp{\vtX} \vtA$}
\begin{frame}
	\frametitle{\normalsize Derivada de $\Transp{\vtX} \vtA$}
	\begin{itemize}
		\item Considerando os mesmo vetores $\vtA, \vtX \in \mathbb{C}^{N \times 1}$, temos que:
		{\tiny
		\begin{align*}
			\dfrac{\partial \Transp{\vtX} \vtA}{\partial \vtX} &= \frac{\partial}{\partial \vtX} \left(
			\begin{bmatrix}
				x_1 & x_2 & \dots & x_N
			\end{bmatrix} \begin{bmatrix}
				a_{1} \\ a_{2} \\ \vdots \\ a_{N}
			\end{bmatrix} \right) 
			= \frac{\partial}{\partial \vtX} \left( \sum_{i = 1}^N x_ia_i \right) \\
			&= \begin{bmatrix}
				\dfrac{\partial}{\partial x_1} \left( \sum_{i = 1}^N x_ia_i \right) \\ \dfrac{\partial}{\partial x_2} \left( \sum_{i = 1}^N x_ia_i \right) \\ \vdots \\ \dfrac{\partial}{\partial x_N} \left( \sum_{i = 1}^N x_ia_i \right) 
			\end{bmatrix} 
			= \begin{bmatrix}
				a_1 \\ a_2 \\ \vdots \\ a_N
			\end{bmatrix}
		\end{align*}}
		\[
			\boxed{\frac{\partial \Transp{\vtX} \vtA}{\partial \vtX} = \vtA}
		\]
	\end{itemize}
\end{frame}

\subsubsection{Derivada de $\mtA \vtX$}
\begin{frame}
	\frametitle{\normalsize Derivada de $\mtA \vtX$}
	\begin{itemize}
		\item Considerando agora uma matriz $\mtA \in \mathbb{C}^{M \times N}$ e um vetor$\vtX \in \mathbb{C}^{N \times 1}$, temos que:
		{\tiny
		\begin{align*}
			\dfrac{\partial \mtA \vtX}{\partial \vtX} &= \dfrac{\partial}{\partial \vtX} \left(
				\begin{bmatrix}
					a_{11} & a_{12} & \dots & a_{1N} \\
					a_{21} & a_{22} & \dots & a_{2N} \\
					\vdots & \vdots & \ddots & \vdots \\
					a_{M1} & a_{M2} & \dots & a_{MN} \\
				\end{bmatrix} \begin{bmatrix}
					x_{1} \\ x_{2} \\ \vdots \\ x_{N}
				\end{bmatrix} \right) 
			%
			= \frac{\partial}{\partial \vtX} \left(\Transp{\begin{bmatrix} 
				\displaystyle \sum_{j = 1}^N a_{1j}x_j & \displaystyle \sum_{j = 1}^N a_{2j}x_j & \dots & \displaystyle \sum_{j = 1}^N a_{Mj}x_j
			\end{bmatrix}} \right) \\
			%
			&\hspace*{-10ex} = \begin{bmatrix}
				\displaystyle \dfrac{\partial}{\partial x_1} \left( \sum_{j = 1}^N a_{1j}x_j \right) & 
				\displaystyle \dfrac{\partial}{\partial x_1} \left( \sum_{j = 1}^N a_{21j}x_j \right) & 
				\dots & 
				\displaystyle \dfrac{\partial}{\partial x_1} \left( \sum_{j = 1}^N a_{Mj}x_j \right) \\
				\displaystyle \dfrac{\partial}{\partial x_2} \left( \sum_{j = 1}^N a_{1j}x_j \right) & 
				\displaystyle \dfrac{\partial}{\partial x_2} \left( \sum_{j = 1}^N a_{21j}x_j \right) & 
				\dots & 
				\displaystyle \dfrac{\partial}{\partial x_2} \left( \sum_{j = 1}^N a_{Mj}x_j \right) \\
				\vdots & \vdots & \ddots & \vdots \\
				\displaystyle \dfrac{\partial}{\partial x_N} \left( \sum_{j = 1}^N a_{1j}x_j \right) & 
				\displaystyle \dfrac{\partial}{\partial x_N} \left( \sum_{j = 1}^N a_{21j}x_j \right) & 
				\dots & 
				\displaystyle \dfrac{\partial}{\partial x_N} \left( \sum_{j = 1}^N a_{Mj}x_j \right) \\
			\end{bmatrix} 
			%
			= \begin{bmatrix}
				a_{11} & a_{21} & \dots & a_{N1} \\
				a_{12} & a_{22} & \dots & a_{N2} \\
				\vdots & \vdots & \ddots & \vdots \\
				a_{1M} & a_{2M} & \dots & a_{NM} \\
			\end{bmatrix}			
		\end{align*}}
		\[
			\boxed{\frac{\partial \mtA \vtX}{\partial \vtX} = \Transp{\mtA}}
		\]
	\end{itemize}
\end{frame}

\subsubsection{Derivada de $\Transp{\vtX} \mtA \vtX$}
\begin{frame}
	\frametitle{\normalsize Derivada de $\Transp{\vtX} \mtA \vtX$}
	\begin{itemize}
		\item Considerando a mesma matriz $\mtA \in \mathbb{C}^{M \times N}$ e um vetor$\vtX \in \mathbb{C}^{N \times 1}$ do caso anterior, temos que:
		{\tiny
		\begin{align*}
			\frac{\partial \Transp{\vtX} \mtA \vtX}{\partial \vtX} &= \frac{\partial}{\partial \vtX} \left(
				\begin{bmatrix}
					x_{1} & x_{2} & \dots & x_{N}
				\end{bmatrix}
				\begin{bmatrix}
					a_{11} & a_{12} & \dots & a_{1N} \\
					a_{21} & a_{22} & \dots & a_{2N} \\
					\vdots & \vdots & \ddots & \vdots \\
					a_{N1} & a_{N2} & \dots & a_{NN} \\
				\end{bmatrix} \begin{bmatrix}
					x_{1} \\ x_{2} \\ \vdots \\ x_{N}
				\end{bmatrix} \right) \\
			%
			&= \frac{\partial}{\partial \vtX} \left(
			\begin{bmatrix}
				\displaystyle \sum_{i = 1}^{N} x_{i}a_{i1} & 
				\displaystyle \sum_{i = 1}^{N} x_{i}a_{i2} & 
				\dots & 
				\displaystyle \sum_{i = 1}^{N} x_{i}a_{iN}
			\end{bmatrix} \begin{bmatrix}
				x_{1} \\ x_{2} \\ \vdots \\ x_{N}
			\end{bmatrix} \right) \\
			%
			&= \dfrac{\partial}{\partial \vtX} \left(
				\sum_{i = 1}^{N}\sum_{j = 1}^{N} x_{i} a_{ij} x_{j}
			\right) 
		\end{align*}}
	\end{itemize}
\end{frame}

\begin{frame}
	\frametitle{\normalsize Derivada de $\Transp{\vtX} \mtA \vtX$}
	{\tiny
	\begin{align*}
		\frac{\partial \Transp{\vtX} \mtA \vtX}{\partial \vtX} &= \begin{bmatrix}
			\displaystyle \frac{\partial}{\partial x_1} \left( \sum_{i = 1}^{N}\sum_{j = 1}^{N} x_{i} a_{ij} x_{j} \right) \\ 
			\displaystyle  \frac{\partial}{\partial x_2} \left( \sum_{i = 1}^{N}\sum_{j = 1}^{N} x_{i} a_{ij} x_{j} \right) \\ 
			\vdots \\ 
			\displaystyle \frac{\partial}{\partial x_N} \left( \sum_{i = 1}^{N}\sum_{j = 1}^{N} x_{i} a_{ij} x_{j} \right) 
		\end{bmatrix} 
		%
		= \begin{bmatrix}
			\displaystyle 2x_1a_{11} + \sum_{\substack{j = 1 \\ j \neq 1}}^{N} a_{1j} x_{j} + \sum_{\substack{i = 1 \\ i \neq 1}}^{N} a_{i1} x_{i} \\
			\displaystyle 2x_2a_{22} + \sum_{\substack{j = 1 \\ j \neq 2}}^{N} a_{2j} x_{j} + \sum_{\substack{i = 1 \\ i \neq 2}}^{N} a_{i2} x_{i} \\
			\vdots \\
			\displaystyle 2x_Na_{NN} + \sum_{\substack{j = 1 \\ j \neq N}}^{N} a_{Nj} x_{j} + \sum_{\substack{i = 1 \\ i \neq N}}^{N} a_{iN} x_{i} 
		\end{bmatrix} \\
		%
		&= \begin{bmatrix}
			\displaystyle \sum_{j = 1}^{N} a_{1j} x_{j} + \sum_{i = 1}^{N} a_{i1} x_{i} \\
			\displaystyle \sum_{j = 1}^{N} a_{2j} x_{j} + \sum_{i = 1}^{N} a_{i2} x_{i} \\
			\vdots \\
			\displaystyle \sum_{j = 1}^{N} a_{Nj} x_{j} + \sum_{i = 1}^{N} a_{iN} x_{i} 
		\end{bmatrix} 
	\end{align*}}
\end{frame}

\begin{frame}
	\frametitle{\normalsize Derivada de $\Transp{\vtX} \mtA \vtX$}
	\begin{itemize}
		\item Que pode ser escrito matricialmente como:
		\[
			\frac{\partial \Transp{\vtX} \mtA \vtX}{\partial \vtX} = \Transp{\mtA} \vtX + \mtA \vtX
		\]
		ou seja,
		\[
			\boxed{\frac{\partial \Transp{\vtX} \mtA \vtX}{\partial \vtX} = (\Transp{\mtA} + \mtA) \vtX}
		\]
		\item No caso de \underline{$\mtA$ ser uma matriz simétrica}, então $\mtA = \Transp{\mtA}$, logo:
		\[
			\boxed{\frac{\partial \Transp{\vtX} \mtA \vtX}{\partial \vtX} = 2\mtA \vtX}
		\]
	\end{itemize}
\end{frame}

\subsubsection{Derivada do Traço de uma matriz $\mtX$}
\begin{frame}
	\frametitle{\normalsize Derivada do Traço de uma matriz $\mtX$}
	\begin{itemize}
		\item Seja uma matriz $\mtA \in \mathbb{C}^{N \times M}$ e uma matriz $\vtX \in \mathbb{C}^{M \times N}$, temos que a derivada do traço de $\mtA \mtX$ é dada por:
		{\tiny
		\begin{align*}
			\frac{\partial \Trace{\mtA \mtX}}{\partial \mtX} &= \frac{\partial}{\partial \mtX} \left( \Trace{\begin{bmatrix}
				a_{11} & a_{12} & \dots & a_{1M} \\
				a_{21} & a_{22} & \dots & a_{2M} \\
				\vdots & \vdots & \ddots & \vdots \\
				a_{N1} & a_{N2} & \dots & a_{NM} \\
			\end{bmatrix}
			\begin{bmatrix}
				x_{11} & x_{12} & \dots & x_{1N} \\
				x_{21} & x_{22} & \dots & x_{2N} \\
				\vdots & \vdots & \ddots & \vdots \\
				x_{M1} & x_{M2} & \dots & x_{MN} \\
			\end{bmatrix}} \right) \\
			%
			&= \frac{\partial}{\partial \mtX} \left( \sum_{i = 1}^{N} \sum_{j = 1}^{N} a_{ij}x_{ji} \right) \\
			%
			&= \begin{bmatrix}
				\displaystyle \frac{\partial}{\partial x_{11}} \left( \sum_{i = 1}^{N} \sum_{j = 1}^{N} a_{ij}x_{ji} \right) & 
				\displaystyle \frac{\partial}{\partial x_{12}} \left( \sum_{i = 1}^{N} \sum_{j = 1}^{N} a_{ij}x_{ji} \right) & 
				\dots & 
				\displaystyle \frac{\partial}{\partial x_{1N}} \left( \sum_{i = 1}^{N} \sum_{j = 1}^{N} a_{ij}x_{ji} \right) \\
				\displaystyle \frac{\partial}{\partial x_{21}} \left( \sum_{i = 1}^{N} \sum_{j = 1}^{N} a_{ij}x_{ji} \right) & 
				\displaystyle \frac{\partial}{\partial x_{22}} \left( \sum_{i = 1}^{N} \sum_{j = 1}^{N} a_{ij}x_{ji} \right) & 
				\dots & 
				\displaystyle \frac{\partial}{\partial x_{2N}} \left( \sum_{i = 1}^{N} \sum_{j = 1}^{N} a_{ij}x_{ji} \right) \\
				\vdots & \vdots & \ddots & \vdots \\
				\displaystyle \frac{\partial}{\partial x_{M1}} \left( \sum_{i = 1}^{N} \sum_{j = 1}^{N} a_{ij}x_{ji} \right) & 
				\displaystyle \frac{\partial}{\partial x_{M2}} \left( \sum_{i = 1}^{N} \sum_{j = 1}^{N} a_{ij}x_{ji} \right) & 
				\dots & 
				\displaystyle \frac{\partial}{\partial x_{MN}} \left( \sum_{i = 1}^{N} \sum_{j = 1}^{N} a_{ij}x_{ji} \right) \\
			\end{bmatrix} 
		\end{align*}}
	\end{itemize}
\end{frame}


\begin{frame}
	\frametitle{\normalsize Derivada do Traço de uma matriz $\mtX$}
	\begin{itemize}
		\item Resultando em
		{\tiny
		\begin{align*}
			\frac{\partial \Trace{\mtA \mtX}}{\partial \mtX} &= \begin{bmatrix}
				a_{11} & a_{21} & \dots & a_{N1} \\
				a_{12} & a_{22} & \dots & a_{N2} \\
				\vdots & \vdots & \ddots & \vdots \\
				a_{1M} & a_{2M} & \dots & a_{NM} \\
			\end{bmatrix}
		\end{align*}}
		
		\[
			\boxed{\frac{\partial \Trace{\mtA \mtX}}{\partial \mtX} = \Transp{\mtA}}
		\]
	\end{itemize}
\end{frame}

\subsubsection{Derivada do Determinante de uma matriz $\mtX$}
\begin{frame}
	\frametitle{\normalsize Derivada do Determinante de uma matriz $\mtX$}
	\begin{itemize}
		\item Seja uma matriz $\mtX \in \mathbb{C}^{N \times N}$ quadrada.
		\item Através da expansão de Laplace (expansão em cofatores), podemos reescrever o determinante de $\mtX$ como a soma dos cofatores de uma linha ou coluna qualquer, multiplicada pelo seu elemento gerador, ou seja,
		\[
			\Det{\mtX} = \sum_{i = 1}^{N} x_{ki} \Det{\mtC_{ki}} = \sum_{i = 1}^{N} x_{ik} \Det{\mtC_{ik}} \,\,\,\,\,\, \forall k \in (1, N),
		\]
		em que $\mtC_{ij}$ representa o cofator da matriz $\mtX$ gerado a partir do elemento $x_{ij}$.
		\item Vale salientar que o cofator $\Det{\mtC_{ij}}$ independe do valor de qualquer elemento da linha $i$ ou da coluna $j$ de $\mtX$.
	\end{itemize}
\end{frame}

\begin{frame}
	\frametitle{\normalsize Derivada do Determinante de uma matriz $\mtX$}
	\begin{itemize}
		\item Assim, temos que:
		{\tiny
		\begin{align*}
			\frac{\partial \Det{\mtX}}{\partial \mtX} &= \frac{\partial}{\partial \mtX} \left( \sum_{i = 1}^{N} x_{ki} \Det{\mtC_{ki}} \right) \,\,\,\,\,\, \forall k \in (1, N) \\
			%
			&\hspace*{-10ex} = \frac{\partial}{\partial \mtX} \left( \begin{bmatrix}
				\displaystyle \sum_{i = 1}^{N} x_{1i} \Det{\mtC_{1i}} & 
				\displaystyle \sum_{i = 1}^{N} x_{1i} \Det{\mtC_{1i}} & 
				\dots & 
				\displaystyle \sum_{i = 1}^{N} x_{1i} \Det{\mtC_{1i}} \\
				\displaystyle \sum_{i = 1}^{N} x_{2i} \Det{\mtC_{2i}} & 
				\displaystyle \sum_{i = 1}^{N} x_{2i} \Det{\mtC_{2i}} & 
				\dots & 
				\displaystyle \sum_{i = 1}^{N} x_{2i} \Det{\mtC_{2i}} \\
				\vdots & \vdots & \ddots & \vdots \\
				\displaystyle \sum_{i = 1}^{N} x_{Ni} \Det{\mtC_{Ni}} & 
				\displaystyle \sum_{i = 1}^{N} x_{Ni} \Det{\mtC_{Ni}} & 
				\dots & 
				\displaystyle \sum_{i = 1}^{N} x_{Ni} \Det{\mtC_{Ni}}
			\end{bmatrix} \right) \\
			%
			&\hspace*{-10ex} = \begin{bmatrix}
				\displaystyle \frac{\partial}{\partial x_{11}} \left( \sum_{i = 1}^{N} x_{1i} \Det{\mtC_{1i}} \right) & 
				\displaystyle \frac{\partial}{\partial x_{12}} \left( \sum_{i = 1}^{N} x_{1i} \Det{\mtC_{1i}} \right) & 
				\dots & 
				\displaystyle \frac{\partial}{\partial x_{13}} \left( \sum_{i = 1}^{N} x_{1i} \Det{\mtC_{1i}} \right) \\
				\displaystyle \frac{\partial}{\partial x_{21}} \left( \sum_{i = 1}^{N} x_{2i} \Det{\mtC_{2i}} \right) & 
				\displaystyle \frac{\partial}{\partial x_{22}} \left( \sum_{i = 1}^{N} x_{2i} \Det{\mtC_{2i}} \right) & 
				\dots & 
				\displaystyle \frac{\partial}{\partial x_{33}} \left( \sum_{i = 1}^{N} x_{2i} \Det{\mtC_{2i}} \right) \\
				\vdots & \vdots & \ddots & \vdots \\
				\displaystyle \frac{\partial}{\partial x_{N1}} \left( \sum_{i = 1}^{N} x_{Ni} \Det{\mtC_{Ni}} \right) & 
				\displaystyle \frac{\partial}{\partial x_{N2}} \left( \sum_{i = 1}^{N} x_{Ni} \Det{\mtC_{Ni}} \right) & 
				\dots & 
				\displaystyle \frac{\partial}{\partial x_{N3}} \left( \sum_{i = 1}^{N} x_{Ni} \Det{\mtC_{Ni}} \right) \\
			\end{bmatrix}
		\end{align*}}
	\end{itemize}
\end{frame}

\begin{frame}
	\frametitle{\normalsize Derivada do Determinante de uma matriz $\mtX$}
	\begin{itemize}
		\item Resultando em:
		{\tiny
		\begin{align*}
			\frac{\partial \Det{\mtX}}{\partial \mtX} &= \begin{bmatrix}
				\Det{\mtC_{11}} & \Det{\mtC_{12}} & \dots & \Det{\mtC_{1N}} \\
				\Det{\mtC_{21}} & \Det{\mtC_{22}} & \dots & \Det{\mtC_{2N}} \\
				\vdots & \vdots & \ddots & \vdots \\
				\Det{\mtC_{N1}} & \Det{\mtC_{N2}} & \dots & \Det{\mtC_{NN}} \\
			\end{bmatrix}
		\end{align*}}
		\item A matriz $N \times N$ composta pelo determinante dos cofatores de $\mtX$ é denominada matriz adjunta de $\mtX$, ou simplesmente $\Adj{\mtX}$.
		
		\[
			\boxed{\frac{\partial \Det{\mtX}}{\partial \mtX} = \Adj{\mtX}}
		\]
	\end{itemize}
\end{frame}
